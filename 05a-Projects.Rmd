# Projects

Some *significant* applications are demonstrated in this chapter.

## Version control with Github {#section-github}

All analysis projects need to be saved via Git (on your local computer) and pushed to Github. Doing so has several benefits to both you, to your future self, and to your teammates:

-   Since Git is a version control system, you get to save and track changes in your work (data, source code, reports, PowerPoint decks, Shiny dashboards) incrementally.
-   Incremental saving means you can recover from any accidental plunders. It's like Track Changes in Word, but for multiple files and folders. Spill a Diet Coke on your laptop in the middle of a big analysis? No big deal (if you've been pushing commits to Github, it'll all be there!)
-   Collaboration is much more structured, with powerful tools for asynchronous work and managing versions.
-   Referencing and reviewing code, tracking issues, and sharing what you've done is seamless, which means ...
-   Your work will be reproducible: anyone from R&A can pull your repo from Github, run your analyses, add to or edit what you've done, and share those changes back in a way that is communicative and documented.
-   setting up web documentation for any R packages you build become seamless.

But enough on the **why** let's get to **how** (if you do want to know more on the why, [check out this excellent article by Jenny Bryan](https://doi.org/10.7287/peerj.preprints.3159v2))

### Getting Started with Git, Github, and RStudio

Here's a quick overview of what you'll need to do, with details to follow:

-   Dedicate a directory (a.k.a. "folder") to it.

-   Make it an RStudio Project.

-   Make it a Git repository.

-   Go about your usual business. But instead of only saving individual files, periodically you make a commit, which takes a multi-file snapshot of the entire project.

-   Push commits to GitHub periodically.

    -   This is like sharing a document with colleagues on OneDrive or DropBox or sending it out as an email attachment.

#### First steps {#first-steps}

**These steps are borrowed with some light editing from [Happy git with R](https://happygitwithr.com/) by Jenny Bryan.**

1.  [Register for GitHub account.](https://happygitwithr.com/github-acct.html#github-acct)
2.  [Install or update R and RStudio](https://happygitwithr.com/install-r-rstudio.html#install-r-rstudio)
3.  [Install Git](https://happygitwithr.com/install-git.html#install-git)
4.  Those on Windows will want [to do these steps as well](https://happygitwithr.com/shell.html#windows-shell-hell)
5.  [Introduce yourself to Git.](https://happygitwithr.com/hello-git.html#hello-git)
6.  [Prove local Git can talk to GitHub.](https://happygitwithr.com/push-pull-github.html#push-pull-github)
7.  [Cache your username and password](https://happygitwithr.com/credential-caching.html#credential-caching) so you don't need to authenticate yourself to GitHub interactively *ad nauseum*.
8.  Create and save a [GitHub Personal Access Token (PAT)](https://happygitwithr.com/credential-caching.html#credential-caching).
9.  [Prove RStudio can find local Git and, therefore, can talk to GitHub](https://happygitwithr.com/rstudio-git-github.html#rstudio-git-github).

### Feature Branch Worklow

There are [many workflows using Git and remote repositories like Github](https://www.atlassian.com/git/tutorials/comparing-workflows). All of thenm boil down to the following steps:

1.  Pull or fetch or clone a repo on Github to your local machine. If you are starting a new project, then you'll need to create a new repo on Github (but you can also start one on your machine). **This is usually called the main (formerly master) branch.**
2.  Create a new branch that you will work on.\
3.  Do some analysis, coding, writing.
4.  Periodically save a snapshot of your entire project (all the files and folders, except those that you explicitly ignore). This is called \*committing changes\*\*.
5.  Every once ins while **push your commits** to the remote repo. Congrats! You've just backed up your remotely and made it easy to share.
6.  **Merge** your new analysis and code back into the main branch. This is usually initiated by something called a *pull request* (which is admittedly a little confusing).

The specific workflow we use on IDEA's R&A team is the [Feature Branch workflow](https://www.atlassian.com/git/tutorials/comparing-workflows/feature-branch-workflow), which has the benefit of being both simple, while minimizing merge conflicts. The core idea behind the Feature Branch Workflow is that all feature development should take place in a dedicated branch instead of the main branch. This encapsulation makes it easy for multiple analysts to work on a particular analysis without disturbing the main codebase. It also means the main branch will never contain broken code. Moreover, it means you'll be more likely to get a second or third set of eyes on our analysis. This makes your work more transparent, helps enforce coding standards, and helps spread all the cool new techniques you've implemented in your analysis.

So what does this look like? Well, here's a picture of the feature branch workflow in use for this manual:

[![This image shows the network diagram from Github and is an illustration of the feature branch workflow.](img/feature_branch_example.png "Feature Branch Workflow Network Chart for this manual")](https://github.com/idea-analytics/r_and_a_manual/network)

This picture shows the development of this manual over time (from left to right) [as rendered by Github's network diagram](https://github.com/idea-analytics/r_and_a_manual/network): it includes new branches being created, commits being made and merges back into the main branch. The black line is the main branch and includes the most up-to-date, "official" version of this book. The green and blue lines are feature branches, which diverge from the main when you checkout a new branch. The dots represent commits. Colored lines returning to the main branch indicate a merge: the new code is now part of of the main branch. You might be wondering what the unmerged yellow line labeld `gh-page` represents. That is a special branch that is used by [Githbub Actions](https://github.blog/2019-08-08-github-actions-now-supports-ci-cd/) that uses the concept of continuous integration/continuous to build the website that hosts this manual. You don't need to worry about that one; it's simply used to build out the site magically.

### Example worflow with this manual, or getting your feet wet

This section is going to walk you through how to use git/github by updating this manual. You'll (i) clone the Github repo locally on your laptop, (ii) create a feature branch, (iii) make some changes to this documentation, save those changes, and then commit those changes git (i.e., locally take a snapshot), (iv) push those changes (including all of your commits) up to the Github repo, (v) initiate a pull request (i.e., ask to merge your branch into the main branch), and finally (vi) merge your changes into the master branch.

But first things first:

1.  Verify you did [the initial set-up steps above](#first-steps)
2.  Get your bio ready.

\*\*Note that throughout the steps below I'll show you how to each step Ok. Your ready? Great! Here we go.

#### Get the [R_and_A\_Manual](https://github.com/idea-analytics/r_and_a_manual/) repository URL

1.  Go to [R_and_A\_Manual](https://github.com/idea-analytics/r_and_a_manual/) repo in your browser.

2.  On the main page for the repo click the green *Code* button, Click on HTTPS (the default), and click the clipboard to copy the repo's URL:

    ![](img/clone_github_repo.png "Getting the clone URL from github")

#### Clone the repo {.tabset}

Now you'll pull the remote repo from Github onto you computer. You'll want to think about where you want to save this. For example I save data analysis projects in seperate folders under a `Data_Analysis/` folder. I save the manual just on my one drive.

##### Comand line

Here I'm saving this to temporary space, by navigating to `~/tmp/~` and then cloning the data:

```{bash cloning, eval=FALSE}
cd ~/tmp/
git clone https://github.com/idea-analytics/r_and_a_manual.git
```

This will pull down all the content of the repo: files, folders, all commits, all branches. Really the whole kit and kaboodle.

##### RStudio

Here's how you do it from RStudio:

1.  In RStudio, start a new Project: **File \> New Project \> Version Control \> Git**, or click on project icon in the upper right-hand corner of the IDE and select **New Project...**. ![](img/new_project_selection.png "Creating a new project")
2.  In the "repository URL" paste the URL of your new GitHub repository. That is: <https://github.com/idea-analytics/r_and_a_manual.git>
3.  Be intentional about where you create this project.
4.  You should click "Open in new session". ![](img/new_session_checkbox.png "Making sure you create a new session")
5.  Click **Create Project** to create a new directory, which will be all of these things:

-   a directory or "folder" on your computer
-   a Git repository, linked to the remote GitHub repository
-   an RStudio Project

Cool. You should now have the R&A Manual files on repo history on your computer!

#### Checkout a branch

*Before you start doing anything* you should check out a branch. A branch is like your own, tempory, disposable workspace. When you checkout a branch you create a new copy of the the repo and changes you make only happen on the branch. When your happy with the changes and are ready to share them you'll to a pull request. But we'll get to that below.

##### Command line

It's pretty straightforward. You create the branch, by giving a short but meaningful name, and then check it out.

```{bash branch, eval=FALSE}
git branch update-bio-cjh
git checkout update-bio-cjh
```

Or you can do both of those moves in one line by using `git checkout` with the `-b` flag:

```{bash branch_one_line, eval=FALSE}
git checkout -b update-bio-cjh
```

##### RStudio

1.  Click on the *Git* panel (usually in the upper right on that standard RStudio layout, but YMMV if you've customized your layouts).
2.  Click on the purple "branch" icon (it kinda looks like a piece of a flowchart). ![](img/rstduio_create_branch.png "Creating a branch in RStudio")
3.  Giving a short but meaningful name (something like, `update-bio-cjh`). Make sure the **Sync branch with remote** checkbox is selected; this will save you a step later when you push you changes up to the repo.

#### Making changes and saving them

You now on a new branch and go go makes some changes. Go ahead and open `02-Who_We_Are.Rmd` file and add your name as a section, update your bio and save it, as you usually would

Now you'll want to commit those changes, which takes a snapshot of the current state on the branch you are working on.

#### Command line

after saving you'll run the `git commit` command with the `-a` (adds all changes) and `-m` (add commit message) flag with a short description of what you did.

```{bash commit, eval=FALSE}
git commit -a -m "Updated Chris's bio"
```

You should do this often. After a while you'll want to push your changes up to Github (frequently, but not as often as commits):

```{bash push, eval = FALSE}
git push
```

You've likely not yet defined where this remote branch should go, but git will give you a helpful error which gives you the command for syncing your local branch with a new remote branch.:

```{bash, eval=FALSE}
fatal: The current branch update-bio-cjh has no upstream branch.
To push the current branch and set the remote as upstream, use

    git push --set-upstream origin update-bio-cjh
```

Go ahead and copy and run that command.

```{bash, eval=FALSE}
git push --set-upstream origin update-bio-cjh
```

After that you can just use `git push` and you'll branch changes will be saved remotely.

#### Maintaining large files

If you have a file that is over 100 MB in size (e.g. a PowerBI dashboard), GitHub will block your commit (or at least send you a warning). There are two ways of approaching this issue:

1.  Use the large file storage (LFS) extension to commit the file.
2.  Add the file to .gitignore so the file is not committed.

##### Large file storage (LFS)

Scenario: Suppose you have a PowerBI file `Persistence_Dashboard.pbix` that is 137 MB in size, so it cannot be handled through normal a Git workflow. You will need to download the LFS extension from this site: <https://git-lfs.github.com/>. Once you download, then you will follow these steps:

1.  Open the Git command line. (I have not found a solution through RStudio or GitHub Desktop).
2.  Switch to the repo and branch that has the large file.
3.  Install the LFS extension in the correct repo and branch using 

```{bash lfs install, eval = FALSE}
git lfs install
```

4.  Track the file extension (in our scenario, .pbix), using

```{bash lfs track, eval = FALSE}
git lfs track "*.pbix"
```

5.  Track .gitattributes.

```{bash add gitattributes, eval = FALSE}
git add .gitattributes
```
N.B.  Do not complete steps 3, 4, and 5 until after you have switched to the correct repo and branch.
6.  Commit the file and push normally.

When you commit, you will see a message indicating the LFS extension was used on that file.

##### Using `.gitignore`
Alternatively, you could choose not to commit the file and just keep the large file locally. (This is a good option if the file can be generated through code, like a `.csv` output). You will follow these steps:

1.  If the file does not already exist, create a text file called `.gitignore` in your repo.
2.  Add the name of the file you wish to ignore in the `.gitignore` file (e.g. `.Rdata` is ignored when using ProjectTemplate).
3.  Save `.gitignore` and commit.


#### RStudio

1.  In your git panel you see changed (or new) files show up. You'll want to select the check box for any file that's been modified (indicated by an M) or that needs to be added (indicated by an A). Doing so readies the file to be updated in the commit: ![](img/rstudio_git_staged.png)
2.  Click the commit button and new dialog box will open, which will show any changes you've made in an y file. Select the check box for staged, if isn't already selected, add a commit message and click **Commit** ![](img/rstudio_commit.png)
3.  When your ready to save those to the repo, simply press the *Push* button.

#### Merging changes.

Merging changes in your feature branch with the main branch requires you go to Github and to a *pull request*. A pull request is essential asking the main branch to "pull" in your changes and is technically known as a merge.. So here are the steps.

1.  Go to the repo (<https://github.com/idea-analytics/r_and_a_manual>).\
2.  You may see an info box suggesting you can merge your branch. If so, click on the **Compare & pull request** button. If not select your branch and click the Pull Request icon.
3.  If you are able to merge (you'll know) click the **Create pull request** button.
4.  Ask someone to review you request (ideally)
5.  Click the **Merge pull request button** and confirm the merge.
6.  If you are done with our feature branch feel free to delete it.

You done!

::: {.gotcha}
You'll want to be careful here if you are working with others. If you pulled your main branch down a while ago there is a risk that the main branch on your laptop is not up-to-date with the main branch on Github (because others have merged changes there).

The best remedy is to checkout and pull main---which gets up to date---and then checkout your branch and run `git merge main`. You may have to resolve conflicts.
:::


## Project Process

When we are working on a project, we need to document not only our analytical work, but also our organization and management. There are multiple tools we use to plan, track, modify and evaluate our progress among our team and with our stakeholders, and as we evolve, we certainly can add to our best practices for each of these tools. Namely, we will discuss how to set up Wrike projects, set up a GRPI/RASI, use operating mechanisms, and other important project structures.

::: {.tip}
The key point is that we backwards plan from our final product. The project management tools document these processes in greater detail, in the same way that a teacher might backwards plan from the assessment to the lesson, or a leader might document and follow up with the next steps from a meeting.
:::

Below is an overview of the backwards planning process. ![](img/project_workflow_diagram.png)


### Wrike

We use [Wrike](https://login.wrike.com/login/) as our main project management system to track the workflow for our team. All [information requests](https://ideapublicschoolsorg.sharepoint.com/RA/SitePages/Information-Requests.aspx) (note - this link may change as we transition to the IST team) generate a Wrike task, which will be accepted or rejected from our work. Once we know who will be assigned to the request, we can use Wrike in the following ways to organize the work.

#### Onboarding and setup

1. During tactical or throughout the week, review any new Wrike requests.

2. Determine if the request will be accepted, and if so, determine who will be assigned to the task.

3. Evaluate the scope of the request with your team AND the requester.
  * Vet the request thoroughly and ask clarifying questions with the requester.
  * If this is an ad-hoc request, keep the request in the original queue and set up tasks (see next section).
  * If this is a medium- to long-term request, then set up a project and related tasks into an appropriate folder:
    - Evaluation Projects
    - Impact Analyses
  * **N.B.** For medium- to long-term requests, you should consider establishing a GRPI and a RASI for your stakeholders, so that you have a common document for the scope and progress of the work.

#### Goal setting and deadlines

1. Once your Wrike project or task has a page, then you should start backwards planning from the deliverable. Ask yourself:
  * What is the *goal* of the project? (document it!)
  * What is the *deliverable* for the project? (document it!)
  * When is that deliverable due? (document it!)

2. Assign the goal and the deliverable to the task or project.
  * You can create a task/subtask for the deliverable and assign a due date. 
  * You can use the project/task description, leave a comment, or create a task/subtask for the goal.
  * Ex. "Deliver slide deck and final report to Dolores" could be the deliverable, and "2021-12-17" could be the due date.

#### Defining tasks and dependencies

1. Now, you need to accomplish those goals and produce the deliverable. Ask yourself:
  * What steps do I need to produce the deliverable?
  * What is the order in which I need to do those steps?
  * Do certain steps require other tasks to be completed first?

2. Create tasks/subtasks for each step.
  * Tasks could refer to a group of related steps that could be accomplished over a fixed period of time.
  * Subtasks could be specific, bite-sized steps that could be accomplished in a single day.
  * Ex. "Make visualizations of data" could be the task, and "Make time series plot of AP Spanish Language scores at Mission CP" could be the subtask.

3. Create dependencies for relevant steps.
  * Link the *predecessors* (i.e. the steps before) and the *successors* (i.e. the steps after) to the task/subtask.

4. Establish due dates for ALL steps, including dependencies, subtasks, etc.

#### Documenting and evaluating progress towards goal

1. Your project or task is ready-to-go! You still need to document your progress as you move forward.
  * After a meeting, jot down key points and next steps as a comment, or create new tasks.
  * Add comments after relevant tasks are completed.
  * Include links to any completed products.

::: {.hat}
Are you taking notes in a notebook, on a OneNote, or in a Teams chat? Capture the links to all your relevant documents, GitHub repos, notes, pictures, etc. in a relevant task, as a comment, or in a description in the Wrike project! You'll be able to find things you forgot about months later.
:::

2. If you used a GRPI/RASI for the project, be sure to update your stakeholders, as well.
  * Include a link to the GRPI/RASI in the Wrike project.
  * When you update Wrike, update the GRPI/RASI.
  * Your GRPI/RASI should succinctly summarize what you are doing in the project.

3. Things might change in your project. Each week, ask yourself:
  * Did the scope of the work change?
  * Did the timeline of the project change?
  * Did I realize something else needed to be completed?, and so on

If you need to modify the project, carve out time to do so. Evaluating your progress will help keep yourself on track, and will alert others that the project may take longer or may end differently than originally planned. And of course, **document it!**


### GRPI and RASI

### Project operating mechanisms



## ProjectTemplate (for analyses) {#section-ProjectTemplate}

[ProjectTemplate](http://projecttemplate.net/) is both an R package and an approach. Simply put, it's a package that builds the scaffolding for a project, provides good features for tracking package dependencies (though we also use `renv` for that), and separating data loading and prep from analysis and output, as well as caching of long running processes, which speeds up analytical time.

Is it perfect? No. It has more features than we'll likely ever use (logging, code profiling, and unit testing).

::: {.tip}
IF you are new to ProjectTemplate the [Getting Started Tutorial is great](http://projecttemplate.net/getting_started.html). Indeed, it's always good to [read the docs](https://tyk.io/wp-content/webp-express/webp-images/uploads/2017/06/documentation_matrix.jpg.webp)
:::

Seriously, though. The [ProjectTemplate documentation](http://projecttemplate.net/) is great and you should really set aside an hour to read through it. This manual will provide a very abbreviated overview of how to use ProjectTemplate at IDEA, but will not be a stand in for the official documentation.

### Installing ProjectTemplate

Installing the package is straightforward

```{r install_pt, eval=FALSE}
install.packages("ProjectTemplate")
```

### Creating a project with ProjectTemplate

Creating a project is pretty simple. Navigate to where you typically save projects vai RStudio and then run the following commands

```{r create-poject, eval=FALSE}
library(ProjectTemplate)
create.project("learning-pt")
```

Doing that bestows upon you the following directory structure:

![ProjectTemplate Directory Structure](img/learning-pt-directory-structure.png "ProjectTemplate Directory Structure")

::: {.gotcha}
You'll notice in the image above that there is `.Rproj` file, which isn't added by ProjectTemplate. I created an Project in RStudio first and then navigated to that direction and ran the following

`create.project("../learning-pt/", merge.strategy = "allow.non.conflict")`,

which allows you to scaffold ProjectTemplate in an existing directory while ignoring any existing files and folders (usually \*.Rproj and .git).
:::

### What goes where

We only use about half of the directories that ProjectTemplate Scaffolds. Here's what we use ,in the order it's evaluated by ProjectTemplate when you run `load.project()`:

**config** This directory contains the configuration file `global.dcf`. This is the first thing that `load.project()` looks at. The [fill list of what each setting does is here](http://projecttemplate.net/configuring.html). Here are a few highlights, in thier order of importance.

-   `load_libraries`: This can be set to 'on' or 'off'. If `load_libraries` is on, the system will load all of the R packages listed in the libraries field described below. By default, `load_libraries` is off. *I highly recommend that you turn this on*
-   'libraries': This is a comma separated list of all the R packages that the user wants to automatically load when `load.project()` is called. These packages must already be installed before calling `load.project()`. By default, the reshape2, plyr, tidyverse, stringr and lubridate packages are included in this list. *I recommend dropping these and only adding packages you are using for the project*.
-   `data_loading`: This can be set to 'on' or 'off'. If data_loading is on, the system will load data from both the cache and data directories with cache taking precedence in the case of name conflict. By default, `data_loading` is on.
-   `cache_loading`: This can be set to 'on' or 'off'. If `cache_loading` is on, the system will load data from the cache directory before any attempt to load from the data directory. By default, `cache_loading` is on.
-   `munging`: This can be set to 'on' or 'off'. If `munging` is on, the system will execute the files in the `munge` directory sequentially using the order implied by the `sort()` function. If `munging` is off, none of the files in the `munge` directory will be executed. By default, `munging` is on.
-   `as_factors`: This can be set to 'on' or 'off'. If `as_factors` is on, the system will convert every character vector into a factor when creating data frames; most importantly, this automatic conversion occurs when reading in data automatically. If 'off', character vectors will remain character vectors. By default, `as_factors` is off. \*This is a very good default, and the opposite of base R.

**lib**: this directory is used to house helper functions. You can store them in an `*.R` file (like the included `helpers.R` file). \*\* As a general rule of thumb, if you've copied-and-pasted a block of code twice, don't do it a third time; rather, abstract that block into a function and save it in this directory. `load.project` sources the files in this directory after readying `global.dcf`.

**cache**: Here you'll store any data sets that (i) are generated during a preprocessing step and (ii) don't need to be regenerated every single time you analyze your data. You can use the `cache()` function to store data to this directory automatically. Any data set found in both the cache and data directories will be drawn from cache instead of data based on ProjectTemplate's priority rules. ProjectTemplate always checks this before running code in the `data/` and `munge/`. directories. You can [learn more about caching here](http://projecttemplate.net/caching.html).

**data**: You store your raw data files here. If they are encoded [in a supported file format](http://projecttemplate.net/file_formats.html), they'll automatically be loaded when you call `load.project()`, \*unless cached versions of their output exist in the `cache/` directory.

**munge**: Here you can store any preprocessing or "data munging" code for your project. For example, if you need to add columns at runtime, merge normalized data set,s or globally censor any data points, that code should be stored in the `munge` directory. The preprocessing scripts stored in `munge` will be executed in alphabetical order when you call `load.project()`, *so you should prepend numbers (to digit like `01-aggregate_schools`, `02-calc-means`, ...) to the filenames to indicate their sequential order.*. Files in here are run after those in `cache/` and `data/` are loaded.

**reports**: Here you can store any output reports, especially RMarkdown reports, that you produce. This is where final reports live

::: {.gotcha}
ProjectTemplate doesn't always play well with RMarkdown (well, really the issue is with `knitr`. The biggest issue is running `load.project()` inside of RMarkdown. ProjectTemplate will complain that your `reports/` directory is not a ProjectTemplate directory, which is annoying. It's easily fixed with this line of code at the top of your RMarkdown and you are in Rproject:

`setwd(here::here()); load.project()`

This will change the working directory to the top of project and then run `load.project`. After the project is loaded `knitr` will set the working directory back to `reports/`, so all other file references should be relative to `reports/`
:::

**src**: Here you'll store your statistical analysis and mahcine scripts. You should add the following piece of code to the start of each analysis script: `library('ProjectTemplate); load.project()` (you don't need the tip above here). You should also do your best to ensure that any code that's shared between the analyses in src is moved into the munge directory; if you do that, you can execute all of the analyses in the src directory in parallel. A future release of ProjectTemplate will provide tools to automatically execute every individual analysis from src in parallel. You may want to `cache()` your results here as well.

**graphs**: Here you can store any graphs or PowerPoints that you produce, with the exception of those already contained in RMarkdown files.

## The `renv` package: Ensuring reproducibility

The [`renv` package](https://rstudio.github.io/renv/) goes a long way to solving a pernicious problem: Dependency Hell. Here's a good description of the problem:

[![Definition of dependency hell](img/dependency_hell.jpeg "renv helps avoid this"){width="447"}](https://arxiv.org/pdf/1410.0846.pdf)

This problem most manifest when you revisit a recurring project a year later. You updated some year variables in your code and rerun your code and ready to pat yourself on the back and then BAM! Your code breaks. It breaks because over the previous 12 months the packages you relied upon a year ago have been updated over time and don't work with other packages. Sometimes all you need to to is update all of your packages and everything is hunky-dory; but other times this brute force process doesn't work. You are left trying to understand whats changed over multiple packages, what your need to change in your code and something you thought would take you an hour takes a a week, with most days feeling like your just banging your heard against the wall.

This issues alone should be enough to convince you that your need to create **re**producible **env**ironments.

Another reason is related and often very immediate: working on the same analysis with someone else. Perhaps you are are fastidious and only use the most current version of every package and your partner is risk averse and only updates packages after they've been out in the wild for at least a year. If you are in this boat your going to have problems.

Again, you need to create **re**producible **env**ironments.

This is a common technique in Python and there it goes by the name of virtual environments. `renv` is an R implementation of the concept that is pretty elegant Along with [ProjectTemplate](#section-ProjectTemplate) we use the [`renv` package](https://rstudio.github.io/renv/) to ensure that our analysis projects are:

1.  **Isolated**: Each project gets its own library of R packages, so you can feel free to upgrade and change package versions in one project without worrying about breaking your other projects.

2.  **Portable**: Because `renv` captures the state of your R packages within a `lockfile`, you can more easily share and collaborate on projects with others, and ensure that everyone is working from a common base.

3.  **Reproducible**: Use `renv::snapshot()` to save the state of your R library to the lockfile `renv.lock`. You can later use `renv::restore()` to restore your R library exactly as specified in the lockfile.

These three points are crucial for working well with others and protection your future self.

::: {.tip}
As usual, you really should [read the docs for `renv`](https://rstudio.github.io/renv/articles/renv.html). They are thorough and clear.

There's also an excellent [FAQ](https://rstudio.github.io/renv/articles/faq.html)
:::

### Setting up `renv` with a new project

If you are starting a new project in RStudio then setting up `renv` is easy: just be sure to click the Use renv with this project.

![Definition of dependency hell](img/renv-new-Project.png "Be sure to click the "Use renv with this project" checkbok"){width="447"}

Checking that box will add the bootstrap `renv`, installing the package if necessary as well as a few folders and files that serve as infrastructure

### Setting up `renv` with an existing project

If you've got a project that you want to starting using `renv` it's pretty straightforward:.

1.  Install `renv` if you don't have it: `install.packages('renv')`
2.  Use `renv::init()` to initialize a project. `renv` will discover the R packages used in your project, and install those packages into a private project library.

It's really that simple!

### Using `renv`

The `renv` workflow is super, duper simple, after you've set it up:

1.  Work in your project as usual, installing and upgrading R packages as required as your project evolves.
2.  Use `renv::snapshot()` to save the state of your project library. The project state will be serialized into a file called `renv.lock`.
3.  If you want to revert to the previous state of your project---i.e., you installed a new version of a package and it's wreaking havoc on your project---simply use `renv::restore()`.

::: {.hat}
Following these simple steps isolates your projects environment from the rest of your other projects. So downloading a bleeding-edge, dev version of package from GitHub b/c you need a new feature won't pollute your other projects that are running just fine. This is the isolation bit mentioned in the three points above
:::

### Collaborating with `renv`

The `renv` developers recommend the following steps when using `renv` in collaborative settings

1.  Use a [git](https://git-scm.com/)/[GitHub](https://github.com/) repo.

2.  One user should explicitly initialize `renv` in the project, via [`renv::init()`](https://rstudio.github.io/renv/reference/init.html) or when starting an RStudio project. Doing so will create the initial `renv` lockfile, and also write the `renv` auto-loaders to the project's `.Rprofile` and `renv/activate.R`. These will ensure the right version of `renv` is downloaded and installed for your collaborators when they start in this project.

3.  Share your project sources, alongside the generated lockfile `renv.lock` via GitHub. Be sure to also share the generated auto-loaders in `.Rprofile` and `renv/activate.R` via the repo.

4.  When a collaborator first launches in this project, `renv` should automatically bootstrap itself, thereby downloading and installing the appropriate version of `renv` into the project library. After this has completed, they can then use [`renv::restore()`](https://rstudio.github.io/renv/reference/restore.html) to restore the project library locally on their machine.

::: {.gotcha}
While working on a project, you or your collaborators may need to update or install new packages in your project. **When this occurs, you'll also want to ensure your collaborators are then using the same newly-installed packages.** In general, the process looks like this:

1.  A user installs, or updates, one or more packages in their local project library;

2.  That user calls [`renv::snapshot()`](https://rstudio.github.io/renv/reference/snapshot.html) to update the `renv.lock` lockfile;

3.  That user then shares the updated version of `renv.lock` with their collaborators via github;

4.  Other collaborators then---after a git pull---call [`renv::restore()`](https://rstudio.github.io/renv/reference/restore.html) to install the packages specified in the newly-updated lockfile.

**A bit of care is required** if collaborators wish to update the shared `renv.lock` lockfile concurrently -- in particular, if multiple collaborators are installing new packages and updating their own local copy of the lockfile, then conflicts would need to be sorted out afterwards.

Use teams to ensure any changes to `renv.lock` are communicated so that everyone knows and understands when and why packages have been installed or updated.
:::

For more information on collaboration strategies, please visit [environments.rstudio.com](https://environments.rstudio.com/).

### `renv` and RStudio Connect

In short there should be no issues, but there is one important thing to always keep in mind when publishing to RStudio connect:

::: {.gotcha}
The `renv` generated `.Rprofile` file should **not** be included in deployments to RStudio Connect.
:::


## Publishing Projects

Our reports can be published for our stakeholders through a variety of sites. The location is up to you, but there are some nice options to do so.

### RStudio Connect

If your report is written in RMarkdown, then you can directly publish your report from RStudio to [RStudio Connect](https://www.rstudio.com/products/connect/) at our [team site](https://analytics.ideapublicschools.org/connect/).

To publish a report, do the following:

1. Make sure your output in the header section is set to `output: html_document`. You can add other options, like a table of contents and formatting.

2. Knit your .Rmd to find any compiling errors before you publish.

3. Click on the blue Publish button in the upper right hand corner of the html output or in the editor. 

4. Select RStudio Connect.

5. Choose if you want to publish with the source code or just publish the finished document. (The source code is useful if your report is recurring and can be updated, while the finished document is appropriate for things like analytical reports.)

6. Select analytics.ideapublicschools.org as the destination. Then, choose an appropriate title for your report. Finally, click Publish.

::: {.gotcha}
If the .Rmd filename is too long, then the report will not publish. You'll need to shorten the filename if you get errors.
:::

7. Check your report on RStudio Connect and make adjustments under the Settings gear.

A detailed guide about publishing can be found in the RStudio Connect [documentation](https://docs.rstudio.com/connect/user/publishing/).

### Power BI
If your dashboard was created in Power BI and you want to directly publish your dashboard to app.powerbi.com (Power BI Online) these are the steps to do so. You can go directly online to view your [Power BI](https://app.powerbi.com).

1. Make sure at the top right you are signed in with your account.

2. Save the report before Publishing.

3. On the Home menu, on the far right click Publish

4. Select the Workspace that you want to Publish the dashboard to, then click "Select"
  If you want to later post this Published dashboard to the team site (TheHub), select the "Research and Analysis" Workspace or a Premium Content Workspace, noted by a diamond on the Workspace.

5. Go to [Power BI](https://app.powerbi.com) then go to your dahsboard in the Workspace you Published and there will be two items, the dashboard and the data.

6. Click on the dashboard report and then go to File > Embed report > Website or portal and copy the like in "Here's a link you can use to embed this content."
  Use that like to share out or post to a Team Site

### Team Site (TheHub)

The R&A Team Site Documents folder contains files we upload to the Team Site.

The [Team Site (TheHub)][Team Site (TheHub)] Documents folder is not to be confused with the R&A Private Group Documents folder. The [R&A Private Group][R&A Private Group] Documents folder contains shared folders R&A team members use for collaborative work and storage.

#### Published Evaluation Plans

On an annual basis, save a copy of the final evaluation plan to the Team Site Documents folder and select for upload to the R&A Team Site.

The [Team Site (TheHub)][Team Site (TheHub)] and 
Team Site Documents folder are not to be confused with the R&A Private Group Documents folder. The [R&A Private Group][R&A Private Group] Documents folder contains shared folders R&A team members use for collaborative work and storage.

Evaluation plans are published annually to the [Team Site (TheHub)][Team Site (TheHub)]. Published evaluation plan files are stored in the Team Site `Documents folder > Evaluation > Evaluation Plans > FY 20##` for accessibility, consistency, and organization purposes.

On an annual basis, save copy of each final, approved, ready-for-publication evaluation plan to the appropriate Team Site Documents folder. Select files for upload to the R&A Team Site.


## Specific Projects

### Math Curriculum Redesign

This project was examining the differences between student achievement for pilot vs. non-pilot schools, for a new math curriculum that is being implemented. The pilot was not set up at-random, and project design was not created by R&A. It was challenging to find meaningful results because it was hard to tease apart the impacts of particular things on student achievement. It was difficult to know if a change in test scores was due to the new curriculum or something else. The results showed that although the test scores may not have been significantly different/better for pilot than the non-pilot schools, the pilot schools lowest scores were not as low as the non-pilot schools. In effect, the pilot schools did not do worse in terms of the lowest scores, so we could likely say the the new math curriculum was not harming the students (although it was hard to say if it was helping). We may revisit this project for AP classes and help our partners design the pilots and project design for the next round, which will hopefully support getting more meaningful results from the analysis.


### TSLIP

Ongoing TSLIP activities include TeachBoost counts (cumulative counts of the number of observations for GET rubrics submitted to evaluate teachers, and SLL rubrics submitted to evaluate leaders).

#### Focus Group Interviews

We are interviewing students at 10 College Preps and 10 Academies and asking several questions to find out "what is working" and "what is not working" at schools. The samples was carefully created using a model that fits Most Different Systems Design (MDSD). We considered using Most Similar Systems Design, but ran into the issue of finding schools that scored similarly acorss the independent variables.

For Most Similar
•	Keep all x’s the same except for one xn; that particular xn can vary on different levels or bins
•	Expect to see different y values (outcome variable)
•	Then we can say xn caused changes in y (dependent variable varies with xn)
o	Can eliminate other x’s as the causes for change/variation in y


For Most Different Systems Design:
•	All x’s are different, or better yet “random” (we don’t care if they are the same or different), except for one xn is the same (xn is the only thing that is the same across subjects)
•	Expect to see same/similar y (no differences in outcome variable)
•	Then we can say regardless of all the different values for the various x’s, that the same value of xn caused same/similar values of y (the independent variable xn – which was the same for all subjects – caused the outcome variable to be the same for all subjects)


Variables
•	x’s (independent/predictor variable)
o	x1 = School Maturity (Scaling vs. Fully Scaled)
o	x2 = Econ Dis
o	x3 = Overall rating
o	x4 = Domain 1/student achievement
o	x5 = Domain 2/ growth
o	x6 = Domain 3/other
o	x7 = Unknown variable (similar across schools, hoping to extract during interview process)
•	y (dependent/outcome variable)
o	y = Teacher Retention Rate


1.	Most Similar Systems Design (MSSD)   
               (NOTE: We would have 2 of these MSSD designs: one for AC, one for CP; not comparing AC to CP)
*Different x1 + same x2 + same x3 + same x4 = Different y
*Different maturity + same econ  dis + same overall rating + same Domain1 = Different Retention Rate
o	*can substitute another xi for the “different” x (It doesn’t have to be Maturity that is the x with different levels/bins); if wanting to see if another particular xi predicts variation in y (retention). For example, can hold Maturity constant and have differences on EconDis.

2.	Most Different Systems Design (MDSD)
               (NOTE: We would have 2 of these MDSD designs: one for AC, one for CP; not comparing AC to CP)
*Same x1 + different x2 + different x3 + different x4 = Same y
*Same unknown variable + different maturity + different econ dis + different overall rating + different Domain1/student achievement = Same Retention Rate
o	*can substitute another xi for the “same” x (It doesn’t have to be Maturity that is the x with same values across subjects); if wanting to see if another particular xi predicts sameness/lack of variation in values of y (retention). For example, can have differences on Maturity, but hold EconDis constant/same across subjects.

We are using #2 Most Different Systems Design, with the above model. Focus group interviews are in the process of being scheduled. 8-12 students spanning STAAR tested subjects/grades will be selected at each AC or CP (3rd-5th at Academy, 6th-12th at College Prep), for a total of 20 focus groups.

#### TSLIP Continuance/ Year 4 of the Grant

Blanca Carillo applied for the extension of the grant into Year 4. All departments and teams involved in the grant submitted a requested budget for Year 4 (October 1, 2023 to September 30, 2024), for Supplies, Equipment, Training and Professional Development, Travel, Stipends (such as for teacher or leaders in pilot groups), and contractor hours/work. 

Ilissa and Maura submitted a budget during the last week of April 2023 that included requests for funds for a contractor analyst for up to 20 hours per week for 32 weeks (2 semesters for the SY 23-24), as well as for contractor services for transcription for 20 hours of focus group interviews with students and teachers (and possibly teacher leaders). The analyst would provide high level technical service in analyzing grant data and possibly synthesizing all cumulative years of the grant into a product to help school leaders with decision making.

Funds were also requested for audio recorders, travel to conduct the interviews, travel to NLI and NTI for observations, travel for 1-2 professional conferences per teammate, and additional training for professional development (such as technical or qualitative training that would help further our analysis for the grant work to help inform leaders of information to improve metrics such as teacher retention. Budget fund requests will be processed and Blanca will let our team know what has been approved. There are still close to $4M in the TSLIP grant and the budget requests will come from this funding for the 4th year (continuance year). 

The usual supplies funding that is provided each year of the grant is for office supplies for items such as pencils, planners, printers, paper, etc., that aid in the work for the TSLIP grant.
