[["index.html", "IDEA R&amp;A Manual Chapter 1 A little about this manual 1.1 What goes in this manual? 1.2 How do you contribute to this book?", " IDEA R&amp;A Manual Chris Haid, Edison Coronado, Mishan Jensen, Maura Carter, Steven Macapagal, Aline Orr, Ilissa Madrigal, Marlena Coco 2022-05-13 Chapter 1 A little about this manual The IDEA R&amp;A Manual is IDEA Public Schools’ Research &amp; Analysis Team’s ever evolving encyclopedia of resources and best practices. It’s purpose is to make any R&amp;A team member’s (or really any other interested reader’s) job easier, by sharing how we do our jobs. The manual is particularly aimed at a technical audience (e.g. statisticians, data analysts, data engineers, data scientists) and most contributions are from technical authors. Indeed, the expectation is that technical members for the R&amp;A team will contribute to this manual regularly (i.e., weekly or daily during the festive (information) harvesting celebration of Doctoberfest) 1.1 What goes in this manual? Anything and everything we learn during the course of our work at IDEA. Of particular help is any information, learning, or insights, but some topics that are especially useful to your colleagues (and our future self) include: Locations of important tables in the data warehouse Information about how to use data in the warehouse (e.g. how do you identify students in CSI Links and examples to useful R packages Tips and tricks in R, SQL, or Python that you discover in the course of your work Templates: output templates, input templates, ProjectTemplates, . . . all the templates. 1.2 How do you contribute to this book? This manual is written in Markdown and managed with git/Github. If you are unfamiliar with git/Github you’ll want to read throughhow we use git in the Projects chapter The general process is as follows: Clone the repo: If you haven’t already, pull this book down from it’s Github repo. From the command line run git clone https://github.com/idea-analytics/r_and_a_manual.git. Or, pull down the latest version of the master branch git checkout master and then git pull In File Explorer/Finder find the r_and_a_manaul folder and double click r_and_a_manual.Rproj, which will open the book’s project in RStudio Check out a branch git checkout -b my-manual-update. Make updates. You can run in R bookdown::serve_book to have it recompile the book on save. Commit frequently. Push your changes on yoru branch up to Github. Intitiate a pull request to merge your changes. If they are minor you can do the whole process your self. IF they are major–especially changing the books structure–then have another team member review your pull request before merging. Complete the merge. Rinse and repeat! Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. Use second (##) and third (###) level headings for sections in each chapter. 1.2.1 Adding “Tips” and other callouts Do you see the blue box just above this section? That’s a tips box, but we have a few others you can add. Doing so is super simple by using a special block tag (:::) in your RMarkdown that creates a CSS-styled &lt;div&gt; block in the resulting HTML when this manual’s RMD files are rendered. Here’s an example. Writing this in your RMD: ::: {.tip} This is s a tip box! You should use this for quick callouts that are important pieces of hard-earned wisdom ::: Results in this: This is s a tip box! You should use this for quick callouts that are important pieces of hard-earned wisdom There are four other callout boxes we can use, each with a different icon: ::: {.rstudio-tip} This is an **RSTudio** Tip box This is s a different tip box to use for any sage advice regarding RStduio products like RStudio Connect, the RStudio IDE, or even RStudio supported packages. ::: ::: {.rstudio-tip} This is a **RSTudio** Tip box This is s a different tip box to use for any sage advice regarding RStduio products like RStudio Connect, the RStudio IDE, or even RStudio supported packages. ::: This is a RSTudio Tip box This is s a different tip box to use for any sage advice regarding RStudio products like RStudio Connect, the RStudio IDE, or even RStudio supported packages. ::: {.gotcha} This is a **gotcha** call out Warn your reader of things to look out for with this box. ::: This is a gotcha call out Warn your reader of things to look out for with this box. ::: {.design} This is a **design** call out This is great looking spot to provide extra advice on look-and-feel and design conventions. ::: This is a design call out This is great looking spot to provide extra advice on look-and-feel and design conventions. ::: {hat} This is a **hat** call out You get a hat! You can use this extra special tips that seem like magic. Or something like that. ::: This is a hat call out You get a hat! You can use this extra special tips that seem like magic. Or something like that. "],["who.html", "Chapter 2 Who we are 2.1 About Research and Analysis", " Chapter 2 Who we are 2.1 About Research and Analysis We are the research and analysis team at IDEA Public Schools. We are evaluators, statisticians, data scientists, and engineers, seeking above all to generate insight. 2.1.1 Edison Coronado Edison supports IDEA by making sure our stakeholders have access to the highest quality information that is streamlined, efficient, and accurate. He works with all teams across IDEA to increase the accuracy of our information used to help guide all stakeholders in making the best possible decisions. Edison also works with different teams in identifying opportunities to improve the process we use to collect, share, and use information. 2.1.2 Chris Haid Chris Haid is VP of Data Analytics at IDEA Public Schools, where leads research, evaluation, and data analysis across 160 schools in Texas and Louisiana. Previously he was KIPP Chicago’s Chief of Staff, leading progress monitoring, strategic planning, data analysis, and technology planning. He also supported external affairs, growth strategy, communications, and development. Other roles he held at KIPP Chicago included Chief Information Officer and the Director of Research and Analysis. Chris has held academic positions at Yale University, New York University, and the University of Chicago. He has taught graduate and undergraduate courses in social scientific theory, statistical analysis, game theory, international development, and globalization. Chris attended the College of William and Mary where he earned a BA in Economics and the University of Chicago earning both a MPP and a MA in Political Science. 2.1.3 Mishan Jensen, PhD Dr. Mishan Jensen is a Statistician on IDEA’s Research and Analysis Team with 4 years of experience in program evaluation in a large urban school district and 2 years of experience as a Statistician for the Texas Department of Public Safety. She holds a Master’s and Doctoral degrees from the University of Texas at Austin in educational psychology, with a specialization in Quantitative Methods. Previous work included evaluating multilingual education, teacher appraisal programs, and integration of arts into the classroom. During her time at UT, Mishan assisted with the development of the College of Education’s first fully web-based Introduction to Statistics course, as well as provided assistant teaching support for most of the graduate level statistics courses offered. Recent work at IDEA has included Student Persistence, Enrollment, Teacher Career Pathways and Advanced Placement, focusing on impact analyses and predictive modeling. 2.1.4 Steven Macapagal Steven Macapagal (Mac) is a statistician with IDEA Public Schools on the Research and Analysis team and has taught 6th through 12th grade. Most recently, he was a middle school math teacher and grade team leader at IDEA Bluff Springs College Prep in Austin, TX and prior to that, started his career at IDEA Frontier College Prep in Brownsville, TX as a 2014 corps member with Teach For America. Throughout his time at IDEA Frontier, he taught AP and IB math and economics, served as 12th grade team leader, and witnessed IDEA fulfill its promise to his 5 senior classes, all of whom achieved 100 percent matriculation to college. He earned his BA in economics and mathematics at The University of Texas at Austin and is currently working on his MS in statistics at Texas A&amp;M University. His recent projects have centered on instruction and curriculum, college success, DEI, and Camp Rio. 2.1.5 Maura Carter Maura Carter is a Statistician on the R&amp;A Team at IDEA, and a former mathematics educator and university instructor in the Statistics department at University of California, Santa Cruz. She holds a master’s degree from University of Notre Dame in nonprofit business management, and a master’s degree from California State University in Statistics and Data Science. She enjoys volunteering, mentoring students in STEM topics, ocean science, and scuba diving. Maura supports IDEA by providing analytical and statistical support to a variety of teams and decision makers across the organization. 2.1.6 Aline Orr, PhD Dr. Aline Orr has over 20 years of combined experience in research and program evaluation. She holds a Master’s degree in educational psychology and quantitative methods from the University of Texas at Austin and a PhD in Neuroscience from the University of Pittsburgh, where she focused on brain pathways underlying emotional responses. In the recent past, Aline has led the evaluation of educational programs serving elementary and secondary students in a large urban public school district. Before that she was conducting educational research for a large assessment and textbook publishing house. Aline joined IDEA’s Research and Analysis team as an evaluator in 2021. In this position, she supports the evaluation of initiatives associated with the Charter School Program grants, the 21st Century Afterschool program, and the Camp Rio program. 2.1.7 Ilissa Madrigal Ilissa Madrigal is an Evaluator on the R&amp;A team, often assisting with qualitative methods and frequently partnering with HA and AST teams. Recent work includes providing thematic analyses for GPTW open-ended questions, conducting focus groups on Core Values, designing training evaluation surveys, and providing evaluation support for the Teacher &amp; School Leader Incentive Program grant. Before working at IDEA, Ilissa taught high school chemistry in the RGV before earning her master’s in Industrial-Organizational Psychology from the University of Tulsa. She holds a BS in Psychology and a BA in Biology from The University of Texas at Austin. 2.1.8 Marlena Coco, PhD Dr. Marlena Coco is an Evaluator on the R&amp;A Team with over 15 years of experience in education and higher education. She earned a PhD in Higher Education Leadership and Research Methodology at Florida Atlantic University, MA in College Student Development at St. Edward’s University, and BS in Psychology at UT-Austin. She teaches undergraduate and graduate courses, chairs doctoral students dissertation research, reviews articles for journal publication, provides education consulting services, and conducts strengths-based leadership development. In previous roles, she led research, evaluation, and data support for college and career readiness at a large, urban school district, implemented district wide surveys, and analyzed postsecondary retention and academic support data at multiple universities. She evaluates Charter School Program grants. "],["the-data-warhehouse-and-how-to-access-it.html", "Chapter 3 The Data Warhehouse and how to access it 3.1 The Data Warehouse 3.2 Accessing the data warehouse from R 3.3 Key tables", " Chapter 3 The Data Warhehouse and how to access it The first section that follows provides lots of context about IDEA’s data warehouse. If, however, you are in a rush and want to go about accessing the data warehouse, then skip to section 3.2 on accessing from R below. 3.1 The Data Warehouse The ‘data warehouse’ is actually a collection of disparate databases hosted by IDEA Public Schools. Some databases are used to host the original source data, some are used as the data model for production applications, while others site between those two points. While the IDEA has ~28 servers hosting SQL Server databases, R&amp;A typically only accesses 9 servers hosting 96 databases and 2,718 distinct tables. It’s a lot, for sure. 3.1.1 Core data warehouse server configuration These facts below are scheduled to change in May-June of 2022. The IT team is moving all servers from Backspace to on-prem and as a result the Data warehousing team is upgrading servers and doing some maintenance on on the warehouse. Check back here for updates! The servers are not islands unto themselves, but rather form a small economy of data flows, where data moves from source servers, to production servers (i.e., the sweetspot for us) to reporting servers that host transformed data for our Logi-powered Locus dashboards. Figure 3.1: Data in the warehouse is initially stored in ‘src_` databases in ’DS’ servers and flows to the right towads Logi hosted Locus dashboards. Figure 3.1 shows the flows of data in the warehouse, which essentially move from left to right. TheDS-* servers host databases holding source data. That data is processed and saved in databases in the two production servers (PROD1 and PROD2). These two production servers host most the data that R&amp;A uses in analysis, but know where the source data comes from in is often helpful. Some data is (ostensibly) snapshotted and stored in the two *-HIS servers. To be sure, as of March 2020, R&amp;A doesn’t know much about these servers. The reporting servers—RS1, RS2, and RS-HIS, serve transformed flat files that serve as the data layer for Logi, which serves up Locus dashboards on the hub. The following sections provide some details on the what databases might be found on each server. 3.1.1.1 DS Servers NOTE: the DS servers have names that do not correspond the the servers’ URLS. Why this is the case is not clear, but this is something to pay attention to when work with data from these sources DS-PS is hosted on 887192-SQLDS and serves as a source for PROD1. Most the data hosted on DS-PS mirror PowerSchool’s Oracle back-end (espececially the Schools, ADA, and Enrollment databases), but it also has data from Teams Insights, staffing, college applications and outcomes, student persistence, among other things. Databases: ADA Attendance BlendedLearning CNP Colleges Enrollment Finance Insights: data from Microsoft Teams Persistence Schools: key data from PowerSchool Schools.DL Staffing Survey Sources: PowerSchool Other undefined sources Targets: Prod1 DS-III is hosted on 1044407-SQLDS2 and serves as a source for PROD2. (note that this is a bit confusing . . . which makes this writer think we may have this wrong; why would something called DS-III have server named DS2 that mostly provides source data to PROD2?) The data hosted on DS-III is almost exclusively source data from testing platforms like Illuminate DnA, AP, IB, ACT, and SAT. Databases: SRC_IA SRC_IB SRC_AP SRC_LEAP SRC_EA SRC_Dibles SRC_NWEA SRC_Plan SRC_SAT SRC_Telpas SRC_ACT SRC_Explorer Sources: See the database names just above PROD1 for lookup data about students and schools Targets: Prod2 DS-OT is hosted on 964592-SQLDS and serves as a source for PROD1. Databases focus on source data from variance external systems, including Naviance, Tyler Munis, Teachboost, Schoolmint, and a slew if individualized learning/blended learing platforms. Databases: SRC_Zendesk SRC_Naviance SRC_StMath SRC_DreamBox SRC_IHT SRC_TylerMunis SRC_TeachBoost SRC_StaffRetention SRC_SchoolMint SRC_Recruitment SRC_OpsCampusRanking SRC_JobVite SRC_HR SRC_GetRating SRC_ConnerStoneEvaluations SRC_BlendedLearning IdeaInstructionReporting Sources: See the database names just above PROD1 for lookup data about students and schools Targets: Prod1 3.1.1.2 PROD Servers PROD1 is hosted on 1065574-SQLPRD1 and contains a variety of data bases sourced from DS-PS and DS-OT. The key studetns, schools, and regions data is found here and is infact used by DS-III and DS-OT. Microsoft Insights data is now found here as well. This is the where transformed data is stored that is moved to RS2 (again, it is not clear why the server number doesn’t quite line up as we move from initial DS to PROD to RS). Databases: ADA Attendance BlendedLearning CNP Colleges Enrollment Finance Insights: data from Microsoft Teams Persistence Schools: key data from PowerSchool Schools.DL Staffing Survey Sources: DS-PS DS-OT Targets: RS2 ST-HIS FO1 PROD2 hosts all other data that is not on PROD, which is to say data initially stored in DS-III. This is the source server for RS1 and includes assessments (e.g., IAs, bi-weekly and unit assessments, AP, IB) data and accountability tables. Databases: Assessments Schools : this actually contains data \"inventories at the school level, like lists of employees, equipment and asset inventories, students schedules, correspondences between teachers and students. 3.1.1.3 RS and Hisotrical data Servers R&amp;A tends not to use these tables very often. RS1, RS2, and RS-HIS all have flat tables that utilized by by Logi for Locus Dashboards that are developed by Software Development. ST-HIS ostensibly has snapshots of data from the PROD servers, but that is just an unverified hunch ST-HIS Databases: unknown Sources: PROD1 PROD2 something called ST1 and ST2`? Targets RS-HIS RS1 Databases: unknown Sources: PROD2 something called ST2? Targets Logi RS2 Databases: unknown Sources: PROD1 something called ST1? Targets Logi RS-HIS Databases: unknown Sources: ST-HIS Targets Logi 3.1.2 Server, Table, and Field Lookup Table 3.2 provides details on servers, databases, tables, and fields. While it is not complete, it does cover the majority of our data infrastucture. Figure 3.2: Data warehouse details 3.1.3 Databases that R&amp;A frquently uses. : List of Windows Servers hosting SQL Server DBs and their IP Addresses 3.2 Accessing the data warehouse from R The most straightforward way to access the data warehouse is to use our own, bespoke r package: ideadata, which is maintained on github 3.2.1 Installation Since ideadata is an internal IDEA package there is only a development version, which is installed from GitHub with: #install.packages(&quot;remotes&quot;) remotes::install_github(&quot;idea-analytics/ideadata&quot;) #renv::install(&quot;idea-analytics/ideadata@main&quot;) also works 3.2.2 Example Here’s how you connect to the Schools table in the warehouse. library(dplyr) library(ideadata) schools &lt;- get_schools() glimpse(schools) The schools object above is tbl object. That means it works with dplyr verbs and functions, but what happens in the background is that dplyr and dbplyr generate SQL that is sent to the database you are connected to and all that computation (e.g., filtering, selecting, joining, calculations, aggregation) are completed on the remote SQL Server instance and not on your computer. Nevertheless, you will eventually want to pull that data down onto your machine when you want to use R or Python do what they can do (like modeling or graphics) that the database can’t do. Pulling that data down is easy with [dplyr::collect()] library(dplyr) schools_df &lt;- schools %&gt;% collect() %&gt;% janitor::clean_names() (Here janitor::clean_names() snake_cases all the column names). 3.2.2.1 What if I am pulling down lots of data (say, millions of rows)? In this instance the database connection may fail. It’s not ideal, but it happens. One way to deal with this is to pull down the data piecemeal. The collector() function in ideadata makes this task trivial. It takes column names as arguments (unquote) from the table you want to pull down and those columns are used to break up the data into smaller sets of data that are pulled down from the database onto your computer and then recombined into a single table. schools_df &lt;- schools %&gt;% collector(SchoolState, CountyName) %&gt;% janitor::clean_names() The ideadata package is clever in that it updates its knowledge of the data warehouse every time you load the package with library(ideadata). However to do that you need to have access to the the warehouse, and that requires that you are on the VPN/behind the firewall. Do makes sure that is true before invoking the package! 3.2.3 Finding things in the data warehouse The ideadata package has a function that will open up a sortable, filterable, and searchable table in the RStudio idea: view_warehouse_meta_data() 3.2.4 Where to learn more ideadata has it’s own documentation here. If you can’t find an answer there, then reach out to Chris Haid (he’s responsible for this craziness). Go to this article to set up your credentials in R so that the warehouse can authenticate and authorize you as verified user 3.3 Key tables This section details important tables. In general you want to stick with databases PROD1 or PROD2 but your mileage may vary. Another important aspect of the warehouse is that the sources system for most student data is our SIS: PowerSchool. The PowerSchool Tables Data Dictionary is a helpful resource in understanding fields and relationships between tables. "],["models-and-methods.html", "Chapter 4 Models and Methods 4.1 Research design 4.2 Descriptive statistics 4.3 Inferential statistics 4.4 Modeling and predictive statistics", " Chapter 4 Models and Methods Our research involves a wide variety of methods in statistics and data science to analyze problems and answer questions in educational data. In consultation with our stakeholders, we carefully select the most relevant approach(es) to provide an appropriate and accurate product. We use methods in descriptive, inferential, and predictive statistics to understand information about our organization. The choice of frequentist vs. Bayesian methods is up to you and should always be justified, but choosing the simplest, appropriate tool should be the starting point for analysis But, before we start any analysis, the design of the program influences how we establish our methods of research. 4.1 Research design When our works requires us to assign causation, we must consider how the data were collected before deciding on an appropriate model. 4.1.1 Causal inference Most of our data is observational, so if we want to describe causal relationships in our data, we must appeal to potential outcomes. Thus, we will briefly cover some of relevant causal inference concepts and techniques. When a student is marked for inclusion in, say, a tutorial group, we would like to see how much the tutorial increased their score on a test, compared to a scenario where the student did not attend the tutorial. If the student attends the tutorial, we can measure the outcome of the treatment (the tutorial) by seeing what grade they earned after a test; we cannot observe, however, the outcome of the control (no tutorial). Potential outcomes aims to remedy this. 4.1.1.1 Potential outcomes Suppose we have both the treatment and control outcomes for each student. We would like to measure \\(\\delta_i = Y^1_i - Y^0_i\\), the difference in the treatment outcome \\(Y^1_i\\) and control outcome \\(Y^0_i\\) for student \\(i\\). Then, our main estimators for the tutorial effect are \\(ATE, ATT\\) and \\(ATU\\), which are defined below: \\(ATE = E[\\delta_i]\\) represents the Average Treatment Effect \\(E[\\delta_i] = E[Y^1_i - Y^0_i]\\). In other words, \\(ATE\\) answers the question “What is the additional effect of the treatment?” \\(ATT = E[\\delta_i \\mid \\textrm{in treatment group}]\\) represents the Average Treatment effect of the Treatment group, \\(E[Y^1_i - Y^0_i \\mid \\textrm{in treatment group}]\\). In other words, \\(ATT\\) answers the question “What is the additional effect of the treatment for those in the treatment group?” \\(ATU = E[\\delta_i \\mid \\textrm{in control group}]\\) represents the Average Treatment effect of the Untreated (control) group, \\(E[Y^1_i - Y^0_i \\mid \\textrm{in control group}]\\). In other words, \\(ATU\\) answers the question “What is the additional effect of the treatment for those in the control group?” \\(ATE\\) can be decomposed into a weighted average of \\(ATT, ATU\\), such that \\(ATE = p\\times ATT + (1-p)\\times ATU\\), where \\(p\\) is the weight of the treatment group. In our scenario, \\(ATE\\) represents the expected marginal effect of the tutorial group on test scores for all students, \\(ATT\\) represents the expected marginal effect of the tutorial group for students who participate in tutorials, and \\(ATU\\) represents the expected marginal effect of the tutorial group for students who do not participate in tutorials. Estimates may be biased because we do not randomly assign students to tutorials - we select them for each group based on some criteria. Using randomization inference, we can separate assignment bias from the estimates of \\(ATT, ATU\\) to find appropriate estimates and p-values on \\(ATE\\). 4.1.1.2 Matching models We can leverage large samples to find a potential outcome for a student by matching other students who are reasonably similar, using a specified distance measure. Thus, we can reasonably estimate \\(ATE\\) since we have both the treatment and control outcomes. By using the matchit package in R, we can specify covariates we would like to match in our data between the treatment and control groups. #use matchit like a glm m_out &lt;- matchit(ActiveIL ~ Race + Gender + Grade + SPED + LEP + EcoDis + GradeEquivalentBOY, #response specifies control/treatment, covariates specify what to match data = rs_il_mod, method = &quot;cem&quot;, #specify matching method link = &quot;logit&quot;, estimand = &quot;ATT&quot;) #produces data frame with needed weights m_data &lt;- match.data(m_out) #model uses weighted linear model to estimate causal effect of using Imagine Learning m_att &lt;- lm_robust(GradeEquivalentGrowth ~ ActiveIL, data = m_data, weights = m_data$weights) summary(m_att) 4.1.1.3 Regression discontinuity 4.1.1.4 Instrumental variables 4.1.1.5 Difference-in-differences 4.1.2 Experimental design Under ideal conditions, we would test the effect of a program by randomized controlled trials – this usually does not happen. However, we can still use the tools of design to structure our data and variance appropriately. 4.1.2.1 Differences among distributions of groups ANOVA, ANCOVA, MANOVA, Kruskal-Wallis Code, diagnostics, communication and limitations coming soon… 4.1.2.2 Differences among levels of variables Educational data can often be described in a hierarchy: Level 1: student Level 2: student within a school Level 3: student within a school within a region A mixed model (also, hierarchical or multilevel) allows for both fixed and random effects, so this nesting will give us different variances at each level. This is ideal for modeling an outcome that can vary by school and regional differences - in other words, we can vary the slope and intercept by each school and region. Code, diagnostics, communication and limitations coming soon… 4.2 Descriptive statistics When summarizing a distribution of data, we are usually tasked with describing (1) measures of center, such as counts and proportions for categorical data, or means and medians for quantitative data and then displaying those values for our stakeholders. To provide context and increase the statistical literacy of our team and family, we also describe the (2) spread, (3) shape, and (4) unusual features of the distribution. 4.2.1 Numerical summaries - broad principles For categorical data, consider the distribution of proportions and counts (the sample size) across the various categories. One-way and two-way frequency tables, with marginal and total proportions and counts, are useful for displaying the data. For univariate, quantitative data, use the shape of the distribution to determine appropriate numerical measures. For symmetric data, the mean \\(\\bar{x}\\) mean(x) and standard deviation \\(s\\) sd(x) can be appropriate for reporting and is accessible by a broad audience. For skewed data, consider the median \\(\\tilde{x}\\) median(x) (and possibly the MAD - median absolute deviation mad(x)), and provide context for choosing these measures. Note: The median is also a widely accessible statistic to most people, since it describes the midpoint of your distribution. Consider using it alongside the mean. For bivariate data, use a scatterplot to determine if linear measures can be used appropriately. The Pearson correlation \\(r\\) cor(x, y) can be easily understood by a wide audience to describe the linear strength of a bivariate association. However, the Spearman rank correlation cor(x, y, method = \"spearman\") can provide an associative measure for monotonic nonlinear associations, and Kendall’s \\(\\tau\\) cor(x, y, method = \"kendall\") can describe concordance for ordinal data. When describing outliers in a report, care should be taken when describing how we identify and use (or remove) the outliers from the data set. One heuristic for deciding what to do with an outlier is to ask yourself, “Does this observation belong in the sample?” If you decide that the observation is fundamentally different than the others (e.g. a school is off-model and does not implement the Imagine Learning the same way that other schools do), then you might consider removing that observation. If the observation represents some exceptional case of the proposed model, then you might consider keeping that observation. In general, we should be modeling a measure of center and its spread as a function of covariates (see Modeling and predictive statistics). 4.2.2 Graphical summaries - broad principles While there are plenty of ways to display data, all graphs should be accompanied by an explanation of key features. Our aim is to help educators interpret the data in memorable, meaningful ways, and graphs without context can convey the wrong information or may obscure what the researcher attempted to communicate. Each graph should include a descriptive title, axes, and appropriate labels; consider using the ideacolors package for aesthetics. Selecting an appropriate graphical display depends on the purpose of the analysis and the communication of results. Bar graphs, line graphs, histograms, and scatterplots each have their place in displaying central tendency; however, emphasis should be placed on displaying variation within the data. 4.2.2.1 Displaying variation Variation can be addressed by the following (non-exhaustive) list: Comparing sample sizes, both for the entire distribution and within each group Comparing variation within and between groups Displaying variation across time Clustering of values in a region (and conversely, gaps in data) Different approaches to this could be altering alpha values, jittering, color scales, size, and the shape of the marker. Some sample code for doing is found below: make_Data_By_Launch_Cohort() %&gt;% ggplot(aes(x = LaunchSchoolYear, y = jitter(MedianGrowth), #jittering of clustered points group = School)) + geom_point(aes(color = School, #change color by school alpha = 0.2), #set transparency level to 0.2 size = 3.5) + #increases size of point geom_point(aes(y = MedianGrowthLaunch, color = LaunchOrder), shape = 3, #changes shape to plus sign + size = 5) + scale_color_idea() + theme_idea_light() + facet_wrap(~LaunchOrder, labeller = labeller(LaunchOrder = c(&quot;1&quot; = &quot;First Year&quot;, &quot;2&quot; = &quot;Second Year&quot;, &quot;3&quot; = &quot;Third Year&quot;))) + labs(title = &quot;Median RenStar Reading Growth&quot;, subtitle = &quot;Results grouped by schools launching in the same year&quot;, caption = &quot;+ represents median RenStar reading growth for that launching cohort&quot;, x = &quot;Launch Year&quot;, y = &quot;Median RenStar Growth&quot;) + scale_x_discrete(labels = c(&quot;2017-2018&quot; = &quot;2017-18&quot;, &quot;2018-2019&quot; = &quot;2018-19&quot;, &quot;2019-2020&quot; = &quot;2019-20&quot;, &quot;2020-2021&quot; = &quot;2020-21&quot;)) + theme(legend.position = &quot;none&quot;) This code spaces out MedianGrowth if there are multiple schools with the same median growth, changes the transparency level to 0.2, and changes color wtih each school. The value of MedianGrowthLaunch is plotted with a “plus” shape at a larger size to emphasize the median growth for each school year. This image shows a graph of median RenStar growth levels from launching schools in different school years and shows variation in those medians. 4.3 Inferential statistics If the goal of the project is to do inference, or drawing conclusions about the population from the data, then we should provide context about these conclusions, particularly around variability and likelihood. 4.3.1 Interval estimates Suppose we are providing an estimate, like forecasted student persistence rates. A single point estimate may give us our best target, but due to variability, we surely will not reach that exact forecast. Thus, we also provide a confidence interval (or a Bayesian credible interval) to give a lower and upper bound around our estimate. These bounds could be found by computing a margin of error around the point estimate, or could be determined by the bootstrapped quantiles. It may be instructive for a general audience to interpret these bounds as a “worst case scenario” and “best case scenario” and give a visual around these estimates. For reference, see the Persistence Dashboard (no longer actively updated). 4.3.2 p-values and hypothesis testing When deciding if a pattern or relationship even exists, we can choose a model and set of hypotheses to test. Given the type of explanatory and response variables in your model, you should choose an appropriately designed test (reference for common tests). Often, the test statistics and p-values are already given when using a package. Note the term “statistical significance” may carry a lot of weight with an audience, but it should be use with great caution. When using or explaining a p-value, first recall that a p-value is the probability of obtaining a result as extreme or greater from repeated sampling under the proposed distribution of the null hypothesis meaning that our probability operates under the assumption that the proposed model (the null hypothesis) is correct. (This means if we change the null hypothesis, then we could get an entirely different p-value with the same data). Thus, we should look carefully at how our choices in modeling and distribution affected the significance of the result. Furthermore, the choice of level (typically, \\(\\alpha = 0.05\\)) is arbitrary, so we cannot make certain conclusions, such as: If a result has a p-value of 0.06, and another has 0.04, then one result is not necessarily “more significant” than the other. The p-value does not imply how large the effect size is. Statistical significance does not necessarily represent practical significance. When communicating your results, do not merely rely on a p-value. Supplement a p-value with other explanations of the effect in context, like an interval estimate, graphs, a margin of error, or transformed values. For reference, see the American Statistical Association’s statement on p-values (2016). 4.4 Modeling and predictive statistics A large chunk of our statistical work involves developing a model and making predictions about a new set of data. The methods we use range from the basic models, like a least-squares linear model, to non-parametric methods and machine learning, like random forests. The choice of model largely depends on the goal of the project. 4.4.1 Linear modeling For short-term, ad-hoc models, we usually start with a simple linear regression, which essentially models a mean (quantitative) response value as a function of other explanatory variables. The mathematical model looks like this: \\(\\mu_i = \\beta_0 + \\beta_1 x_{1i} + \\cdots \\beta_n x_{ni} + \\epsilon_i\\) In R, we use the command lm() to generate estimates of the beta coefficients. To obtain the p-values and basic fit statistics, use summary(lm_object). To obtain the diagnostic plots, use plot(lm_object). We can describe the coefficients as an additional unit effect on the mean response. If the model has been identified using a DAG, then we can interpret the coefficient of a direct path as the causal effect. 4.4.2 Generalized linear modeling When a transformed response is linear in the betas, then we use a generalized linear model (GLM). A common example of GLM is a logistic regression, where the response is binary, so we effectively predict the odds of an event happening relative to another event. The mathematical model looks like this: \\(g(\\mu_i) = \\beta_0 + \\beta_1 x_{1i} + \\cdots + \\beta_n x_{ni} + \\epsilon_i\\) where \\(g(\\cdot)\\) is a link function to the mean \\(\\mu\\) is the mean response Note that there are many link functions, so the choice of link and mean response will affect the interpretation of the model. Different implementations in R will provide differing pieces of information, so it is best to look up the documentation. However, base R provides the glm() function, which gives options to choose a link. If the response is multinomial (multiple categories), then it is suggested to use a multinomial logistic regression. If the categories are ordered, which are common in assessment data, TCP ratings, Likert scales, etc., the model may benefit from using ordered responses in an ordinal logistic regression, which improves the power. See this reference for implementations of OLR using various packages. Code, diagnostics, communication and limitations coming soon… 4.4.3 Machine learning If the goal moves beyond inference towards prediction (getting as accurate of a response as possible), then we can use machine (statistical) learning methods to model the data. These tools are particularly useful when we are modeling nonlinear trends and cannot assume a functional form. Note that communicating results from these models may be beyond a general audience’s reach, so caution should be taken when describing a marginal effect of a variable. The ISLR website and tidymodels implementation should be consulted for more details about these models. Code, diagnostics, communication and limitations coming soon… "],["tips-and-tricks.html", "Chapter 5 Tips and Tricks 5.1 Data processing 5.2 Statistical pitfalls", " Chapter 5 Tips and Tricks This section of the R&amp;A Manual is for sharing various tricks that are will make you life easier. Rules of thumb, explanations of how things work, and useful design patterns are all fair game. 5.1 Data processing 5.1.1 De-duplicating rows in a table. There are two general purpose design patterns for de-duplicating rows in a table in either R or SQL. In R use the dplyr::distinct() function and in SQL use a DISTINCT clause. This is pretty simple and robust, with the typical use case of wanting to reduce many identical rows to a single row. If you are trying to de-duplicate where you may have non-identical rows (e.g., a student has taken the same assessment multiple times in a testing window and you’d like to get the latest date) then you can use a group_by-arrange-row_number-filter pattern: Group you data by the columns that uniquely identify your object of inquiry (e.g., student number) Arrange or sort the data by a column that puts an ordering you care about within the grouped data (e.g., assessment date) Create a new column that contains the row number within the group (e.g, a student with 5 assessments in a window would have row_number = 1 for the first date and row_number = 5 for the last date). Filter for the maximum row number(with dplyr::filter() in R or a WHERE clause in SQL) (optional) Ungroup your data. 5.1.2 Creating “Scaffolds” when joining data. Coming Soon. . . 5.2 Statistical pitfalls 5.2.1 Simpson’s Paradox When identifying trends in a distribution, be careful of interpreting those trends, particularly when there is hierarchy involved in the population. The ranking, magnitude, and direction of the trends can reverse when conditioning on a certain level of a hierarchy. For example, STAAR achievement levels as a function of minutes on an individual learning program like Dreambox may not be significant at the district level, but could be significant when at the school or classroom level. 5.2.2 Bias-variance tradeoff "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
