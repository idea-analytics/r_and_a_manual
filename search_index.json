[["index.html", "IDEA R&amp;A Manual Chapter 1 A little about this manual 1.1 What goes in this manual? 1.2 How do you contribute to this book?", " IDEA R&amp;A Manual Chris Haid, Edison Coronado, Mishan Jensen, Maura Carter, Steven Macapagal, Aline Orr, Ilissa Madrigal, Marlena Coco 2022-05-12 Chapter 1 A little about this manual The IDEA R&amp;A Manual is IDEA Public Schools’ Research &amp; Analysis Team’s ever evolving encyclopedia of resources and best practices. It’s purpose is to make any R&amp;A team member’s (or really any other interested reader’s) job easier, by sharing how we do our jobs. The manual is particularly aimed at a technical audience (e.g. statisticians, data analysts, data engineers, data scientists) and most contributions are from technical authors. Indeed, the expectation is that technical members for the R&amp;A team will contribute to this manual regularly (i.e., weekly or daily during the festive (information) harvesting celebration of Doctoberfest) 1.1 What goes in this manual? Anything and everything we learn during the course of our work at IDEA. Of particular help is any information, learning, or insights, but some topics that are especially useful to your colleagues (and our future self) include: Locations of important tables in the data warehouse Information about how to use data in the warehouse (e.g. how do you identify students in CSI Links and examples to useful R packages Tips and tricks in R, SQL, or Python that you discover in the course of your work Templates: output templates, input templates, ProjectTemplates, . . . all the templates. 1.2 How do you contribute to this book? This manual is written in Markdown and managed with git/Github. If you are unfamiliar with git/Github you’ll want to read throughhow we use git in the Projects chapter The general process is as follows: Clone the repo: If you haven’t already, pull this book down from it’s Github repo. From the command line run git clone https://github.com/idea-analytics/r_and_a_manual.git. Or, pull down the latest version of the master branch git checkout master and then git pull In File Explorer/Finder find the r_and_a_manaul folder and double click r_and_a_manual.Rproj, which will open the book’s project in RStudio Check out a branch git checkout -b my-manual-update. Make updates. You can run in R bookdown::serve_book to have it recompile the book on save. Commit frequently. Push your changes on yoru branch up to Github. Intitiate a pull request to merge your changes. If they are minor you can do the whole process your self. IF they are major–especially changing the books structure–then have another team member review your pull request before merging. Complete the merge. Rinse and repeat! Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. Use second (##) and third (###) level headings for sections in each chapter. 1.2.1 Adding “Tips” and other callouts Do you see the blue box just above this section? That’s a tips box, but we have a few others you can add. Doing so is super simple by using a special block tag (:::) in your RMarkdown that creates a CSS-styled &lt;div&gt; block in the resulting HTML when this manual’s RMD files are rendered. Here’s an example. Writing this in your RMD: ::: {.tip} This is s a tip box! You should use this for quick callouts that are important pieces of hard-earned wisdom ::: Results in this: This is s a tip box! You should use this for quick callouts that are important pieces of hard-earned wisdom There are four other callout boxes we can use, each with a different icon: ::: {.rstudio-tip} This is an **RSTudio** Tip box This is s a different tip box to use for any sage advice regarding RStduio products like RStudio Connect, the RStudio IDE, or even RStudio supported packages. ::: ::: {.rstudio-tip} This is a **RSTudio** Tip box This is s a different tip box to use for any sage advice regarding RStduio products like RStudio Connect, the RStudio IDE, or even RStudio supported packages. ::: This is a RSTudio Tip box This is s a different tip box to use for any sage advice regarding RStudio products like RStudio Connect, the RStudio IDE, or even RStudio supported packages. ::: {.gotcha} This is a **gotcha** call out Warn your reader of things to look out for with this box. ::: This is a gotcha call out Warn your reader of things to look out for with this box. ::: {.design} This is a **design** call out This is great looking spot to provide extra advice on look-and-feel and design conventions. ::: This is a design call out This is great looking spot to provide extra advice on look-and-feel and design conventions. ::: {hat} This is a **hat** call out You get a hat! You can use this extra special tips that seem like magic. Or something like that. ::: This is a hat call out You get a hat! You can use this extra special tips that seem like magic. Or something like that. "],["who.html", "Chapter 2 Who we are 2.1 About Research and Analysis", " Chapter 2 Who we are 2.1 About Research and Analysis We are the research and analysis team at IDEA Public Schools. We are evaluators, statisticians, data scientists, and engineers, seeking above all to generate insight. 2.1.1 Edison Coronado Edison supports IDEA by making sure our stakeholders have access to the highest quality information that is streamlined, efficient, and accurate. He works with all teams across IDEA to increase the accuracy of our information used to help guide all stakeholders in making the best possible decisions. Edison also works with different teams in identifying opportunities to improve the process we use to collect, share, and use information. 2.1.2 Chris Haid Chris Haid is VP of Data Analytics at IDEA Public Schools, where leads research, evaluation, and data analysis across 160 schools in Texas and Louisiana. Previously he was KIPP Chicago’s Chief of Staff, leading progress monitoring, strategic planning, data analysis, and technology planning. He also supported external affairs, growth strategy, communications, and development. Other roles he held at KIPP Chicago included Chief Information Officer and the Director of Research and Analysis. Chris has held academic positions at Yale University, New York University, and the University of Chicago. He has taught graduate and undergraduate courses in social scientific theory, statistical analysis, game theory, international development, and globalization. Chris attended the College of William and Mary where he earned a BA in Economics and the University of Chicago earning both a MPP and a MA in Political Science. 2.1.3 Mishan Jensen, PhD Dr. Mishan Jensen is a Statistician on IDEA’s Research and Analysis Team with 4 years of experience in program evaluation in a large urban school district and 2 years of experience as a Statistician for the Texas Department of Public Safety. She holds a Master’s and Doctoral degrees from the University of Texas at Austin in educational psychology, with a specialization in Quantitative Methods. Previous work included evaluating multilingual education, teacher appraisal programs, and integration of arts into the classroom. During her time at UT, Mishan assisted with the development of the College of Education’s first fully web-based Introduction to Statistics course, as well as provided assistant teaching support for most of the graduate level statistics courses offered. Recent work at IDEA has included Student Persistence, Enrollment, Teacher Career Pathways and Advanced Placement, focusing on impact analyses and predictive modeling. 2.1.4 Steven Macapagal Steven Macapagal (Mac) is a statistician with IDEA Public Schools on the Research and Analysis team and has taught 6th through 12th grade. Most recently, he was a middle school math teacher and grade team leader at IDEA Bluff Springs College Prep in Austin, TX and prior to that, started his career at IDEA Frontier College Prep in Brownsville, TX as a 2014 corps member with Teach For America. Throughout his time at IDEA Frontier, he taught AP and IB math and economics, served as 12th grade team leader, and witnessed IDEA fulfill its promise to his 5 senior classes, all of whom achieved 100 percent matriculation to college. He earned his BA in economics and mathematics at The University of Texas at Austin and is currently working on his MS in statistics at Texas A&amp;M University. His recent projects have centered on instruction and curriculum, college success, DEI, and Camp Rio. 2.1.5 Maura Carter Maura Carter is a Statistician on the R&amp;A Team at IDEA, and a former mathematics educator and university instructor in the Statistics department at University of California, Santa Cruz. She holds a master’s degree from University of Notre Dame in nonprofit business management, and a master’s degree from California State University in Statistics and Data Science. She enjoys volunteering, mentoring students in STEM topics, ocean science, and scuba diving. Maura supports IDEA by providing analytical and statistical support to a variety of teams and decision makers across the organization. 2.1.6 Aline Orr, PhD Dr. Aline Orr has over 20 years of combined experience in research and program evaluation. She holds a Master’s degree in educational psychology and quantitative methods from the University of Texas at Austin and a PhD in Neuroscience from the University of Pittsburgh, where she focused on brain pathways underlying emotional responses. In the recent past, Aline has led the evaluation of educational programs serving elementary and secondary students in a large urban public school district. Before that she was conducting educational research for a large assessment and textbook publishing house. Aline joined IDEA’s Research and Analysis team as an evaluator in 2021. In this position, she supports the evaluation of initiatives associated with the Charter School Program grants, the 21st Century Afterschool program, and the Camp Rio program. 2.1.7 Ilissa Madrigal Ilissa Madrigal is an Evaluator on the R&amp;A team, often assisting with qualitative methods and frequently partnering with HA and AST teams. Recent work includes providing thematic analyses for GPTW open-ended questions, conducting focus groups on Core Values, designing training evaluation surveys, and providing evaluation support for the Teacher &amp; School Leader Incentive Program grant. Before working at IDEA, Ilissa taught high school chemistry in the RGV before earning her master’s in Industrial-Organizational Psychology from the University of Tulsa. She holds a BS in Psychology and a BA in Biology from The University of Texas at Austin. 2.1.8 Marlena Coco, PhD Dr. Marlena Coco is an Evaluator on the R&amp;A Team with over 15 years of experience in education and higher education. She earned a PhD in Higher Education Leadership and Research Methodology at Florida Atlantic University, MA in College Student Development at St. Edward’s University, and BS in Psychology at UT-Austin. She teaches undergraduate and graduate courses, chairs doctoral students dissertation research, reviews articles for journal publication, provides education consulting services, and conducts strengths-based leadership development. In previous roles, she led research, evaluation, and data support for college and career readiness at a large, urban school district, implemented district wide surveys, and analyzed postsecondary retention and academic support data at multiple universities. She evaluates Charter School Program grants. "],["the-data-warhehouse-and-how-to-access-it.html", "Chapter 3 The Data Warhehouse and how to access it 3.1 The Data Warehouse 3.2 Accessing the data warehouse from R 3.3 Key tables", " Chapter 3 The Data Warhehouse and how to access it The first section that follows provides lots of context about IDEA’s data warehouse. If, however, you are in a rush and want to go about accessing the data warehouse, then skip to section @ref(#accessing-the-data-warehouse-from-r) on accessing from R below. 3.1 The Data Warehouse The ‘data warehouse’ is actually a collection of disparate databases hosted by IDEA Public Schools. Some databases are used to host the original source data, some are used as the data model for production applications, while others site between those two points. While the IDEA has ~28 servers hosting SQL Server databases, R&amp;A typically only accesses 9 servers hosting 96 databases and 2,718 distinct tables. It’s a lot, for sure. 3.1.1 Core data warehouse server configuration The servers are not islands unto themselves, but rather form a small economy of data flows, where data moves from source servers, to production servers (i.e., the sweetspot for us) to reporting servers that host transformed data for our Logi-powered Locus dashboards. Figure 3.1: Data in the warehouse is initially stored in ‘src_` databases in ’DS’ servers and flows to the right towads Logi hosted Locus dashboards. Figure 3.1 shows the flows of data in the warehouse, which essentially move from left to right. TheDS-* servers host databases holding source data. That data is processed and saved in databases in the two production servers (PROD1 and PROD2). These two production servers host most the data that R&amp;A uses in analysis, but know where the source data comes from in is often helpful. Some data is (ostensibly) snapshotted and stored in the two *-HIS servers. To be sure, as of March 2020, R&amp;A doesn’t know much about these servers. The reporting servers—RS1, RS2, and RS-HIS, serve transformed flat files that serve as the data layer for Logi, which serves up Locus dashboards on the hub. The following sections provide some details on the what databases might be found on each server. 3.1.1.1 DS Servers NOTE: the DS servers have names that do not correspond the the servers’ URLS. Why this is the case is not clear, but this is something to pay attention to when work with data from these sources DS-PS is hosted on 887192-SQLDS and serves as a source for PROD1. Most the data hosted on DS-PS mirror PowerSchool’s Oracle back-end (espececially the Schools, ADA, and Enrollment databases), but it also has data from Teams Insights, staffing, college applications and outcomes, student persistence, among other things. Databases: ADA Attendance BlendedLearning CNP Colleges Enrollment Finance Insights: data from Microsoft Teams Persistence Schools: key data from PowerSchool Schools.DL Staffing Survey Sources: PowerSchool Other undefined sources Targets: Prod1 DS-III is hosted on 1044407-SQLDS2 and serves as a source for PROD2. (note that this is a bit confusing . . . which makes this writer think we may have this wrong; why would something called DS-III have server named DS2 that mostly provides source data to PROD2?) The data hosted on DS-III is almost exclusively source data from testing platforms like Illuminate DnA, AP, IB, ACT, and SAT. Databases: SRC_IA SRC_IB SRC_AP SRC_LEAP SRC_EA SRC_Dibles SRC_NWEA SRC_Plan SRC_SAT SRC_Telpas SRC_ACT SRC_Explorer Sources: See the database names just above PROD1 for lookup data about students and schools Targets: Prod2 DS-OT is hosted on 964592-SQLDS and serves as a source for PROD1. Databases focus on source data from variance external systems, including Naviance, Tyler Munis, Teachboost, Schoolmint, and a slew if individualized learning/blended learing platforms. Databases: SRC_Zendesk SRC_Naviance SRC_StMath SRC_DreamBox SRC_IHT SRC_TylerMunis SRC_TeachBoost SRC_StaffRetention SRC_SchoolMint SRC_Recruitment SRC_OpsCampusRanking SRC_JobVite SRC_HR SRC_GetRating SRC_ConnerStoneEvaluations SRC_BlendedLearning IdeaInstructionReporting Sources: See the database names just above PROD1 for lookup data about students and schools Targets: Prod1 3.1.1.2 PROD Servers PROD1 is hosted on 1065574-SQLPRD1 and contains a variety of data bases sourced from DS-PS and DS-OT. The key studetns, schools, and regions data is found here and is infact used by DS-III and DS-OT. Microsoft Insights data is now found here as well. This is the where transformed data is stored that is moved to RS2 (again, it is not clear why the server number doesn’t quite line up as we move from initial DS to PROD to RS). Databases: ADA Attendance BlendedLearning CNP Colleges Enrollment Finance Insights: data from Microsoft Teams Persistence Schools: key data from PowerSchool Schools.DL Staffing Survey Sources: DS-PS DS-OT Targets: RS2 ST-HIS FO1 PROD2 hosts all other data that is not on PROD, which is to say data initially stored in DS-III. This is the source server for RS1 and includes assessments (e.g., IAs, bi-weekly and unit assessments, AP, IB) data and accountability tables. Databases: Assessments Schools : this actually contains data \"inventories at the school level, like lists of employees, equipment and asset inventories, students schedules, correspondences between teachers and students. 3.1.1.3 RS and Hisotrical data Servers R&amp;A tends not to use these tables very often. RS1, RS2, and RS-HIS all have flat tables that utilized by by Logi for Locus Dashboards that are developed by Software Development. ST-HIS ostensibly has snapshots of data from the PROD servers, but that is just an unverified hunch ST-HIS Databases: unknown Sources: PROD1 PROD2 something called ST1 and ST2`? Targets RS-HIS RS1 Databases: unknown Sources: PROD2 something called ST2? Targets Logi RS2 Databases: unknown Sources: PROD1 something called ST1? Targets Logi RS-HIS Databases: unknown Sources: ST-HIS Targets Logi 3.1.2 Server, Table, and Field Lookup Table 3.2 provides details on servers, databases, tables, and fields. While it is not complete, it does cover the majority of our data infrastucture. Figure 3.2: Data warehouse details 3.1.3 Databases that R&amp;A frquently uses. : List of Windows Servers hosting SQL Server DBs and their IP Addresses 3.2 Accessing the data warehouse from R The most straightforward way to access the data warehouse is to use our own, bespoke r package: ideadata, which is maintained on github 3.2.1 Installation Since ideadata is an internal IDEA package there is only a development version, which is installed from GitHub with: #install.packages(&quot;remotes&quot;) remotes::install_github(&quot;idea-analytics/ideadata&quot;) #renv::install(&quot;idea-analytics/ideadata@main&quot;) also works 3.2.2 Example Here’s how you connect to the Schools table in the warehouse. library(dplyr) library(ideadata) schools &lt;- get_schools() glimpse(schools) The schools object above is tbl object. That means it works with dplyr verbs and functions, but what happens in the background is that dplyr and dbplyr generate SQL that is sent to the database you are connected to and all that computation (e.g., filtering, selecting, joining, calculations, aggregation) are completed on the remote SQL Server instance and not on your computer. Nevertheless, you will eventually want to pull that data down onto your machine when you want to use R or Python do what they can do (like modeling or graphics) that the database can’t do. Pulling that data down is easy with [dplyr::collect()] library(dplyr) schools_df &lt;- schools %&gt;% collect() %&gt;% janitor::clean_names() (Here janitor::clean_names() snake_cases all the column names). 3.2.2.1 What if I am pulling down lots of data (say, millions of rows)? In this instance the database connection may fail. It’s not ideal, but it happens. One way to deal with this is to pull down the data piecemeal. The collector() function in ideadata makes this task trivial. It takes column names as arguments (unquote) from the table you want to pull down and those columns are used to break up the data into smaller sets of data that are pulled down from the database onto your computer and then recombined into a single table. schools_df &lt;- schools %&gt;% collector(SchoolState, CountyName) %&gt;% janitor::clean_names() The ideadata package is clever in that it updates its knowledge of the data warehouse every time you load the package with library(ideadata). However to do that you need to have access to the the warehouse, and that requires that you are on the VPN/behind the firewall. Do makes sure that is true before invoking the package! 3.2.3 Finding things in the data warehouse The ideadata package has a function that will open up a sortable, filterable, and searchable table in the RStudio idea: view_warehouse_meta_data() 3.2.4 Where to learn more ideadata has it’s own documentation here. If you can’t find an answer there, then reach out to Chris Haid (he’s responsible for this craziness). Go to this article to set up your credentials in R so that the warehouse can authenticate and authorize you as verified user 3.3 Key tables This section details important tables. In general you want to stick with databases PROD1 or PROD2 but your mileage may vary. Another important aspect of the warehouse is that the sources system for most student data is our SIS: PowerSchool. The PowerSchool Tables Data Dictionary is a helpful resource in understanding fields and relationships between tables. "],["projects.html", "Chapter 4 Projects 4.1 Version control with Github 4.2 Project Process 4.3 ProjectTemplate (for analyses) 4.4 The renv package: Ensuring reproducibility 4.5 Publishing Projects", " Chapter 4 Projects Some significant applications are demonstrated in this chapter. 4.1 Version control with Github All analysis projects need to be saved via Git (on your local computer) and pushed to Github. Doing so has several benefits to both you, to your future self, and to your teammates: Since Git is a version control system, you get to save and track changes in your work (data, source code, reports, PowerPoint decks, Shiny dashboards) incrementally. Incremental saving means you can recover from any accidental plunders. It’s like Track Changes in Word, but for multiple files and folders. Spill a Diet Coke on your laptop in the middle of a big analysis? No big deal (if you’ve been pushing commits to Github, it’ll all be there!) Collaboration is much more structured, with powerful tools for asynchronous work and managing versions. Referencing and reviewing code, tracking issues, and sharing what you’ve done is seamless, which means … Your work will be reproducible: anyone from R&amp;A can pull your repo from Github, run your analyses, add to or edit what you’ve done, and share those changes back in a way that is communicative and documented. setting up web documentation for any R packages you build become seamless. But enough on the why let’s get to how (if you do want to know more on the why, check out this excellent article by Jenny Bryan) 4.1.1 Getting Started with Git, Github, and RStudio Here’s a quick overview of what you’ll need to do, with details to follow: Dedicate a directory (a.k.a “folder”) to it. Make it an RStudio Project. Make it a Git repository. Go about your usual business. But instead of only saving individual files, periodically you make a commit, which takes a multi-file snapshot of the entire project. Push commits to GitHub periodically. This is like sharing a document with colleagues on OneDrive or DropBox or sending it out as an email attachment. 4.1.1.1 First steps These steps are borrowed with some light editing from Happy git with R by Jenny Bryan. Register for GitHub account. Install or update R and RStudio Install Git Those on Windows will want to do these steps as well Introduce yourself to Git. Prove local Git can talk to GitHub. Cache your username and password so you don’t need to authenticate yourself to GitHub interactively ad nauseum. Create and save a GitHub Personal Access Token (PAT). Prove RStudio can find local Git and, therefore, can talk to GitHub. 4.1.2 Feature Branch Worklow There are many workflows using Git and remote repositories like Github. All of thenm boil down to the following steps: Pull or fetch or clone a repo on Github to your local machine. If you are starting a new project, then you’ll need to create a new repo on Github (but you can also start one on your machine). This is usually called the main (formerly master) branch. Create a new branch that you will work on. Do some analysis, coding, writing. Periodically save a snapshot of your entire project (all the files and folders, except those that you explicitly ignore). This is called *committing changes**. Every once ins while push your commits to the remote repo. Congrats! You’ve just backed up your remotely and made it easy to share. Merge your new analysis and code back into the main branch. This is usually initiated by something called a pull request (which is admittedly a little confusing). The specific workflow we use on IDEA’s R&amp;A team is the Feature Branch workflow, which has the benefit of being both simple, while minimizing merge conflicts. The core idea behind the Feature Branch Workflow is that all feature development should take place in a dedicated branch instead of the main branch. This encapsulation makes it easy for multiple analysts to work on a particular analysis without disturbing the main codebase. It also means the main branch will never contain broken code. Moreover, it means you’ll be more likely to get a second or third set of eyes on our analysis. This makes your work more transparent, helps enforce coding standards, and helps spread all the cool new techniques you’ve implemented in your analysis. So what does this look like? Well, here’s a picture of the feature branch workflow in use for this manual: This picture shows the development of this manual over time (from left to right) as rendered by Github’s network diagram: it includes new branches being created, commits being made and merges back into the main branch. The black line is the main branch and includes the most up-to-date, “official” version of this book. The green and blue lines are feature branches, which diverge from the main when you checkout a new branch. The dots represent commits. Colored lines returning to the main branch indicate a merge: the new code is now part of of the main branch. You might be wondering what the unmerged yellow line labeld gh-page represents. That is a special branch that is used by Githbub Actions that uses the concept of continuous integration/continuous to build the website that hosts this manual. You don’t need to worry about that one; it’s simply used to build out the site magically. 4.1.3 Example worflow with this manual, or getting your feet wet This section is going to walk you through how to use git/github by updating this manual. You’ll (i) clone the Github repo locally on your laptop, (ii) create a feature branch, (iii) make some changes to this documentation, save those changes, and then commit those changes git (i.e., locally take a snapshot), (iv) push those changes (including all of your commits) up to the Github repo, (v) initiate a pull request (i.e., ask to merge your branch into the main branch), and finally (vi) merge your changes into the master branch. But first things first: Verify you did the initial set-up steps above Get your bio ready **Note that throughout the steps below I’ll show you how to each step Ok. Your ready? Great! Here we go. 4.1.3.1 Get the R_and_A_Manual repository URL Go to R_and_A_Manual repo in your browser. On the main page for the repo click the green Code button, Click on HTTPS (the default), and click the clipboard to copy the repo’s URL: 4.1.3.2 Clone the repo Now you’ll pull the remote repo from Github onto you computer. You’ll want to think about where you want to save this. For example I save data analysis projects in seperate folders under a Data_Analysis/ folder. I save the manual just on my one drive. 4.1.3.2.1 Comand line Here I’m saving this to temporary space, by navigating to ~/tmp/~ and then cloning the data: cd ~/tmp/ git clone https://github.com/idea-analytics/r_and_a_manual.git This will pull down all the content of the repo: files, folders, all commits, all branches. Really the whole kit and kaboodle. 4.1.3.2.2 RStudio Here’s how you do it from RStudio: In RStudio, start a new Project: File &gt; New Project &gt; Version Control &gt; Git, or click on project icon in the upper right-hand corner of the IDE and select New Project…. In the “repository URL” paste the URL of your new GitHub repository. That is: https://github.com/idea-analytics/r_and_a_manual.git Be intentional about where you create this project. You should click “Open in new session”. Click Create Project to create a new directory, which will be all of these things: a directory or “folder” on your computer a Git repository, linked to the remote GitHub repository an RStudio Project Cool. You should now have the R&amp;A Manual files on repo history on your computer! 4.1.3.3 Checkout a branch Before you start doing anything you should check out a branch. A branch is like your own, tempory, disposable workspace. When you checkout a branch you create a new copy of the the repo and changes you make only happen on the branch. When your happy with the changes and are ready to share them you’ll to a pull request. But we’ll get to that below. 4.1.3.3.1 Command line It’s pretty straightforward. You create the branch, by giving a short but meaningful name, and then check it out. git branch update-bio-cjh git checkout update-bio-cjh Or you can do both of those moves in one line by using git checkout with the -b flag: git checkout -b update-bio-cjh 4.1.3.3.2 RStudio Click on the Git panel (usually in the upper right on that standard RStudio layout, but YMMV if you’ve customized your layouts). Click on the purple “branch” icon (it kinda looks like a piece of a flowchart). Giving a short but meaningful name (something like, update-bio-cjh). Make sure the Sync branch with remote checkbox is selected; this will save you a step later when you push you changes up to the repo. 4.1.3.4 Making changes and saving them You now on a new branch and go go makes some changes. Go ahead and open 02-Who_We_Are.Rmd file and add your name as a section, update your bio and save it, as you usually would Now you’ll want to commit those changes, which takes a snapshot of the current state on the branch you are working on. 4.1.3.5 Command line after saving you’ll run the git commit command with the -a (adds all changes) and -m (add commit message) flag with a short description of what you did. git commit -a -m &quot;Updated Chris&#39;s bio&quot; You should do this often. After a while you’ll want to push your changes up to Github (frequently, but not as often as commits): git push You’ve likely not yet defined where this remote branch should go, but git will give you a helpful error which gives you the command for syncing your local branch with a new remote branch.: fatal: The current branch update-bio-cjh has no upstream branch. To push the current branch and set the remote as upstream, use git push --set-upstream origin update-bio-cjh Go ahead and copy and run that command. git push --set-upstream origin update-bio-cjh After that you can just use git push and you’ll branch changes will be saved remotely. 4.1.3.6 Maintaining large files If you have a file that is over 100 MB in size (e.g. a PowerBI dashboard), GitHub will block your commit (or at least send you a warning). There are two ways of approaching this issue: Use the large file storage (LFS) extension to commit the file. Add the file to .gitignore so the file is not committed. 4.1.3.6.1 Large file storage (LFS) Scenario: Suppose you have a PowerBI file Persistence_Dashboard.pbix that is 137 MB in size, so it cannot be handled through normal a Git workflow. You will need to download the LFS extension from this site: https://git-lfs.github.com/. Once you download, then you will follow these steps: Open the Git command line. (I have not found a solution through RStudio or GitHub Desktop). Switch to the repo and branch that has the large file. Install the LFS extension in the correct repo and branch using git lfs install Track the file extension (in our scenario, .pbix), using git lfs track &quot;*.pbix&quot; Track .gitattributes. git add .gitattributes N.B. Do not complete steps 3, 4, and 5 until after you have switched to the correct repo and branch. 6. Commit the file and push normally. When you commit, you will see a message indicating the LFS extension was used on that file. 4.1.3.6.2 Using .gitignore Alternatively, you could choose not to commit the file and just keep the large file locally. (This is a good option if the file can be generated through code, like a .csv output). You will follow these steps: If the file does not already exist, create a text file called .gitignore in your repo. Add the name of the file you wish to ignore in the .gitignore file (e.g. .Rdata is ignored when using ProjectTemplate). Save .gitignore and commit. 4.1.3.7 RStudio In your git panel you see changed (or new) files show up. You’ll want to select the check box for any file that’s been modified (indicated by an M) or that needs to be added (indicated by an A). Doing so readies the file to be updated in the commit: Click the commit button and new dialog box will open, which will show any changes you’ve made in an y file. Select the check box for staged, if isn’t already selected, add a commit message and click Commit When your ready to save those to the repo, simply press the Push button. 4.1.3.8 Merging changes. Merging changes in your feature branch with the main branch requires you go to Github and to a pull request. A pull request is essential asking the main branch to “pull” in your changes and is technically known as a merge.. So here are the steps. Go to the repo (https://github.com/idea-analytics/r_and_a_manual). You may see an info box suggesting you can merge your branch. If so, click on the Compare &amp; pull request button. If not select your branch and click the Pull Request icon. If you are able to merge (you’ll know) click the Create pull request button. Ask someone to review you request (ideally) Click the Merge pull request button and confirm the merge. If you are done with our feature branch feel free to delete it. You done! You’ll want to be careful here if you are working with others. If you pulled your main branch down a while ago there is a risk that the main branch on your laptop is not up-to-date with the main branch on Github (because others have merged changes there). The best remedy is to checkout and pull main—which gets up to date—and then checkout your branch and run git merge main. You may have to resolve conflicts. 4.2 Project Process When we are working on a project, we need to document not only our analytical work, but also our organization and management. There are multiple tools we use to plan, track, modify and evaluate our progress among our team and with our stakeholders, and as we evolve, we certainly can add to our best practices for each of these tools. Namely, we will discuss how to set up Wrike projects, set up a GRPI/RASI, use operating mechanisms, and other important project structures. The key point is that we backwards plan from our final product. The project management tools document these processes in greater detail, in the same way that a teacher might backwards plan from the assessment to the lesson, or a leader might document and follow up with the next steps from a meeting. Below is an overview of the backwards planning process. 4.2.1 Wrike We use Wrike as our main project management system to track the workflow for our team. All information requests (note - this link may change as we transition to the IST team) generate a Wrike task, which will be accepted or rejected from our work. Once we know who will be assigned to the request, we can use Wrike in the following ways to organize the work. 4.2.1.1 Onboarding and setup During tactical or throughout the week, review any new Wrike requests. Determine if the request will be accepted, and if so, determine who will be assigned to the task. Evaluate the scope of the request with your team AND the requester. Vet the request thoroughly and ask clarifying questions with the requester. If this is an ad-hoc request, keep the request in the original queue and set up tasks (see next section). If this is a medium- to long-term request, then set up a project and related tasks into an appropriate folder: Evaluation Projects Impact Analyses N.B. For medium- to long-term requests, you should consider establishing a GRPI and a RASI for your stakeholders, so that you have a common document for the scope and progress of the work. 4.2.1.2 Goal setting and deadlines Once your Wrike project or task has a page, then you should start backwards planning from the deliverable. Ask yourself: What is the goal of the project? (document it!) What is the deliverable for the project? (document it!) When is that deliverable due? (document it!) Assign the goal and the deliverable to the task or project. You can create a task/subtask for the deliverable and assign a due date. You can use the project/task description, leave a comment, or create a task/subtask for the goal. Ex. “Deliver slide deck and final report to Dolores” could be the deliverable, and “2021-12-17” could be the due date. 4.2.1.3 Defining tasks and dependencies Now, you need to accomplish those goals and produce the deliverable. Ask yourself: What steps do I need to produce the deliverable? What is the order in which I need to do those steps? Do certain steps require other tasks to be completed first? Create tasks/subtasks for each step. Tasks could refer to a group of related steps that could be accomplished over a fixed period of time. Subtasks could be specific, bite-sized steps that could be accomplished in a single day. Ex. “Make visualizations of data” could be the task, and “Make time series plot of AP Spanish Language scores at Mission CP” could be the subtask. Create dependencies for relevant steps. Link the predecessors (i.e. the steps before) and the successors (i.e. the steps after) to the task/subtask. Establish due dates for ALL steps, including dependencies, subtasks, etc. 4.2.1.4 Documenting and evaluating progress towards goal Your project or task is ready-to-go! You still need to document your progress as you move forward. After a meeting, jot down key points and next steps as a comment, or create new tasks. Add comments after relevant tasks are completed. Include links to any completed products. Are you taking notes in a notebook, on a OneNote, or in a Teams chat? Capture the links to all your relevant documents, GitHub repos, notes, pictures, etc. in a relevant task, as a commment, or in a description in the Wrike project! You’ll be able to find things you forgot about months later. If you used a GRPI/RASI for the project, be sure to update your stakeholders, as well. Include a link to the GRPI/RASI in the Wrike project. When you update Wrike, update the GRPI/RASI. Your GRPI/RASI should succintly summarize what you are doing in the project. Things might change in your project. Each week, ask yourself: Did the scope of the work change? Did the timeline of the project change? Did I realize something else needed to be completed?, and so on If you need to modify the project, carve out time to do so. Evaluating your progress will help keep yourself on track, and will alert others that the project may take longer or may end differently than originally planned. And of course, document it! 4.2.2 GRPI and RASI 4.2.3 Project operating mechanisms 4.3 ProjectTemplate (for analyses) ProjectTemplate is both an R package and an approach. Simply put, it’s a package that builds the scaffolding for a project, provides good features for tracking package dependencies (though we also use renv for that), and separating data loading and prep from analysis and output, as well as caching of long running processes, which speeds up analytical time. Is it perfect? No. It has more features than we’ll likely ever use (logging, code profiling, and unit testing). IF you are new to ProjectTemplate the Getting Started Tutorial is great. Indeed, it’s always good to read the docs Seriously, though. The ProjectTemplate documentation is great and you should really set aside an hour to read through it. This manual will provide a very abbreviated overview of how to use ProjectTemplate at IDEA, but will not be a stand in for the official documentation. 4.3.1 Installing ProjectTemplate Installing the package is straightforward install.packages(&quot;ProjectTemplate&quot;) 4.3.2 Creating a project with ProjectTemplate Creating a project is pretty simple. Navigate to where you typically save projects vai RStudio and then run the following commands library(ProjectTemplate) create.project(&quot;learning-pt&quot;) Doing that bestows upon you the following directory structure: ProjectTemplate Directory Structure You’ll notice in the image above that there is .Rproj file, which isn’t added by ProjectTemplate. I created an Project in RStudio first and then navigated to that direction and ran the following create.project(\"../learning-pt/\", merge.strategy = \"allow.non.conflict\"), which allows you to scaffold ProjectTemplate in an existing directory while ignoring any existing files and folders (usually *.Rproj and .git). 4.3.3 What goes where We only use about half of the directories that ProjectTemplate Scaffolds. Here’s what we use ,in the order it’s evaluated by ProjectTemplate when you run load.project(): config This directory contains the configuration file global.dcf. This is the first thing that load.project() looks at. The fill list of what each setting does is here. Here are a few highlights, in thier order of importance. load_libraries: This can be set to ‘on’ or ‘off’. If load_libraries is on, the system will load all of the R packages listed in the libraries field described below. By default, load_libraries is off. I highly recommend that you turn this on ‘libraries’: This is a comma separated list of all the R packages that the user wants to automatically load when load.project() is called. These packages must already be installed before calling load.project(). By default, the reshape2, plyr, tidyverse, stringr and lubridate packages are included in this list. I recommend dropping these and only adding packages you are using for the project. data_loading: This can be set to ‘on’ or ‘off’. If data_loading is on, the system will load data from both the cache and data directories with cache taking precedence in the case of name conflict. By default, data_loading is on. cache_loading: This can be set to ‘on’ or ‘off’. If cache_loading is on, the system will load data from the cache directory before any attempt to load from the data directory. By default, cache_loading is on. munging: This can be set to ‘on’ or ‘off’. If munging is on, the system will execute the files in the munge directory sequentially using the order implied by the sort() function. If munging is off, none of the files in the munge directory will be executed. By default, munging is on. as_factors: This can be set to ‘on’ or ‘off’. If as_factors is on, the system will convert every character vector into a factor when creating data frames; most importantly, this automatic conversion occurs when reading in data automatically. If ‘off’, character vectors will remain character vectors. By default, as_factors is off. *This is a very good default, and the opposite of base R. lib: this directory is used to house helper functions. You can store them in an *.R file (like the included helpers.R file). ** As a general rule of thumb, if you’ve copied-and-pasted a block of code twice, don’t do it a third time; rather, abstract that block into a function and save it in this directory. load.project sources the files in this directory after readying global.dcf. cache: Here you’ll store any data sets that (i) are generated during a preprocessing step and (ii) don’t need to be regenerated every single time you analyze your data. You can use the cache() function to store data to this directory automatically. Any data set found in both the cache and data directories will be drawn from cache instead of data based on ProjectTemplate’s priority rules. ProjectTemplate always checks this before running code in the data/ and munge/. directories. You can learn more about caching here. data: You store your raw data files here. If they are encoded in a supported file format, they’ll automatically be loaded when you call load.project(), *unless cached versions of their output exist in the cache/ directory. munge: Here you can store any preprocessing or “data munging” code for your project. For example, if you need to add columns at runtime, merge normalized data set,s or globally censor any data points, that code should be stored in the munge directory. The preprocessing scripts stored in munge will be executed in alphabetical order when you call load.project(), so you should prepend numbers (to digit like 01-aggregate_schools, 02-calc-means, …) to the filenames to indicate their sequential order.. Files in here are run after those in cache/ and data/ are loaded. reports: Here you can store any output reports, especially RMarkdown reports, that you produce. This is where final reports live ProjectTemplate doesn’t always play well with RMarkdown (well, really the issue is with knitr. The biggest issue is running load.project() inside of RMarkdown. ProjectTemplate will complain that your reports/ directory is not a ProjectTemplate directory, which is annoying. It’s easily fixed with this line of code at the top of your RMarkdown and you are in Rproject: setwd(here::here()); load.project() This will change the working directory to the top of project and then run load.project. After the project is loaded knitr will set the working directory back to reports/, so all other file references should be relative to reports/ src: Here you’ll store your statistical analysis and mahcine scripts. You should add the following piece of code to the start of each analysis script: library('ProjectTemplate); load.project() (you don’t need the tip above here). You should also do your best to ensure that any code that’s shared between the analyses in src is moved into the munge directory; if you do that, you can execute all of the analyses in the src directory in parallel. A future release of ProjectTemplate will provide tools to automatically execute every individual analysis from src in parallel. You may want to cache() your results here as well. graphs: Here you can store any graphs or PowerPoints that you produce, with the exception of those already contained in RMarkdown files. 4.4 The renv package: Ensuring reproducibility The renv package goes a long way to solving a pernicious problem: Dependency Hell. Here’s a good description of the problem: This problem most manifest when you revisit a recurring project a year later. You updated some year variables in your code and rerun your code and ready to pat yourself on the back and then BAM! Your code breaks. It breaks because over the previous 12 months the packages you relied upon a year ago have been updated over time and don’t work with other packages. Sometimes all you need to to is update all of your packages and everything is hunky-dory; but other times this brute force process doesn’t work. You are left trying to understand whats changed over multiple packages, what your need to change in your code and something you thought would take you an hour takes a a week, with most days feeling like your just banging your heard against the wall. This issues alone should be enough to convince you that your need to create reproducible environments. Another reason is related and often very immediate: working on the same analysis with someone else. Perhaps you are are fastidious and only use the most current version of every package and your partner is risk averse and only updates packages after they’ve been out in the wild for at least a year. If you are in this boat your going to have problems. Again, you need to create reproducible environments. This is a common technique in Python and there it goes by the name of virtual environments. renv is an R implementation of the concept that is pretty elegant Along with ProjectTemplate we use the renv package to ensure that our analysis projects are: Isolated: Each project gets its own library of R packages, so you can feel free to upgrade and change package versions in one project without worrying about breaking your other projects. Portable: Because renv captures the state of your R packages within a lockfile, you can more easily share and collaborate on projects with others, and ensure that everyone is working from a common base. Reproducible: Use renv::snapshot() to save the state of your R library to the lockfile renv.lock. You can later use renv::restore() to restore your R library exactly as specified in the lockfile. These three points are crucial for working well with others and protection your future self. As usual, you really should read the docs for renv. They are thorough and clear. There’s also an excellent FAQ 4.4.1 Setting up renv with a new project If you are starting a new project in RStudio then setting up renv is easy: just be sure to click the Use renv with this project. Definition of dependency hell Checking that box will add the bootstrap renv, installing the package if necessary as well as a few folders and files that serve as infrastructure 4.4.2 Setting up renv with an existing project If you’ve got a project that you want to starting using renv it’s pretty straightforward:. Install renv if you don’t have it: install.packages('renv') Use renv::init() to initialize a project. renv will discover the R packages used in your project, and install those packages into a private project library. It’s really that simple! 4.4.3 Using renv The renv workflow is super, duper simple, after you’ve set it up: Work in your project as usual, installing and upgrading R packages as required as your project evolves. Use renv::snapshot() to save the state of your project library. The project state will be serialized into a file called renv.lock. If you want to revert to the previous state of your project—i.e., you installed a new version of a package and it’s wreaking havoc on your project—simply use renv::restore(). Following these simple steps isolates your projects environment from the rest of your other projects. So downloading a bleeding-edge, dev version of package from GitHub b/c you need a new feature won’t pollute your other projects that are running just fine. This is the isolation bit mentioned in the three points above 4.4.4 Collaborating with renv The renv developers recommend the following steps when using renv in collaborative settings Use a git/GitHub repo. One user should explicitly initialize renv in the project, via renv::init() or when starting an RStudio project. Doing so will create the initial renv lockfile, and also write the renv auto-loaders to the project’s .Rprofile and renv/activate.R. These will ensure the right version of renv is downloaded and installed for your collaborators when they start in this project. Share your project sources, alongside the generated lockfile renv.lock via GitHub. Be sure to also share the generated auto-loaders in .Rprofile and renv/activate.R via the repo. When a collaborator first launches in this project, renv should automatically bootstrap itself, thereby downloading and installing the appropriate version of renv into the project library. After this has completed, they can then use renv::restore() to restore the project library locally on their machine. While working on a project, you or your collaborators may need to update or install new packages in your project. When this occurs, you’ll also want to ensure your collaborators are then using the same newly-installed packages. In general, the process looks like this: A user installs, or updates, one or more packages in their local project library; That user calls renv::snapshot() to update the renv.lock lockfile; That user then shares the updated version of renv.lock with their collaborators via github; Other collaborators then—after a git pull—call renv::restore() to install the packages specified in the newly-updated lockfile. A bit of care is required if collaborators wish to update the shared renv.lock lockfile concurrently – in particular, if multiple collaborators are installing new packages and updating their own local copy of the lockfile, then conflicts would need to be sorted out afterwards. Use teams to ensure any changes to renv.lock are communicated so that everyone knows and understands when and why packages have been installed or updated. For more information on collaboration strategies, please visit environments.rstudio.com. 4.4.5 renv and RStudio Connect In short there should be no issues, but there is one important thing to always keep in mind when publishing to RStudio connect: The renv generated .Rprofile file should not be included in deployments to RStudio Connect. 4.5 Publishing Projects Our reports can be published for our stakeholders through a variety of sites. The location is up to you, but there are some nice options to do so. 4.5.1 RStudio Connect If your report is written in RMarkdown, then you can directly publish your report from RStudio to RStudio Connect at our team site. To publish a report, do the following: Make sure your output in the header section is set to output: html_document. You can add other options, like a table of contents and formatting. Knit your .Rmd to find any compiling errors before you publish. Click on the blue Publish button in the upper right hand corner of the html output or in the editor. Select RStudio Connect. Choose if you want to publish with the source code or just publish the finished document. (The source code is useful if your report is recurring and can be updated, while the finished document is appropriate for things like analytical reports.) Select analytics.ideapublicschools.org as the destination. Then, choose an appropriate title for your report. Finally, click Publish. If the .Rmd filename is too long, then the report will not publish. You’ll need to shorten the filename if you get errors. Check your report on RStudio Connect and make adjustments under the Settings gear. A detailed guide about publishing can be found in the RStudio Connect documentation. ###Specific Projects ##Teacher Hiring Jobvite Data *Obtained from HA team. It is the data from the candidates who apply for IDEA jobs through Jobvite. Not all candidates applied through Jobvite in the past, but HA is trying to streamline the application process and require all applicants for all jobs (at all campuses) to apply through a central system, which is Jobvite. A particular Jobvite file is labeled with a year, such as “Jobvite 19-20”, and it will have column “Candidate Submit Date”, which is a range from October 2019 to July 2019 (October to July, proceeding the year the applicant will likely begin work). An applicant from that range will most likely have a Start Date of August 2019 (or sometime in the calendar year 2019, starting AFTER the 19-20 Academic Year). However, there are cases when applicants will, for example, apply during the 10/2018 - 07/2019 cycle (Jobvite 19-20), but will not have a Start Date until 2020 or 2021 (so they do not start the following FAll after the application cycle). This data has columns including Candidate Full Name, Candidate Email (personal email used for the application), and some of the pre-hire selection measures, such as GPA. However, all 5 of the pre-hire selection measures are not included yet (Stem Major, Teach For America status, Experience prior to IDEA, GPA, and Teacher Certification). The data also has no Employee IDs yet, as this is a list of candidates. There is a Hire Yes/No column, which has “Yes” for those candidates who were offered a position and “No” for those who were not offered a position. Also, there is a Requisition ID which is basically an application ID number, and it is unique per application. If a single candidate applies for more than one position, the candidate will have a unique ID for each application. Teacher Export Data ##Camp Rio ##TSLIP "],["coding-standards.html", "Chapter 5 Coding Standards 5.1 File naming standards.", " Chapter 5 Coding Standards We will be writing more here in the coming weeks, months, and years. But for all intents and purposes, we should follow the Tidyverse Style Guide, which is was originally derived from Google’s R Style guide, but in a an amazing case of an Ouroboros, is now informed by the Tidyverse Style Guide. 5.1 File naming standards. We will also come up with more here soon, but in the meantime, Jenny Bryan’s guidance is pretty excellent. You can read more here "],["visualization-standards.html", "Chapter 6 Visualization Standards 6.1 IDEA branding", " Chapter 6 Visualization Standards 6.1 IDEA branding When producing a visualization for presentation or publication, use the Brand Guideline standards established by the Marketing team. Many of these guidelines, particularly colors, have already been incorporated into an R package, ideacolors. 6.1.1 The ideacolors package The ideacolors package is used in conjunction with ggplot2 to produce graphs and visualizations consistent with the primary and secondary brand color schemes. For more information, consult the ideacolors documentation. To install the package, use the following code: # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;idea-analytics/ideacolors&quot;) 6.1.2 Typography The primary brand font is Proxima Nova; the secondary fonts are Arial or Helvetica. Before using the code, ensure that these fonts have already been installed. Then, to add these fonts to a visualization, use the following code: # install.packages(&quot;showtext&quot;) library(showtext) showtext_auto() font_add(&quot;proxima&quot;, regular = &quot;proximanova-regular.ttf&quot;, bold = &quot;proximanova-extrabold.ttf&quot;, italic = &quot;proximanova-regularit.ttf&quot;, bolditalic = &quot;proximanova-extraboldit.ttf&quot;) This code will apply the font consistently to all text in any visualization created for that R session. "],["measures.html", "Chapter 7 Measures 7.1 The Students Table 7.2 Student Persistence 7.3 College Application &amp; Matriculation Metrics 7.4 Critical Student Intervention (CSI) Identification 7.5 Assessment Data 7.6 Teacher, Staff, and Hiring Data", " Chapter 7 Measures Within this chapter we will find the measures IDEA Public Schools stakeholders are the most interested about. 7.1 The Students Table The Students table is probably the most important table of all. It contains historical student personal data since IDEA’s inception. We use this table to know to what subpopulation a student belongs to. Also, we can link this table to other student data using the [StudentNumber] field. Table: [1065574-SQLPRD1].[PROD1].[Schools].[Students] Main fields: [AcademicYear] [StudentNumber] | [SchoolNumber] | [GradeLevelID] | [EnrollmentStatus] | [Gender] | [SPED] | [ELLCode] | [PrimaryDisabilityCode ] | [SecondaryDisabilityCode ] | [TertiaryDisabilityCode ] | [EconomicDisadvantageCode] | [FederalHispanicFlag] | [FederaRaceI] | [FederaRaceA] | [FederaRaceB] | [FederaRaceP] | [FederaRaceW] |[EntryDate] | [ExitDate] | [RowIsCurrent] Who is what?: [EnrollmentStatus]: If EnrollmentStatus = 0, it means that the student is/was an active student for the corresponding academic year, If EnrollmentStatus = 2, it means the student left the district before completing the academic year, If EnrollmentStatus = 3, it means the student graduated from IDEA’s high school. [ELLCode]: English Learner (EL) A.K.A. Limited English Proficiency (LEP) If ELLCode = 0, student is not identified as an EL student, If ELLCode = 1, the student is identified as an EL student, If ELLCode not in (0,1), it means the student is not currently identified as an EL but was identified before as one, If ELLCode = “F”, the student is not identified as an EL student but is in the First Year of Monitoring status, IF ELLCode = “S”, the student is not identified as an EL student but is in the Second Year of Monitoring status, If ELLCode = 3, the student is not identified as an EL student but is in the Third Year of Monitoring status, If ELLCode = 4, the student is not identified as an EL student but is in the Fourth Year of Monitoring status, If ELLCode = 5, the student is not identified as an EL student but was at one point more than 4 years ago. EL Reclassification An EL who has passed the Texas English Language Proficiency Assessment System (TELPAS) will be reclassified as a Non-EL and monitored for 4 years. After reclassification the students ELLCode will not change to 0, it will move through the codes for the 4 years of monitoring status and then change to an ELLCode of 5 [SPED]: SPED students are composed of RISE and Life Skills students, not including Critical Student Intervention (CSI) students. If SPED = 0, it means the student is not identified as a SPED student, otherwise then SPED = 1. [InstructionalSettingCode]: Students who are classified as SPED may receive additional services. Their services can be found in the InstructionalSettingDescription column, which is linked to a specific PEIMS code: InstructionalSettingCode Description 0 No additional SPED services 1 Homebound 40 Mainstream 41 Resource Room/Services &lt; 21% 42 Resource Room/Services 21- &lt;50% 43 Resource Room/Services 50- 60% 44 Resource Room/Services &gt; 60% If InstructionalSettingCode = 43 or 44, the student is typically in RISE. [DisabilityCode] DisabilityCode Description 0 No disability 1 Orthopedic impairment 2 Other health impairment 3 Auditory impairment 4 Visual impairment 5 Deaf-Blind 6 Intellectual Disability 7 Emotional disturbance 8 Learning disability 9 Speech impairment 10 Autism 13 Traumatic brain injury 14 Noncategorical early childhood SELECT DISTINCT [StudentNumber], [PrimaryDisabilityCode] AS [DisabilityCode], [PrimaryDisabilityDescription] AS [DisabilityDescription] FROM ( SELECT DISTINCT [StudentNumber],[PrimaryDisabilityCode], [PrimaryDisabilityDescription] FROM [PROD1].[Schools].[Students] WHERE AcademicYear=&#39;2018-2019&#39; UNION ALL SELECT DISTINCT [StudentNumber], [SecondaryDisabilityCode], [SecondaryDisabilityDescription] FROM [PROD1].[Schools].[Students] WHERE AcademicYear=&#39;2018-2019&#39; UNION ALL SELECT DISTINCT [StudentNumber], [TertiaryDisabilityCode], [TertiaryDisabilityDescription] FROM [PROD1].[Schools].[Students] WHERE AcademicYear=&#39;2018-2019&#39;) A WHERE [PrimaryDisabilityDescription] &lt;&gt; &#39;&#39; ORDER BY StudentNumber [EconomicDisadvantageCode]: A.K.A ECD If EconomicDisadvantageCode = 0, then the student is not identified as economically disadvantaged, otherwise the student is identified as economically disadvantaged, If EconomicDisadvantageCode = 1, then the student is identified as an economically disadvantaged and eligible for free meals, If EconomicDisadvantageCode = 2, the student is also identified as an economically disadvantaged but only eligible for reduced-price meals, If EconomicDisadvantageCode = 99, it means the student is identified as an economically disadvantaged one but has another economic disadvantage. ECDFlag Consolidates the codes above to 0/1 indicator If ECDFlag = TRUE (1), the student is identified as economically disadvantaged, If ECDFlag = FALSE (0), the student is not identified as economically disadvantaged. Race / Ethnicity: Same like in TX If the student identifies as Hispanic or Latino then the student is classified as Hispanic/Latino, If the student does not identify as Hispanic or Latino and only selected one race the student is classified as that race, If the student does not identify as Hispanic or Latino and selected more than one race, then the student is classified as Two or more races. [RowIsCurrent]: Indicates the most current row for a student, Only to be used on the current school year 7.1.1 Common queries We frequently need to disaggregate data by year, region, school, and so on, while retaining basic information about a student. The Students table does not directly contain the school or region names, so we need to join the Schools table and Regions table. For example, the following are two queries to pull currently enrolled seniors: SQL example: SELECT [AcademicYear] ,C.[RegionDescription] ,B.[SchoolName] ,[GradeLevelID] ,[StudentNumber] ,[StudentFullName] ,[FirstName] ,[MiddleInitial] ,[LastName] --join Schools and Regions tables FROM [1065574-SQLPRD1].[PROD1].[Schools].[Students] AS A LEFT JOIN [1065574-SQLPRD1].[PROD1].[Schools].[Schools] AS B ON A.[SchoolNumber] = B.[SchoolNumber] INNER JOIN [1065574-SQLPRD1].[PROD1].[Schools].[Regions] AS C ON B.[RegionID] = C.[RegionID] --filter for currently enrolled seniors in 2021-2022 WHERE [AcademicYear] = &#39;2021-2022&#39; AND [GradeLevelID] = &#39;12&#39; AND [ExitDate] &gt; &#39;2022-05-01&#39; AND [RowIsCurrent] = &#39;1&#39; R example: get_students() %&gt;% #filter for currently enrolled seniors in 2021-2022 filter(AcademicYear == &quot;2021-2022&quot;, GradeLevelID == &quot;12&quot;, ExitDate &gt; &quot;2022-05-01&quot;, RowIsCurrent == 1) %&gt;% #join Schools and Regions tables inner_join(get_schools(), by = c(&quot;SchoolNumber&quot; = &quot;SchoolNumber&quot;)) %&gt;% inner_join(get_regions(), by = c(&quot;RegionID&quot; = &quot;RegionID&quot;)) %&gt;% select(AcademicYear, RegionDescription, SchoolName, GradeLevelID, StudentNumber, StudentFullName, FirstName, MiddleInitial, LastName) 7.2 Student Persistence Student Persistence is one of the most important measures that Chiefs, VPs, and School Leaders are always monitoring to understand our efforts in providing high quality services to our students and families. In order for a student to actualize the benefits of IDEA the student must persist through high school. We say a student persisted if the student was enrolled and attended school for the entire academic school year, returned the following school year, and was enrolled and attending on the Monday after the first week of school (the First Day of Persistence FDOP). In other words, the student was enrolled and attended school on the “First Day of Persistence (FDOP)” for two consecutive years. The only exception to this are New Students that enroll after the FDOP, attend the entire year, return the following year, and are enrolled and attending for FDOP. These new students were not enrolled and attending for 2 consecutive FDOPs because they enrolled after the first FDOP but would be considered as persisting. Example: John enrolled in IDEA Public Schools on August 19, 2019, and attended school on FDOP, which is the Monday after the first week of school. John attended the entire 2019-2020 academic year and was enrolled and attending on the FDOP of the following academic year (FDOP for 2020-2021 school year). If John was enrolled but did not attend on FDOP because he didn’t return until September, John would not count as a student who persisted. Database: [1065574-SQLPRD1].[Persistence] Main metrics: District, Regional, School, Sub-population, Grade Level General Formula: 1 – (Leavers/All Students) Tables: [dbo].[PersistenceCode]: This table is used to calculate the persistence rate for the current academic year. The data is updated every three hours, weekdays. LeaverWeek column shows the week the student left the district (beginning the week containing the FDOP, which is the second week of the academic year)? After the end of the current school year but before the persistence year has ended (the summer months), the week the student leaves the district is captured in the ULeaverWeek column. The PersistenceWeek column shows the current week of persistence for the school and region that the student attends (this week will be different across schoools and regions due to differing start dates and FDOPs). [dbo].[PersistenceHistorical]: This table contains the final persistence data for previous persistence years starting with 2018-2019 and up through 2020-2021. This table is updated with the previous years’ data after that year is closed out and the next persistence year has started. Context: Who Counts? To calculate persistence, we need to filter out all students who should be excluded from the denominator, filter(EXCLUDE == 0, FDOPCOUNTP == 1). This limits the denominator to only those students that should be counted. These two filters should be used when looking at current or historical data. Who is a Leaver? All leavers are coded as FDOPLEAVER = 1, if FDOPLEAVER = 0 then they are a current student. Other Useful Fields: AcademicYear: the academic year RegionDescription: the region the student is in SchoolShortName: the school the student attends SchoolType: Academy of College Prep GradeLevelID: Student’s grade level (integer ranging from -1 = Pre-K to 12) NewStudent: indicates if the student is new (1) or returning (0) PersistenceWeek: the current week of persistence based on the student’s region and school (ranges from 1-52) LeaverWeek: the persistence week the student left the district ULeaverWeek: the persistence week the student left the district if it was after the last day of school PersistenceCode: the reason the student left the district PersistenceCategory: the category that the reason falls into for why the student left the district PersistenceComment: any additional comments the SIS clerk has for why the student left the district EnrollmentStatus: contains the enrollment status, but is not accurate Do Not Use EntryDate: the date the student entered school for a particular academic year or the date the student started school at a new campus ExitDate: the last day of the academic year (last day of school) or the day the student left the district (summer leavers are sometimes snapped back to the last day of school, beware) Recapture: 0/1 indicator of students that have left and then returned to the district. 1 = recaptured, 0 = not recaptured. Not to be Trusted SummerLeaver: 0/1 indicator for students that leave during the summer StudentGender: “M”/“F” gender indicator Race: race indicator combined with ethnicity; ex. “WHITE-HISPANIC” IsHispanic: indicates whether the student identifies as Hispanic; values = “HISPANIC” or “NON-HISPANIC” Migrant: 0/1 indicator of student’s migrant status LEP: 0/1 indicator of student’s English Learner status. Inaccurate Don’t Use Sped: 0/1 indicator of student’s SPED status EconDisad: 0/1 indicator of student’s economic disadvantaged status. Inaccurate Don’t Use IsCSI: 0/1 indicator for students identified as CSI ContinuousEnrollment: 0/1 indicates whether the student has been continuously enrolled 7.2.1 Student Persistence using Attendance Tables (A.K.A. Cohort Persistence) Occasionally student persistence is requested for school years that are not captured in the Persistence Tables, i.e., persistence data prior to 2018-2019, or for cohorts of students overtime. When this occurs the Attendance tables can be used to “snapshot” whether a student is attending at a particular date at the beginning of the school year in question and is still attending at a particular date in the beginning of the following school year. This way of calculating persistence has its drawbacks, mainly that it is not as precise as persistence calculated with the persistence table, results will differ slightly depending on the “snapshot” date selected. Database: [1065574-SQLPRD1].[PROD1] Tables: [Attendance].[Students]: This table contains daily attendance data from 2017-2018 school year to present. To see what students are attending on a specific date use the AcademicYear column to select the specific school year and AttDate to select the date (in YYYY-MM-DD format). Be sure to filter before collecting the data due to the large size of the table, contains a row for every school day every student attended for all years contained in the table. [ADA].[StudentDailyMembership] This table contains daily attendance data for 2016-2017 and prior. Contains some of the same columns as the Attendance.Students table, such as AttDate, AcademicYear, and SchoolNumber. To see what students are attending on a specific day you would use the same profess described above. Other Useful Fields: AcademicYear: the academic year (in both tables) SchoolName: the long school name (in both tables) schoolnumber: the school number (in the StudentDailyMembership (2016-2017 &amp; prior) table it is SchoolNumber) SchoolShortName: the school’s short name (ex. Donna; Only in the Attendance.Students table - 2017-2018 to present) SchoolType: indicates Academy or College Prep (Only in the Attendance.Students table - 2017-2018 to present) RegionDescription: the region (Only on the Attendance.Students table - 2017-2018 to present) GradeLevelID: student’s grade level (in both tables) WeekNumber: the number of the week in the academic year (in both tables) Use Cautiously the weeks are NOT numbered properly in 2019-2020 contains in the Attendance.Students table AttDate: the date for the attendance record (format “YYYY-MM-DD”) (in both tables) Membership: indicates full-time = 1.0 or half-time = .5 (half-day pre-k students) SchoolTypeOperation: indicates if the school is “FULL SCALE”, “SCALING”, “or”LAUNCHING\" (Only in the Attendance.Students table - 2017-2018 to present) and applies to Academies and College Preps individually 7.3 College Application &amp; Matriculation Metrics To close the opportunity gap, IDEA Public Schools is committed to a vision of College for All Children. The College Success Team (CST) lead the initiative of monitoring and identifying the best College/University for each senior student, this is possible with data dashboards that use Naviance information. Database: [1065574-SQLPRD1].[PROD1].[Colleges] Main metrics: Percent at least 1 application, Percent at least 1 submission, Percent at least 1 acceptance Most important columns: [Stage] | [ResultCode] Tables: [Colleges]: Here we will find all colleges/universities. What makes this table important is the [CEEB] field which is an ID that can help us connect distinct college related data sources like Naviance with National Student Clearinghouse data! [CollegeTuition]: Besides having in-state tuition and out-of-state tuition data, we can also find two other unique codes ([OPEID] and [ACTCode]), that will aid us to college data across different data sources. [EDocs]: Details about application submitted e-documents. [StudentCollegeApplication]: This is the main table. We use it to calculate most of the metrics the College Application &amp; Matriculation dashboard has. We can find data from 2018 up to the current academic year. [StudentCollegeApplicationsSummary]: [StudentScholarships]: Context: Who Counts? Who is a 4year 2year? Persistence Rate by: District: use all data Region: use the [RegionDescription] field School: [SchoolShortName] + [SchoolType] Grade Level: [GradeLevelID] Student Type (new or returning): use [NewStudent] = 1 to identify new students, if zero then the student is considered as a returning student. Other Useful Fields: Use [PersistenceCode] or [PersistenceCategory] to know student and family reasons on why a student left the district. [LeaverWeek] to know the week when the student left the district. SQL Code Example: SELECT 1 - AVG(CAST([FDOPLEAVER] AS NUMERIC)) FROM [`1065574-SQLPRD1`].[Persistence].[dbo].[PersistenceCode] WHERE [FDOPCOUNTP] = 1 7.4 Critical Student Intervention (CSI) Identification To address the achievement gap among students, Dolores Gonzalez, our Chief Program Office introduced the Critical Student Intervention (CSI) program in 2014. Currently, along with her team, Tricia Lopez our VP of Special Programs lead IDEA Public Schools in the effort of reducing the performance gaps. To help Tricia’s team, the Software Development team put together a table where we can find the students who are enrolled in a CSI math and/or reading intervention program. Database: [1065574-SQLPRD1].[PROD1].[Schools] Main metrics: Percent of CSI students by school Most important columns: [ProgramID] | [InterventionType] Tables: [StudentCSI]: This table will aid us to identify the students who are/were enrolled in math or reading CSI intervention programs. Since sometimes there is need of data adjustments from one year to another, the [ProgramID] number might change. However, this should not be a problem because we can always use the [InterventionType] field to correctly associate an intervention program to either math or reading. SQL Code Example: SELECT DISTINCT AcademicYear, [Subject], ProgramID FROM (SELECT [AcademicYear] ,[ProgramID] ,CASE WHEN [InterventionType] LIKE &#39;%math%&#39; THEN &#39;Math&#39; WHEN [InterventionType] LIKE &#39;%reading%&#39; THEN &#39;Reading&#39; ELSE NULL END AS [Subject] FROM [1065574-SQLPRD1].[PROD1].[Schools].[StudentCSI] WHERE [AcademicYear] = &#39;2018-2019&#39; AND (InterventionType LIKE &#39;%math%&#39; OR InterventionType LIKE &#39;%reading%&#39;)) AS A GROUP BY AcademicYear, [Subject], ProgramID [StudentCSIDetails]: Here we find detailed information about the students who are or were part of a CSI program. We can either use this table to get a student count per school, or we could match the [StudentCSI] table data to the Students table and get the percent of CSI students per school. SQL Code Example: SELECT A.StudentNumber, A.SchoolNumber, B.SchoolName, D.[Subject] FROM [1065574-SQLPRD1].[PROD1].[Schools].[Students] AS A INNER JOIN [1065574-SQLPRD1].[PROD1].[Schools].[Schools] AS B ON A.SchoolNumber = B.SchoolNumber LEFT JOIN [1065574-SQLPRD1].[PROD1].[Schools].[StudentCSI] AS C ON A.StudentNumber = C.StudentNumber AND A.AcademicYear = C.AcademicYear LEFT JOIN (SELECT DISTINCT AcademicYear, [Subject], ProgramID FROM (SELECT [AcademicYear] ,[ProgramID] ,CASE WHEN [InterventionType] LIKE &#39;%math%&#39; THEN &#39;Math&#39; WHEN [InterventionType] LIKE &#39;%reading%&#39; THEN &#39;Reading&#39; ELSE NULL END AS [Subject] FROM [1065574-SQLPRD1].[PROD1].[Schools].[StudentCSI] WHERE [AcademicYear] = &#39;2020-2021&#39; AND (InterventionType LIKE &#39;%math%&#39; OR InterventionType LIKE &#39;%reading%&#39;)) AS A GROUP BY AcademicYear, [Subject], ProgramID) AS D ON C.ProgramID = D.ProgramID WHERE A.AcademicYear = &#39;2020-2021&#39; 7.5 Assessment Data 7.5.1 State of Texas Assessments of Academic Readiness (STAAR) The STAAR assessments are Texas’ standardized tests administered every year, except for 2019-2020 due to COVID-19. Grades 3-8 take the Reading and Math assessments every year with Writing, Science and Social Studies assessed at specific grades. Testing schedule is as follows: Grade 3: Reading and Math Grade 4: Reading, Math, and Writing Grade 5: Reading, Math, and Science Grade 6: Reading and Math Grade 7: Reading, Math, and Writing Grade 8: Reading, Math, Science, and Social Studies Assessments are provided in both English and Spanish up to the 5th grade, after which the test is only in English. Students in High School (and some middle school students) complete End-of-Course (EOC) assessments to fulfill graduation requirements. In order to graduate in Texas a student must have taken and passed 5 EOC assessments: Algebra I (taken in 8th grade at IDEA), Biology, English I, English II, and United States History. Students with significant cognitive disabilities would take the STAAR Alternate 2 (STAAR Alt 2) rather than the STAAR. This alternate test is administered by the teacher and is usually excluded from our analyses on academic achievement. All STAAR assessments, excluding STAAR Alt 2, have 3 performance standards: Approaches, Meets, and Masters. When the assessments were rolled out the passing standard was going to be moved up to the Meets standard after a few years when more than half of students were meeting the Approaches standard. This has not yet happened so the passing standard remains at the Approaches performance level. These three performance levels are NOT mutually exclusive, meaning that students that achieve the Masters standard are included in the Meets count and the Approaches count, and students achieving the Meets standard are included in the Approaches count. The STAAR Alt 2 has two performance levels: Developing, Satisfactory, and Accomplished. The passing standard for the STAAR Alt 2 is the Satisfactory performance standard. Again these performance levels (Satisfactory and Accomplished) are not mutually exclusive categories, students who achieve the Accomplished standrad are included in the counts for Satisfactory. Database: [RGVPDRA-DASQL].[Dashboard] Tables: [dbo].[STAAR]: This table contains all of the STAAR and STAAR Alt 2, 3-8 and EOC assessment data. Data in this table has multiple rows for each student, for each test and each administration that could be up to 3 administrations in one year. For example, a fifth grade student will take reading, math and science in one year, so that’s 3 rows for each test. Further, if this student doesn’t pass reading or math in March (the first administration) then they would take it again in May (second administration or retake). If they didn’t pass it in May they could take it a third time in June. Only 5th and 8th grade students get three opportunities to take Reading and Math in one academic year, this is due to the Student Success Initiative (SSI) requirements that every 5th and 8th grade student pass STAAR Reading and Math to be promoted to the next grade. Although there are exceptions to this rule (i.e., the requirement was waived in 2020-2021), one should expect more than one administration for some students. Below the description of columns is an example in r to get the students best score and de-duplicate to get one row per student. Useful Fields: AdminDate: indicates the two digit month and two digit year of administration for grades 3-8, these are character values. Ex. Spring 2021 administration dates for 3-8 would be “0421” and “0521”; for EOC exams the AdminDate for Spring 2021 is “1521”. GradeLevel: character values, two digit grade. Ex. “06” CountyDistrictCampusNumber: indicates the School Number LastName: Student’s Last Name FirstName: Student’s First Name StudentID: Student’s testing ID number Not the IDEA Student Number RaceReportingCategory: Student’s reported race, Ex. “H” = Hispanic EcoDisadvantage: Student’s economic disadvantaged status, 0, 1, 2, and 9. LEP: Student’s Limited English Proficiency status Codes: C = Current LEP, F = 1st year of monitoring, S = 2nd year of monitoring, T = 3rd year of monitoring, R = 4th year of monitoring, E = former LEP more than 4 years since reclassification, and 0 = Non-LEP SpEd: 0/1 indicator for receiving special education services AtRisk: 0/1 indicator of at-risk status LocalStudentID: Student’s 108 number SubjectCode: tested subject, i.e., Reading, Math, Science, Algebra I, etc. TestDate: equivalent to the AdminDate but only for 3-8 ScoreCode: indicates whether the test was scored or student was absent. ALWAYS filter to ScoreCode == “S” to get the scored tests TestVersion: “S” = STAAR and “T” = STAAR Alt 2 ScaleScore: provides the scaled score for that assessment and can be used to find the best score/most recent administration LevelII: 0/1 indicator for the Approaches performance standard Note When filtered to STAAR ALt 2 this indicates whether or not the student achieved the Satisfactory performance level LevelIIFinal: 0/1 indicator for the Meets performance standard Note When filtered to STAAR ALt 2 this indicates whether or not the student achieved the Satisfactory performance level LevelIII: 0/1 indicator for the Masters performance standard Note When filtered to STAAR ALt 2 this indicates whether or not the student achieved the Accomplished performance level SchoolYear: character value indicating the academic year, Ex. “2020-2021” BestRecord: logical column supposed to indicate if that record is the best for that student for that test DOES NOT WORK R example: STAAR &lt;- get_table(.table_name = &quot;STAAR&quot;, .database_name = &quot;Dashboard&quot;, .schema = &quot;dbo&quot;, .server_name = &quot;RGVPDRA-DASQL&quot;) %&gt;% filter(TestVersion == &quot;S&quot;, ScoreCode == &quot;S&quot;, SubjectCode == &quot;Math&quot;, AdminDate %in% c(&quot;0421&quot;, &quot;0521&quot;)) %&gt;% select(StudentID, LocalStudentID, GradeLevel, SubjectCode, AdminDate, ScoreCode, ScaleScore, Approaches = LevelII, Meets = LevelIIFinal, Masters = LevelIII) %&gt;% mutate(StudentID = as.numeric(StudentID), LocalStudentID = as.numeric(LocalStudentID), StudentNumber = if_else(LocalStudentID %in% c(0, NA), StudentID, LocalStudentID), StudentNumber = as.numeric(StudentNumber)) %&gt;% filter(StudentNumber != 0) %&gt;% select(-StudentID, -LocalStudentID, -ScoreCode) %&gt;% group_by(StudentNumber) %&gt;% distinct() %&gt;% collect() # To get a count of distinct student numbers to see how much duplication exists Count_stus &lt;- STAAR %&gt;% distinct(StudentNumber) # Getting max score to de-duplicate staar_math &lt;- STAAR %&gt;% group_by(StudentNumber) %&gt;% mutate(Best_score = max(ScaleScore), BestScoreFlag = if_else(Best_score == ScaleScore, 1, 0)) %&gt;% filter(BestScoreFlag == 1) %&gt;% select(-BestScoreFlag, -Best_score, -AdminDate, -ScaleScore, -GradeLevel) %&gt;% distinct() # Calculating the % Approaches and the % Masters pct_apprhs_mstrs &lt;- staar_math %&gt;% select(Approaches, Masters) %&gt;% summarize(n_students = n(), n_apprchs = sum(Approaches), n_mstrs = sum(Masters)) %&gt;% mutate(pct_app = n_apprchs/n_students, pct_mst = n_mstrs/n_students) 7.5.2 District assessments IDEA produces in-house assessments for each major content area except for national and state EOY assessments. Typically, courses require all teachers and students to administer the same summative assessments and benchmarks, but formative assessments are left to each teacher to create. Summative assessments evaluate what a student has learned over time and is usually more high-stakes than other types of assignments. Examples of the most common summative assessments we administer include: Unit exams End-of-module assessments Mid-unit exams (also mid-unit quizzes) Mid-module assessments Semester exams Final exams Projects, papers, performances, and portfolios Quarterly interim assessments (former) Bi-weekly assessments (former) Formative assessments are used to reveal student progress in the learning cycle and are usually more low-stakes. A small sample of formative assessments we use are: Daily exit tickets Weekly quizzes Progress checks Peer feedback and more… In this section, we will only consider district assessments that are centralized, meaning that most schools must administer and scan in the results into Illuminate (Texas | Louisiana), our main assessment platform. 7.5.2.1 Illuminate data The IABWA tables (for Interim Assessments and Biweekly Assessments) contain student-level and item-level performance records for any assessment administered on Illuminate, Edulastic, SchoolCity, or SchoolNet. Although we have phased out IAs and BWAs in favor of UEs (unit exams) and EOMs (end of module assessments), the table nevertheless contains all types of assessments scanned in since 2014-2015. The most common way of entering data into Illuminate is through a scantron-like answer form, which Illuminate automatically generates when an assessment is created. The teacher has the option of generating answer forms either with the individual student ID prepopulated (most common) or with a generic ID key that must be bubbled in (less common). Students (or teachers, if the response is open-ended) bubble in their responses, upload each answer form via webcam or scanner, until all data is collected. Illuminate then stores all data at two different levels: student-level, and item-level by student, each of which is stored in a different IABWA table. 7.5.2.1.1 Naming conventions Most assessments have a standard naming convention: SUBJECTCODE_COURSE_SEMESTER_ASSESSMENT For example, a 6th grade ELA semester exam in Fall 2021 may have the name ELA_6thReading_F21_SE, or the first AP Spanish Language unit exam may have the name SPA_APSpanishLanguage_F21_UE1. A list of the most common codes used is included below: Subject Code Semester Assessment ELA - English Language Arts Fyy - Fall (year) UE - unit exam MAT - Mathematics SPyy - Spring (year) EOM - end of module SCI - Science MUE - mid-unit exam HUM or SS - Social Studies MM - mid-module TECH - Technology SE or SEM - semester exam RTTC - Road to and through College FE - final exam SPA - Spanish IA - former interim assessment (final exam - IA4) BWA - former biweekly assessment Note that each assessment, except for semester and final exams, is typically numbered (see examples). You can use regular expressions to parse these codes and select which exams are needed. Note that different states may use different names for equivalent exams. Similarly, multiple-part exams may deviate from this convention to indicate each part of the assessment. 7.5.2.1.2 IABWA This table aggregates item-level data for each student and reports a student’s performance on a single assessment. Table: [1064613-SQLPRD2].[PROD2].[Assessments].IABWA Main metrics: Student-level data describing the assessment taken, the overall score, the performance band, and the mastery level Assessment columns: [IABWAID]: This number uniquely links a student to an assessment. [ExternalTestID]: This number uniquely identifies an assessment. [SubjectID]: This number describes if the subject is math, ELA, etc. [Subject]: Same as SubjectID, but in words. [AssessmentDate]: Date of the assessment. [AssessmentName]: Name of the assessment (see naming conventions, above). [AssessmentReporingName]: One of two columns to choose a type of assessment. [AssessmentType]: One of two columns to choose a type of assessment. [RawScore]: Raw number of points earned on the assessment. [PercentCorrect]: Percent earned on the assessment. [PointsPossible]: Total raw points available on the assessment. [PerfomanceBand]: Description of the student’s performance (typically “Critical”, “Did Not Meet”, “Approaches”, “Meets”, “Masters”) [PerfomanceBandNumber]: Numeric code for PerformanceBand (typically 1-5). [PBStartRange]: Lower bound for a performance band, as a percent. [PBEndRange]: Upper bound for a performance band, as a percent. [Description]: Description of performance band interval, in words. [Mastery]: 1 = demonstrated mastery (e.g. passed, approaches+, etc.); 0 = did not [src]: Source of asssessment data (most are Illuminate, but some are from legacy systems) [Scope]: Similar to AssessmentType, but more detailed. [State]: Two-letter state code (TX, LA, FL) where the assessment is administered. Other useful fields: [AcademicYear]: Use to filter the table to the appropriate school year. [StudentNumber]: Use to link to the students table. [TcpCourseCode]: This code uniquely links the assessment to the TCP course code. 7.5.2.1.3 IABWAItems This table displays item-level data for each student for each assessment. This information can be useful for looking up items with specific TEKS, taught/not-taught objectives, multiple-choice vs. open-ended responses, etc. Table: [1064613-SQLPRD2].[PROD2].[Assessments].IABWAItems Main metrics: Item-level data describing the assessment taken, the question and part, the student’s response, the correct answer, correctness, the standard, question type, weight, and points Assessment columns: [IABWAItemID]: This number uniquely links an assessment item part to a student. [IABWAID]: This number uniquely links a student to an assessment. [ExternalTestID]: This number uniquely identifies an assessment. [AssessmentDate]: Date of the assessment. [SubjectID]: This number describes if the subject is math, ELA, etc. [Subject]: Same as SubjectID, but in words. [AssessmentReporingName]: One of two columns to choose a type of assessment. [AssessmentType]: One of two columns to choose a type of assessment. [QuestionNumber]: Describes the question and part (e.g. 1a, 1b, 2, etc.) [ItemStudentResponse]: The response(s) scanned in by the student or teacher [ItemCorrectResponse]: The response(s) needed for full credit on the item. [standard]: Describes the objective number, TEKS, etc. [StandardsDescription]: Description of the objective in words. [Supporting]: 1 = is a supporting objective; 0 = not [Readiness]: 1 = is a readiness objective; 0 = not [isCorrect]: 1 = student earned credit for question number; 0 = not [Points]: Total points available for item. [ReportingGroups]: Describes the reporting group of objectives. [IsRubric]: ? [State]: Two-letter state code (TX, LA, FL) where the assessment is administered. [QuestionTypeName]: Describes the type of question (Multiple Choice, etc.) [QuestionType]: Same as QuestionTypeName but abbr. [ExtraCredit]: 1 = extra credit question; 0 = not [IsAdvanced]: 1 = advanced question; 0 = not [Weight]: [Maximum]: Maximum points available for item. [ItemWeight]: Weight of item on question. [Taught]: empty [MultiRubric]: ? [Taughtabbr]: NT = objective not taught; T = objective taught [PossibleScore]: Maximum (weighted?) points available for item. [RawScore]: Raw points the student earned for this item. [PointsCorrect]: Weighted points the student earned for this item. [PossibleScoreTaught]: Conditional on taught objective: 0 = not taught; PossibleScore = taught [RawScoreTaught]: Conditional on taught objective: 0 = not taught; RawScore = taught [PointsCorrectTaught]: Conditional on taught objective: 0 = not taught; PointsCorrect = taught Other useful fields: [AcademicYear]: Use to filter the table to the appropriate school year. [StudentNumber]: Use to link to the students table. 7.5.2.2 Other sources As IDEA grows nationally, certain regions have assessment-specific platforms that do not directly flow into the data warehouse. For example, Louisiana schools use ANet to house some of their LEAP-aligned assessments. 7.5.3 DIBELS Dynamic Indicators of Basic Early Literacy Skills, or DIBELS is a series of short literacy tests for early grade levels (K-2) originally developed by the University of Oregon Center on Teaching and Learning (CTL). We use these tests to inform us on reading outcomes for our youngest scholars at BOY, MOY, and EOY. Data can be viewed through Amplify but is usually pulled and stored in the warehouse. Table: [791150-HQVRA].[Dashboard].[dbo].[DIBELS_MCLASS] Main metrics: Counts of students meeting the standard, below or above the standard overall and for individual tests; Scores for individual tests Most important columns: [Assessment Measure-Composite Score-Levels] | [Assessment Measure-Composite Score-Score] | [Assessment Measure-FSF-Levels] | [Assessment Measure-FSF-Score] | [Assessment Measure-LNF-Levels] | [Assessment Measure-LNF-Score] | [Assessment Measure-PSF-Levels] | [Assessment Measure-PSF-Score] | [Assessment Measure-NWF (CLS)-Levels] | [Assessment Measure-NWF (CLS)-Score] | [Assessment Measure-NWF (WWR)-Levels] | [Assessment Measure-NWF (WWR)-Score] | [Assessment Measure-DORF (Fluency)-Levels] | [Assessment Measure-DORF (Fluency)-Score] | [Assessment Measure-DORF (Accuracy)-Levels] | [Assessment Measure-DORF (Accuracy)-Score] | [Assessment Measure-DORF (Retell)-Levels] | [Assessment Measure-DORF (Retell)-Score] | [Assessment Measure-DORF (Retell Quality)-Levels] | [Assessment Measure-DORF (Retell Quality)-Score] | [Assessment Measure-DORF (Errors)-Score] | [Assessment Measure-Daze-Levels] | [Assessment Measure-Daze-Score] | [Assessment Measure-Daze (Correct)-Score] | [Assessment Measure-Daze (Incorrect)-Score] Other useful fields: [School Year], [School Name], [Assessment Grade]: use to filter the table to the appropriate students [Benchmark Period]: labeled as BOY, MOY, or EOY [Student ID (District ID)]: use to match students back to the Students table. Note that data for IDEA Travis in 2021-22 BOY did not have Student IDs. You must match these students (and potentially others) by [Student Last Name] and [Student First Name]. If a match is not returned, check for misspellings of the names or hyphenated/dual last names. DIBELS data often omits a hyphen or the second last name entirely. Context: Who is assessed? All students in K, 1 and 2 are assessed using DIBELS. Which tests/columns are actually used? [Assessment Measure-Composite Score-Levels]: A composite level is assigned to all students, regardless if they complete all sections. Levels include “Well Below Benchmark”, “Below Benchmark”, “At Benchmark”, and “Above Benchmark”. [Assessment Measure-Composite Score-Score]: A composite score is assigned to all students, regardless if they complete all sections. The interval of composite scores is [200, 467+). [Assessment Measure-LNF-Levels]: The LNF, or Letter Naming Fluency test, is administered to K-1 students only. Levels include “Well Below Benchmark”, “Below Benchmark”, and “At Benchmark”. [Assessment Measure-LNF-Score]: This is a numerical measure of LNF in the interval [0, 59+). [Assessment Measure-PSF-Levels]: The PSF, or Phonemic Segmentation Fluency test, is administered to K-1 students only. Levels include “Well Below Benchmark”, “Below Benchmark”, “At Benchmark”, and “Above Benchmark”. [Assessment Measure-PSF-Score]: This is a numerical measure of PSF in the interval [0, 61+). [Assessment Measure-NWF (CLS)-Levels]: The NWF, or Nonsense Word Fluency test, is administered to K-2 students. Levels include “Well Below Benchmark”, “Below Benchmark”, “At Benchmark”, and “Above Benchmark”. [Assessment Measure-NWF (CLS)-Score]: This is a numerical measure of NWF in the interval [0, 46+). [Assessment Measure-DORF (Fluency)-Levels]: The DORF, or DIBELS Oral Reading Fluency test, is administered to 1-2 students. Levels include “Well Below Benchmark”, “Below Benchmark”, “At Benchmark”, and “Above Benchmark”. [Assessment Measure-DORF (Fluency)-Score]: This is a numerical measure of DORF - Fluency in the interval [0, 164+). [Assessment Measure-DORF (Accuracy)-Levels]: The DORF, or DIBELS Oral Reading Fluency test, is administered to 1-2 students. Levels include “Well Below Benchmark”, “Below Benchmark”, and “At Benchmark”. [Assessment Measure-DORF (Accuracy)-Score]: This is a numerical measure of DORF - Accuracy in the interval [0, 96+). The remaining assessment measures were used historically but are no longer actively tested. Why are there NAs for certain tests if the student took the assessment? DIBELS has multiple required measures but can be implemented using gating rules, which stop the assessment if a rule is met. If a student (1) scores below a minimum threshold or (2) tests out by scoring above the highest benchmark on a specified gating measure, then the remaining assessments are not administered, and the measure is left blank. The gating measure changes, depending on the grade level and benchmark period. Who can I ask for more context? Nkosi Geary-Smith is the Director of Early Literacy. She has context on the DIBELS assessment and how it is used. Chris Gonzalez is the VP of Accountability. He pulls the DIBELS data and uploads it to the warehouse. The University of Oregon maintains the DIBELS site and publishes guides on administering and interpreting scores. 7.5.4 Renaissance STAR (also RenStar, Little Star) 7.6 Teacher, Staff, and Hiring Data 7.6.1 Jobvite Data Jobvite is the application and hiring platform used for all staff. There job listings are posted, the interview process managed, and even initial onboarding tasks are progress monitored. R&amp;A has used data from Jobvite to analyze pre-hiring measures of hired teachers. Here are some more facts about Jobvite data: Obtained from HA team. It is the data from the candidates who apply for IDEA jobs through Jobvite. Not all candidates applied through Jobvite in the past, but HA is trying to streamline the application process and require all applicants for all jobs (at all campuses) to apply through Jobvite. A particular Jobvite file is labeled with a year, such as “Jobvite 19-20”, and it will have Candidate Submit Date column , which ranges from October 2019 to July 2019 (October to July, proceeding the year the applicant will likely begin work). An applicant from that range will most likely have a Start Date of August 2019 (or sometime in the calendar year 2019, starting AFTER the 19-20 Academic Year). However, there are cases when applicants will, for example, apply during the 10/2018 - 07/2019 cycle (Jobvite 19-20), but will not have a Start Date until 2020 or 2021 (so they do not start the following FAll after the application cycle). This file has columns including Candidate Full Name, Candidate Email (personal email used for the application), and some of the pre-hire selection measures, such as GPA. However, all 5 of the pre-hire selection measures are not included yet (STEM Major, Teach For America status, Experience prior to IDEA, GPA, and Teacher Certification). The data also has no Employee IDs yet, as this is a list of candidates. There is a Hire Yes/No column, which has “Yes” for those candidates who were offered a position and “No” for those who were not offered a position. Also, there is a Requisition ID which is basically an application ID number, and it is unique per application. If a single candidate applies for more than one position, the candidate will have a unique ID for each application. Below are some bullet points describing Jobvite and Teacher Export (TCP) data. Jobvite Data: applications for Teacher Roles through Jobvite (when filtering for Category == \"Teaching\") Contains those who are Hired = Yes and Hired = No. “Hired = Yes” means they were offered the position, but does not indicate if they actually accepted it or not. Each person may apply for more than one job opening, and if they do, they will have a unique Requisition ID for each application (even though it is the same candidate applying for each separate position). Has Personal Email and Full Name Does NOT have IDEA Work Email, nor EmployeeID Contains everyone who applied from October to the following July, for the subsequent August Start Date (or start dates after that year). Most people who applied in Jobvite 19-20 (10/18 to 7/19), started in Fall 2019 (19-20 Academic Year) But a few of those started in Fall 2020 and Fall 2021. Jobvite File facts 1. 19-20 – 2nd year of Jobvite data (18-19 is first and starting year of Jobvite data) * (renamed Jobvite 18-19 SY by Brittany; email 10-12-21) * Contains Tentative Start Date column – shows which month/year a new hire started working (doesn’t have to be immediately following that year’s hiring cycle) 20-21 – 3rd year of Jobvite data * (renamed Jobvite 18-19 SY by Brittany; email 10-12-21) * Contains Tentative Start Date column 21-22 – 4th year of Jobvite data (renamed Jobvite 18-19 SY by Brittany; email 10-12-21) Contains Tentative Start Date column 7.6.2 TCP/Teacher Export Data TCP/Teacher Export Data: Teacher Performance data. * Includes TCP Level Placements, TCP Composite Score, and TCP component scores (all 5 components). + GET (35%): Manager and Self Guidepost Ratings contain 6 components + Lesson Planning &amp; Delivery + Culture, Etc. + Components 1-5 are counted, and Component 6 (Core Values) is calculated outside of this score (it is 1 of 5 Components of TCP, worth 5% on its own). Each Guidepost Component has 4-5 items listed under each as 1A, 1B, etc. + Parent Survey (5%): Show up as BLANK if &lt;10 surveys were completed per teacher + Student Survey (5%) Shows up as BLANK if: + &lt;10 surveys were completed per teacher + Some campuses encourage students and parents to fill out the surveys, and some do not + Teacher taught SPED, pre-K, K, 1st, or 2nd Grades (those students are not able to take the survey/or too young). + Student Achievement (50%) Testing such as STAAR, etc. + Core Values (5%) + Contains IDEA Work Email and EmployeeID + Contains First Name and Last Name (but not Full Name) There is a 2 year HOLD for the pandemic (we don’t know who was “held”), meaning someone at a particular level (E.g., Level 5) could stay at that level for 2 years without having a composite score that is as high as that level. Usually, the hold is for 1 year. If someone got a 4.5+ composite TCP score for 2 consecutive years in a row, they can move to a Level 5 TCP without having the required years of Experience. For those who do not have Student Achievement scores, the highest TCP Level they can reach is a Level 3 (since Student Achievement is 50% of the score). If a person is missing one of the other 4 TCP components, such as Parent Survey, the points that were from that score are reallocated to the other components, weights the in the same ratios as the other components (so it’s not an equal allocation, meaning you don’t divide the percentage 4 equal ways). 18-19 From Alex, see email 19-20 (“Maura Report”, from Alex, new sheet on the Teacher Export 18-19 file from Alex, see email). Need to request this from Blanca (data table we are already using for TSLIP grant, that we can re-use for Teacher Hiring Round 3). 20-21 – 3rd year of Jobvite data from Blanca Carillo, contains ONLY those with “Teacher” roles OR New Hires and Promotions; all co-teachers, etc., have been removed for that academic year Contains some who started work in 21-22 but are New Hires or Promotions, so rated Level 1 TCP (but didn’t work in the 20-21 school year 21-22 – NO Not available yet! No one was rated for this school year because it’s not over yet, and the TCP ratings/placements have not yet been conducted. Will likely be finalized in Fall 2022 by early-October. 7.6.2.1 Data Matching Jobvite 19-20 to Teacher Export 19-20 Jobvite 20-21 to Teacher Export 20-21 Jobvite 21-22 to Teacher Export 21-22 – Can’t do this! TE 21-22 NOT available yet! Thus, we will be looking at 2 years of data: Jobvite 19-20 to Teacher Export 19-20 Jobvite 20-21 to Teacher Export 20-21 + Removing Jobvite 19-20 Hires who did not have a Start Date of Fall 2019 + Removing Jobvite 20-21 Hires who did not have a Start Date of Fall 2020 + Removing all Teacher Export 19-20 and Teacher Export 20-21, who did not have a Job Title of Teacher + Removing all Teacher Export 19-20 and Teacher Export 20-21, who had Internal Role as: SPED CP SPED AC PE CP PE CP (This is due to SPED and PE having state mandated license requirements that are not required for the other subjects, so these should be analyzed separately). Removing Teacher Export 19-20 Teachers who did not work in 19-20 Academic Year (Removing New Hires, Promotions starting in Fall 2020, TCP Level 1) Removing Teacher Export 20-21 Teachers who did not work in 20-21 Academic Year (Removing New Hires, Promotions starting in Fall 2020, TCP Level 1) 7.6.2.2 Join Tables How to join? * Jobvite has no Employee ID &amp; no Work Email and Teacher Export has Employee ID and Work Email. * Possibly join through PROD1.Staffing.Employees table, which contains First Name, Last Name, Birthday, Employee ID, and Work Email ##Camp Rio ##TSLIP Grant that contains 4 key components/areas for research questions. Main points of analysis: Level 5 Pilot Program and “Best Practices” library; TCP Level percentages by District/Region/Campus/School and look at percentage and distribution of TCP Level 5 teachers across schools and possibly down to grade level. "],["models-and-methods.html", "Chapter 8 Models and Methods 8.1 Research design 8.2 Descriptive statistics 8.3 Inferential statistics 8.4 Modeling and predictive statistics", " Chapter 8 Models and Methods Our research involves a wide variety of methods in statistics and data science to analyze problems and answer questions in educational data. In consultation with our stakeholders, we carefully select the most relevant approach(es) to provide an appropriate and accurate product. We use methods in descriptive, inferential, and predictive statistics to understand information about our organization. The choice of frequentist vs. Bayesian methods is up to you and should always be justified, but choosing the simplest, appropriate tool should be the starting point for analysis But, before we start any analysis, the design of the program influences how we establish our methods of research. 8.1 Research design When our works requires us to assign causation, we must consider how the data were collected before deciding on an appropriate model. 8.1.1 Causal inference Most of our data is observational, so if we want to describe causal relationships in our data, we must appeal to potential outcomes. Thus, we will briefly cover some of relevant causal inference concepts and techniques. When a student is marked for inclusion in, say, a tutorial group, we would like to see how much the tutorial increased their score on a test, compared to a scenario where the student did not attend the tutorial. If the student attends the tutorial, we can measure the outcome of the treatment (the tutorial) by seeing what grade they earned after a test; we cannot observe, however, the outcome of the control (no tutorial). Potential outcomes aims to remedy this. 8.1.1.1 Potential outcomes Suppose we have both the treatment and control outcomes for each student. We would like to measure \\(\\delta_i = Y^1_i - Y^0_i\\), the difference in the treatment outcome \\(Y^1_i\\) and control outcome \\(Y^0_i\\) for student \\(i\\). Then, our main estimators for the tutorial effect are \\(ATE, ATT\\) and \\(ATU\\), which are defined below: \\(ATE = E[\\delta_i]\\) represents the Average Treatment Effect \\(E[\\delta_i] = E[Y^1_i - Y^0_i]\\). In other words, \\(ATE\\) answers the question “What is the additional effect of the treatment?” \\(ATT = E[\\delta_i \\mid \\textrm{in treatment group}]\\) represents the Average Treatment effect of the Treatment group, \\(E[Y^1_i - Y^0_i \\mid \\textrm{in treatment group}]\\). In other words, \\(ATT\\) answers the question “What is the additional effect of the treatment for those in the treatment group?” \\(ATU = E[\\delta_i \\mid \\textrm{in control group}]\\) represents the Average Treatment effect of the Untreated (control) group, \\(E[Y^1_i - Y^0_i \\mid \\textrm{in control group}]\\). In other words, \\(ATU\\) answers the question “What is the additional effect of the treatment for those in the control group?” \\(ATE\\) can be decomposed into a weighted average of \\(ATT, ATU\\), such that \\(ATE = p\\times ATT + (1-p)\\times ATU\\), where \\(p\\) is the weight of the treatment group. In our scenario, \\(ATE\\) represents the expected marginal effect of the tutorial group on test scores for all students, \\(ATT\\) represents the expected marginal effect of the tutorial group for students who participate in tutorials, and \\(ATU\\) represents the expected marginal effect of the tutorial group for students who do not participate in tutorials. Estimates may be biased because we do not randomly assign students to tutorials - we select them for each group based on some criteria. Using randomization inference, we can separate assignment bias from the estimates of \\(ATT, ATU\\) to find appropriate estimates and p-values on \\(ATE\\). 8.1.1.2 Matching models We can leverage large samples to find a potential outcome for a student by matching other students who are reasonably similar, using a specified distance measure. Thus, we can reasonably estimate \\(ATE\\) since we have both the treatment and control outcomes. By using the matchit package in R, we can specify covariates we would like to match in our data between the treatment and control groups. #use matchit like a glm m_out &lt;- matchit(ActiveIL ~ Race + Gender + Grade + SPED + LEP + EcoDis + GradeEquivalentBOY, #response specifies control/treatment, covariates specify what to match data = rs_il_mod, method = &quot;cem&quot;, #specify matching method link = &quot;logit&quot;, estimand = &quot;ATT&quot;) #produces data frame with needed weights m_data &lt;- match.data(m_out) #model uses weighted linear model to estimate causal effect of using Imagine Learning m_att &lt;- lm_robust(GradeEquivalentGrowth ~ ActiveIL, data = m_data, weights = m_data$weights) summary(m_att) 8.1.1.3 Regression discontinuity 8.1.1.4 Instrumental variables 8.1.1.5 Difference-in-differences 8.1.2 Experimental design Under ideal conditions, we would test the effect of a program by randomized controlled trials – this usually does not happen. However, we can still use the tools of design to structure our data and variance appropriately. 8.1.2.1 Differences among distributions of groups ANOVA, ANCOVA, MANOVA, Kruskal-Wallis Code, diagnostics, communication and limitations coming soon… 8.1.2.2 Differences among levels of variables Educational data can often be described in a hierarchy: Level 1: student Level 2: student within a school Level 3: student within a school within a region A mixed model (also, hierarchical or multilevel) allows for both fixed and random effects, so this nesting will give us different variances at each level. This is ideal for modeling an outcome that can vary by school and regional differences - in other words, we can vary the slope and intercept by each school and region. Code, diagnostics, communication and limitations coming soon… 8.2 Descriptive statistics When summarizing a distribution of data, we are usually tasked with describing (1) measures of center, such as counts and proportions for categorical data, or means and medians for quantitative data and then displaying those values for our stakeholders. To provide context and increase the statistical literacy of our team and family, we also describe the (2) spread, (3) shape, and (4) unusual features of the distribution. 8.2.1 Numerical summaries - broad principles For categorical data, consider the distribution of proportions and counts (the sample size) across the various categories. One-way and two-way frequency tables, with marginal and total proportions and counts, are useful for displaying the data. For univariate, quantitative data, use the shape of the distribution to determine appropriate numerical measures. For symmetric data, the mean \\(\\bar{x}\\) mean(x) and standard deviation \\(s\\) sd(x) can be appropriate for reporting and is accessible by a broad audience. For skewed data, consider the median \\(\\tilde{x}\\) median(x) (and possibly the MAD - median absolute deviation mad(x)), and provide context for choosing these measures. Note: The median is also a widely accessible statistic to most people, since it describes the midpoint of your distribution. Consider using it alongside the mean. For bivariate data, use a scatterplot to determine if linear measures can be used appropriately. The Pearson correlation \\(r\\) cor(x, y) can be easily understood by a wide audience to describe the linear strength of a bivariate association. However, the Spearman rank correlation cor(x, y, method = \"spearman\") can provide an associative measure for monotonic nonlinear associations, and Kendall’s \\(\\tau\\) cor(x, y, method = \"kendall\") can describe concordance for ordinal data. When describing outliers in a report, care should be taken when describing how we identify and use (or remove) the outliers from the data set. One heuristic for deciding what to do with an outlier is to ask yourself, “Does this observation belong in the sample?” If you decide that the observation is fundamentally different than the others (e.g. a school is off-model and does not implement the Imagine Learning the same way that other schools do), then you might consider removing that observation. If the observation represents some exceptional case of the proposed model, then you might consider keeping that observation. In general, we should be modeling a measure of center and its spread as a function of covariates (see Modeling and predictive statistics). 8.2.2 Graphical summaries - broad principles While there are plenty of ways to display data, all graphs should be accompanied by an explanation of key features. Our aim is to help educators interpret the data in memorable, meaningful ways, and graphs without context can convey the wrong information or may obscure what the researcher attempted to communicate. Each graph should include a descriptive title, axes, and appropriate labels; consider using the ideacolors package for aesthetics. Selecting an appropriate graphical display depends on the purpose of the analysis and the communication of results. Bar graphs, line graphs, histograms, and scatterplots each have their place in displaying central tendency; however, emphasis should be placed on displaying variation within the data. 8.2.2.1 Displaying variation Variation can be addressed by the following (non-exhaustive) list: Comparing sample sizes, both for the entire distribution and within each group Comparing variation within and between groups Displaying variation across time Clustering of values in a region (and conversely, gaps in data) Different approaches to this could be altering alpha values, jittering, color scales, size, and the shape of the marker. Some sample code for doing is found below: make_Data_By_Launch_Cohort() %&gt;% ggplot(aes(x = LaunchSchoolYear, y = jitter(MedianGrowth), #jittering of clustered points group = School)) + geom_point(aes(color = School, #change color by school alpha = 0.2), #set transparency level to 0.2 size = 3.5) + #increases size of point geom_point(aes(y = MedianGrowthLaunch, color = LaunchOrder), shape = 3, #changes shape to plus sign + size = 5) + scale_color_idea() + theme_idea_light() + facet_wrap(~LaunchOrder, labeller = labeller(LaunchOrder = c(&quot;1&quot; = &quot;First Year&quot;, &quot;2&quot; = &quot;Second Year&quot;, &quot;3&quot; = &quot;Third Year&quot;))) + labs(title = &quot;Median RenStar Reading Growth&quot;, subtitle = &quot;Results grouped by schools launching in the same year&quot;, caption = &quot;+ represents median RenStar reading growth for that launching cohort&quot;, x = &quot;Launch Year&quot;, y = &quot;Median RenStar Growth&quot;) + scale_x_discrete(labels = c(&quot;2017-2018&quot; = &quot;2017-18&quot;, &quot;2018-2019&quot; = &quot;2018-19&quot;, &quot;2019-2020&quot; = &quot;2019-20&quot;, &quot;2020-2021&quot; = &quot;2020-21&quot;)) + theme(legend.position = &quot;none&quot;) This code spaces out MedianGrowth if there are multiple schools with the same median growth, changes the transparency level to 0.2, and changes color wtih each school. The value of MedianGrowthLaunch is plotted with a “plus” shape at a larger size to emphasize the median growth for each school year. This image shows a graph of median RenStar growth levels from launching schools in different school years and shows variation in those medians. 8.3 Inferential statistics If the goal of the project is to do inference, or drawing conclusions about the population from the data, then we should provide context about these conclusions, particularly around variability and likelihood. 8.3.1 Interval estimates Suppose we are providing an estimate, like forecasted student persistence rates. A single point estimate may give us our best target, but due to variability, we surely will not reach that exact forecast. Thus, we also provide a confidence interval (or a Bayesian credible interval) to give a lower and upper bound around our estimate. These bounds could be found by computing a margin of error around the point estimate, or could be determined by the bootstrapped quantiles. It may be instructive for a general audience to interpret these bounds as a “worst case scenario” and “best case scenario” and give a visual around these estimates. For reference, see the Persistence Dashboard (no longer actively updated). 8.3.2 p-values and hypothesis testing When deciding if a pattern or relationship even exists, we can choose a model and set of hypotheses to test. Given the type of explanatory and response variables in your model, you should choose an appropriately designed test (reference for common tests). Often, the test statistics and p-values are already given when using a package. Note the term “statistical significance” may carry a lot of weight with an audience, but it should be use with great caution. When using or explaining a p-value, first recall that a p-value is the probability of obtaining a result as extreme or greater from repeated sampling under the proposed distribution of the null hypothesis meaning that our probability operates under the assumption that the proposed model (the null hypothesis) is correct. (This means if we change the null hypothesis, then we could get an entirely different p-value with the same data). Thus, we should look carefully at how our choices in modeling and distribution affected the significance of the result. Furthermore, the choice of level (typically, \\(\\alpha = 0.05\\)) is arbitrary, so we cannot make certain conclusions, such as: If a result has a p-value of 0.06, and another has 0.04, then one result is not necessarily “more significant” than the other. The p-value does not imply how large the effect size is. Statistical significance does not necessarily represent practical significance. When communicating your results, do not merely rely on a p-value. Supplement a p-value with other explanations of the effect in context, like an interval estimate, graphs, a margin of error, or transformed values. For reference, see the American Statistical Association’s statement on p-values (2016). 8.4 Modeling and predictive statistics A large chunk of our statistical work involves developing a model and making predictions about a new set of data. The methods we use range from the basic models, like a least-squares linear model, to non-parametric methods and machine learning, like random forests. The choice of model largely depends on the goal of the project. 8.4.1 Linear modeling For short-term, ad-hoc models, we usually start with a simple linear regression, which essentially models a mean (quantitative) response value as a function of other explanatory variables. The mathematical model looks like this: \\(\\mu_i = \\beta_0 + \\beta_1 x_{1i} + \\cdots \\beta_n x_{ni} + \\epsilon_i\\) In R, we use the command lm() to generate estimates of the beta coefficients. To obtain the p-values and basic fit statistics, use summary(lm_object). To obtain the diagnostic plots, use plot(lm_object). We can describe the coefficients as an additional unit effect on the mean response. If the model has been identified using a DAG, then we can interpret the coefficient of a direct path as the causal effect. 8.4.2 Generalized linear modeling When a transformed response is linear in the betas, then we use a generalized linear model (GLM). A common example of GLM is a logistic regression, where the response is binary, so we effectively predict the odds of an event happening relative to another event. The mathematical model looks like this: \\(g(\\mu_i) = \\beta_0 + \\beta_1 x_{1i} + \\cdots + \\beta_n x_{ni} + \\epsilon_i\\) where \\(g(\\cdot)\\) is a link function to the mean \\(\\mu\\) is the mean response Note that there are many link functions, so the choice of link and mean response will affect the interpretation of the model. Different implementations in R will provide differing pieces of information, so it is best to look up the documentation. However, base R provides the glm() function, which gives options to choose a link. If the response is multinomial (multiple categories), then it is suggested to use a multinomial logistic regression. If the categories are ordered, which are common in assessment data, TCP ratings, Likert scales, etc., the model may benefit from using ordered responses in an ordinal logistic regression, which improves the power. See this reference for implementations of OLR using various packages. Code, diagnostics, communication and limitations coming soon… 8.4.3 Machine learning If the goal moves beyond inference towards prediction (getting as accurate of a response as possible), then we can use machine (statistical) learning methods to model the data. These tools are particularly useful when we are modeling nonlinear trends and cannot assume a functional form. Note that communicating results from these models may be beyond a general audience’s reach, so caution should be taken when describing a marginal effect of a variable. The ISLR website and tidymodels implementation should be consulted for more details about these models. Code, diagnostics, communication and limitations coming soon… "],["tips-and-tricks.html", "Chapter 9 Tips and Tricks 9.1 Data processing 9.2 Statistical pitfalls", " Chapter 9 Tips and Tricks This section of the R&amp;A Manual is for sharing various tricks that are will make you life easier. Rules of thumb, explanations of how things work, and useful design patterns are all fair game. 9.1 Data processing 9.1.1 De-duplicating rows in a table. There are two general purpose design patterns for de-duplicating rows in a table in either R or SQL. In R use the dplyr::distinct() function and in SQL use a DISTINCT clause. This is pretty simple and robust, with the typical use case of wanting to reduce many identical rows to a single row. If you are trying to de-duplicate where you may have non-identical rows (e.g., a student has taken the same assessment multiple times in a testing window and you’d like to get the latest date) then you can use a group_by-arrange-row_number-filter pattern: Group you data by the columns that uniquely identify your object of inquiry (e.g., student number) Arrange or sort the data by a column that puts an ordering you care about within the grouped data (e.g., assessment date) Create a new column that contains the row number within the group (e.g, a student with 5 assessments in a window would have row_number = 1 for the first date and row_number = 5 for the last date). Filter for the maximum row number(with dplyr::filter() in R or a WHERE clause in SQL) (optional) Ungroup your data. 9.1.2 Creating “Scaffolds” when joining data. Coming Soon. . . 9.2 Statistical pitfalls 9.2.1 Simpson’s Paradox When identifying trends in a distribution, be careful of interpreting those trends, particularly when there is hierarchy involved in the population. The ranking, magnitude, and direction of the trends can reverse when conditioning on a certain level of a hierarchy. For example, STAAR achievement levels as a function of minutes on an individual learning program like Dreambox may not be significant at the district level, but could be significant when at the school or classroom level. 9.2.2 Bias-variance tradeoff "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
