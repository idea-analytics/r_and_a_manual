# The Data Warhehouse and how to access it

The [first section](#section-the-data-warehouse) that follows provides lots of context about IDEA's data warehouse.

If, however, you are in a rush and want to go about accessing the data warehouse, then skip to section \@ref(#accessing-the-data-warehouse-from-r) on accessing from R below. 

```{r load_dw, include=FALSE}
rna_dw_usage <- readxl::read_excel("DataWarehouse Usage.xlsx") %>% 
  janitor::clean_names() %>% 
  select(r_and_a_uses = r_a_uses_x,
         server_name, 
         database_name, 
         table_name, 
         column_name, 
         data_type, 
         constraint) %>% 
  mutate(
         across(where(is.character), as_factor),
         r_and_a_uses = !is.na(r_and_a_uses)) %>% 
  distinct()

n_servers <- rna_dw_usage %>% distinct(server_name) %>% nrow()
n_dbs <- rna_dw_usage %>% distinct(database_name) %>% nrow()
n_tables <- rna_dw_usage %>% 
  select(server_name, database_name, table_name) %>% 
  distinct() %>% 
  nrow()
```

## The Data Warehouse
The 'data warehouse' is actually a collection of disparate databases hosted by IDEA Public Schools.  Some databases are used to host the original source data, some are used as the data model for production applications, while others site between those two points.

While the IDEA has ~28 servers hosting SQL Server databases, R&A typically only accesses `r n_servers` servers hosting `r n_dbs` databases and `r prettyNum(n_tables, big.mark = ",")` distinct tables. It's a lot, for sure. 

### Core data warehouse server configuration

The servers are not islands unto themselves, but rather form a small economy of data flows, where data moves from source servers, to production servers (i.e., the sweetspot for us) to reporting servers that host transformed data for our Logi-powered Locus dashboards.  

```{r server-graphs, echo=FALSE, fig.cap="Data in the warehouse is initially stored in 'src_*` databases in 'DS*' servers and flows to the right towads Logi hosted Locus dashboards."}
DiagrammeR::grViz("
  digraph data_wh {
    label='Basic Data Flows in Core Data Warehouse';
    labelloc = 't'; 
  #graph statement
  graph [overlap = true, layout = dot, rankdir = LR]
  
    #node statement
    node [shape = cylinder, 
          style = filled, 
          fontname = Helvetica, 
          fillcolor= '#f9a054', 
          color = '#626363',
          fontcolor = white,
          fontsize=10]
    DSPS [label=<DS-PS<br/><font point-size='6'>887192-SQLDS</font>>]; 
    DS3  [label=<DS-III<br/><font point-size='6'>1044407-SQLDS3</font>>]; 
    DSOT [label=<DS-OT<br/><font point-size='6'>964592-SQLDS</font>>] 
  
    node [fillcolor= '#1a4784']
    PROD1 [label=<PROD1<br/><font point-size='6'>1065574-SQLPRD1</font>>]; 
    PROD2 [label=<PROD2<br/><font point-size='6'>1064613-SQLPRD2</font>>]
    
    node [fillcolor= '#a4c364']
    RS2 [label=<RS2<br/><font point-size='6'>1064596-SQLRPT2 </font>>];
    RS1 [label=<RS1<br/><font point-size='6'>1017144-SQLRPT </font>>]
  
  node [fillcolor= '#53b4cc']
  STHIS [label= <ST-HIS<br/><font point-size='6'> xxxxxx-STHist </font>>]; 
  RSHIS [label=<RS-HIS<br/><font point-size='6'> xxxxxx-SQLRPTHis  </font>>]
 
  node [fillcolor = firebrick]
  FO1 [label =<FO1<br/><font point-size='6'> Local Failover </font>>]
  
  node [shape = box, fontname = Helvetica, fillcolor= '#ffde75', fontcolor = black]
  LOGI

  # subgraph statements
  subgraph cluster_ds {rank = same; style = dotted; DSPS; DS3; DSOT; label='1. Data Storage'}
  subgraph cluster_prod {rank = same; style = dashed; PROD1; PROD2; label = '2. Production'}
  subgraph cluster_rs {rank = same; style = dotted; RS2; RS1; label='4. Reporting'}
  subgraph cluster_his {rank = same; style = dotted; STHIS; RSHIS; label = '3. Historical' }
  
  #edges statement
  edge [color = '#626363', arrowhead=vee]
  {DSPS DS3} -> PROD1 
  {DSOT} -> PROD2
   PROD1 -> {DSOT DS3} [label='Look-ups', color = gray, style=dashed, fontcolor=gray, fontsize=7]
   PROD1 -> RS2
   PROD2 -> RS1
  {PROD1 PROD2} -> FO1 [color = firebrick]
  {PROD1 PROD2} -> STHIS
   STHIS -> RSHIS
  {RS2 RS1 RSHIS} -> LOGI
  
  }
")



```

Figure \@ref(fig:server-graphs) shows the flows of data in the warehouse, which essentially move from left to right.  The` DS-*` servers host databases holding source data.  That data is processed and saved in databases in the two production servers (`PROD1` and `PROD2`).  **These two production servers host most the data that R&A uses in analysis**, but know where the source data comes from in is often helpful.  Some data is (ostensibly) snapshotted and stored in the two `*-HIS` servers.  To be sure, as of March 2020, R&A doesn't know much about these servers.  The reporting servers---`RS1`, `RS2`, and `RS-HIS`, serve transformed flat files that serve as the data layer for [Logi](https://www.logianalytics.com/), which serves up [Locus dashboards on the hub](https://ideapublicschoolsorg.sharepoint.com/sites/dashboards).  

The following sections provide some details on the what databases might be found on each server.

#### DS Servers
**NOTE: the DS servers have names that do not correspond the the servers' URLS.  Why this is the case is not clear, but this is something to pay attention to when work with data from these sources**

* **`DS-PS`** is hosted on  `887192-SQLDS` and serves as a source for `PROD1`.  Most the data hosted on `DS-PS` mirror PowerSchool's Oracle back-end (espececially the `Schools`, `ADA`, and `Enrollment` databases), but it also has data from Teams Insights, staffing, college applications and outcomes, student persistence, among other things. 

  * **Databases**:

    * ADA  
    * Attendance
    * BlendedLearning
    * CNP
    * Colleges
    * Enrollment
    * Finance
    * Insights: data from Microsoft Teams
    * Persistence
    * Schools: key data from PowerSchool
    * Schools.DL
    * Staffing
    * Survey
    
  * **Sources**:
    
    * PowerSchool
    * Other undefined sources 

  * **Targets**:
    
    * `Prod1`
    

* **`DS-III`** is hosted on  `1044407-SQLDS2`  and serves as a source for `PROD2`. (note that this is a bit confusing . . . which makes this writer think we may have this wrong; why would something called `DS-III` have  server named `DS2` that mostly provides source data to `PROD2`?) The data hosted on `DS-III` is almost exclusively source data from testing platforms like Illuminate DnA, AP, IB, ACT, and SAT. 

  * **Databases**:
    
    * SRC_IA 
    * SRC_IB 
    * SRC_AP 
    * SRC_LEAP 
    * SRC_EA 
    * SRC_Dibles 
    * SRC_NWEA 
    * SRC_Plan 
    * SRC_SAT 
    * SRC_Telpas 
    * SRC_ACT 
    * SRC_Explorer  

    
  * **Sources**:
    
    * See the database names just above
    * `PROD1` for lookup data about students and schools

  * **Targets**:
    
    * `Prod2`
    
* **DS-OT** is hosted on `964592-SQLDS `  and serves as a source for `PROD1`.  Databases focus on source data from variance external systems, including Naviance, Tyler Munis, Teachboost, Schoolmint, and a slew if individualized learning/blended learing platforms. 

  * **Databases**:
    
      * SRC_Zendesk
      * SRC_Naviance
      * SRC_StMath
      * SRC_DreamBox
      * SRC_IHT 
      * SRC_TylerMunis 
      * SRC_TeachBoost 
      * SRC_StaffRetention 
      * SRC_SchoolMint
      * SRC_Recruitment
      * SRC_OpsCampusRanking 
      * SRC_JobVite 
      * SRC_HR SRC_GetRating 
      * SRC_ConnerStoneEvaluations 
      * SRC_BlendedLearning 
      * IdeaInstructionReporting 
      
      
  * **Sources**:
    
    * See the database names just above
    * `PROD1` for lookup data about students and schools

  * **Targets**:
    
    * `Prod1`
      
    


#### PROD Servers

* **PROD1** is hosted on ` 1065574-SQLPRD1` and contains a variety of data bases sourced from `DS-PS` and `DS-OT`. The key studetns, schools, and regions data is found here and is infact used by `DS-III` and `DS-OT`. Microsoft Insights data is now found here as well. This is the where transformed data is stored that is moved to `RS2 ` (again, it is not clear why the server number doesn't quite line up as we move from initial DS to PROD to RS).

  * **Databases**:

    * ADA  
    * Attendance
    * BlendedLearning
    * CNP
    * Colleges
    * Enrollment
    * Finance
    * Insights: data from Microsoft Teams
    * Persistence
    * Schools: key data from PowerSchool
    * Schools.DL
    * Staffing
    * Survey
    
  * **Sources**:
    
    * `DS-PS`
    * `DS-OT`
  
 * **Targets**:
 
  * `RS2`
  * `ST-HIS`
  * `FO1`
  
* **PROD2** hosts all other data that is not on `PROD`, which is to say data initially stored in `DS-III`.  This is the source server for `RS1` and includes assessments (e.g., IAs, bi-weekly and unit assessments, AP, IB) data and accountability tables. 

  * **Databases**:
  
    * Assessments
    * Schools : this actually contains data "inventories at the school level, like lists of employees, equipment and asset inventories, students schedules, correspondences between teachers and students. 
    
#### RS and Hisotrical data Servers

R&A tends not to use these tables very often. `RS1`, `RS2`, and `RS-HIS` all have flat tables that utilized by by Logi for Locus Dashboards that are developed by Software Development. `ST-HIS` ostensibly has snapshots of data from the `PROD` servers, but that is just an unverified hunch 



* **ST-HIS**
  
  * **Databases**: unknown
    
  * **Sources**:
      
    * `PROD1`
    * `PROD2`
    * something called `ST1` and ST2`?
    
    * **Targets**
      * `RS-HIS`

* **RS1** 
  
  * **Databases**: unknown
  * **Sources**:
    
    * `PROD2`
    * something called `ST2`?
  
  * **Targets**
    * Logi
    
* **RS2**
  
  * **Databases**: unknown
  * **Sources**:
    
    * `PROD1`
    * something called `ST1`?
  
  * **Targets**
    * Logi
  
* **RS-HIS**
    
    * **Databases**: unknown
    * **Sources**:
    * `ST-HIS`
    
  
  * **Targets**
    * Logi



### Server, Table, and Field Lookup

Table \@ref(fig:display-dw-tables) provides details on servers, databases, tables, and fields.  While it is not complete, it does cover the majority of our data infrastucture.

```{r display-dw-tables, echo=FALSE, warning=FALSE, fig.cap="Data warehouse details"}
rna_dw_usage %>% 
  janitor::clean_names("title") %>% 
  DT::datatable(filter = "top", rownames = FALSE, )
```


#### IP Addresses of current servers

For the fastest performance when working remotely it is best to directly access a database by using the IP address of the server rather than than the URL, which often has multiple hops over the network when resolving the DSN. Here's a list of known IP Addresses and server names.

| IP Adress   | Server Name       |
|-------------|-------------------|
|172.24.16.195|   795184-LOGI     |
|172.24.16.208|   795198-CUSTAPPS |
|172.24.16.209|   795199-CUSTAPSV |
|172.24.16.166|   887192-SQLDS    |
|172.24.16.193|   795182-SQLCUSAP |
|172.24.16.125|   795178-DISERVIC |
|172.24.16.207|   795197-DIONLINE |
|172.24.16.198|   964592-SQLDS    |
|172.24.17.56 |   1123130-SDPDTMP |
|172.24.16.83 |   1064618-SQLDI   |
|172.24.16.216|   1054148-FINAPP  |
|172.24.17.111|   1064596-SQLRPT2 |
|172.24.17.110|   1064599-SQLRPTH |
|172.24.16.80 |   1064611-SRC4    |
|172.24.16.206|   795196-HQSTAGIN |
|172.24.16.196|   795186-HQPROD01 |
|172.24.16.197|   795187-HQSQA01  |
|172.24.16.201|   795191-AUTOMATI |
|172.24.16.82 |   1064615-API1    |
|172.24.16.110|   1017144-SQLRPT  |
|172.24.16.205|   795195-HQSQLSA1 | 
|172.24.16.81 |   1064613-SQLPRD2 |
|10.255.88.201|   HQVSQLDS        |
|172.24.17.160|   1065574-SQLPRD1 |
|172.24.16.210|   1044407-SQLDS3  |
|10.255.88.176|   HQVSDBACKUPS    |
|172.24.16.79 |   924794-AUXAUTO  |
|10.255.88.152|   SQLBI           |

: List of Windows Servers hosting SQL Server Databases and their IP Addresses


### Databases that R&A frquently uses. 
```{r echo=FALSE}
rna_dw_usage %>% filter(r_and_a_uses) %>% 
  select(server_name, database_name) %>% 
  distinct() %>% 
  janitor::clean_names("title") %>% 
  DT::datatable(filter = "top", rownames = FALSE)
  
```
: List of Windows Servers hosting SQL Server DBs and their IP Addresses

## Accessing the data warehouse from R

The most straightforward way to access the data warehouse is to use our own, bespoke r package: `ideadata`, which is [maintained on github](https://github.com/idea-analytics/ideadata)

### Installation


Since `ideadata` is an internal IDEA package there is only a development version, which is installed from [GitHub](https://github.com/) with:

``` {r install, eval=FALSE}
#install.packages("remotes")
remotes::install_github("idea-analytics/ideadata")

#renv::install("idea-analytics/ideadata@main") also works
```
### Example

Here's how you connect to the `Schools` table in the warehouse.

```{r example, eval=FALSE}
library(dplyr)
library(ideadata)

schools <- get_schools()

glimpse(schools)
```


The `schools` object above is `tbl` object.  That means it works with `dplyr` verbs and functions, but  what  happens in the background is that `dplyr` and `dbplyr` generate SQL that is sent to the database you are connected to and  all that computation (e.g., filtering, selecting, joining, calculations, aggregation) are completed on the remote SQL Server instance and **not** on your computer.  

Nevertheless, you will eventually want to pull that data down onto your machine when you want to use R or Python do what they can do (like modeling or graphics) that the database can't do. 

Pulling that data down is easy with [dplyr::collect()]

```{r collect, eval=FALSE}
library(dplyr)

schools_df <- schools %>% 
  collect() %>% 
  janitor::clean_names()
```

(Here `janitor::clean_names()` snake_cases all the column names). 

#### What if I am pulling down lots of data (say, millions of rows)?
In this instance the database connection may fail.  It's not ideal, but it happens.  One way to deal with this is to pull down the data piecemeal.  The `collector()` function in `ideadata` makes this task trivial. It takes column names as arguments (unquote) from the table you want to pull down and those columns are used to break up the data into smaller sets of data that are pulled down from the database onto your computer and then recombined into a single table. 

```{r collector, eval=FALSE}

schools_df <- schools %>% 
  collector(SchoolState, CountyName) %>% 
  janitor::clean_names()

```

::: {.gotcha}
The `ideadata` package is clever in that it updates its knowledge of the data warehouse every time you load the package with `library(ideadata)`.  However to do that you need to have access to the the warehouse, and that requires that you are on the VPN/behind the firewall. Do makes sure that is true before invoking the package!
:::


### Finding things in the data warehouse

The `ideadata` package has a function that will open up a sortable, filterable, and searchable table in the RStudio idea: [`view_warehouse_meta_data()`](https://idea-analytics.github.io/ideadata/reference/view_warehouse_metadata.html)


### Where to learn more

`ideadata` [has it's own documentation here](https://idea-analytics.github.io/ideadata/).  If you can't find an answer there, then reach out to Chris Haid (he's responsible for this craziness).  

::: {.tip}
[Go to this article](https://idea-analytics.github.io/ideadata/articles/setting-up-your-credentials.html) to set up your credentials in R so that the warehouse can authenticate and authorize you as verified user
:::


## Key tables

This section details important tables.  In general you want to stick with databases `PROD1` or `PROD2` but your mileage may vary. 

Another important aspect of the warehouse is that the sources system for most student data is our SIS: PowerSchool.  The [PowerSchool Tables Data Dictionary](http://www.albertapsug.ca/uploads/1/5/1/2/15120130/ps10x_data_dictionary_tbl.pdf) is a helpful resource in understanding fields and relationships between tables. 

