# Key Tables, Measures, and Metrics

Within this chapter we will find the tables and metrics IDEA Public Schools stakeholders are the most interested about.

## The Students Table

The Students table is probably the most important table of all. It contains historical student personal data since IDEA's inception. 

We use this table to know to what subpopulation a student belongs to. Also, we can link this table to other student data using the _[StudentNumber]_ field.

**Table:** [1065574-SQLPRD1].[PROD1].[Schools].[Students]

**Main fields:** [AcademicYear] [StudentNumber] | [SchoolNumber] | [GradeLevelID] | [EnrollmentStatus] | [Gender] | [SPED] | [ELLCode] | [PrimaryDisabilityCode ] | [SecondaryDisabilityCode ] | [TertiaryDisabilityCode ] | [EconomicDisadvantageCode] | [FederalHispanicFlag] | [FederaRaceI] | [FederaRaceA] | [FederaRaceB] | [FederaRaceP] |  [FederaRaceW] |[EntryDate] | [ExitDate] | [RowIsCurrent]

**Who is what?:** 

* **_[EnrollmentStatus]:_** 
  + If EnrollmentStatus = 0, it means that the student is/was an active student for the corresponding academic year,
  + If EnrollmentStatus = 2, it means the student left the district before completing the academic year,
  + If EnrollmentStatus = 3, it means the student graduated from IDEA's high school.

* **_[ELLCode]:_** English Learner (EL) A.K.A. Limited English Proficiency (LEP)
  + If ELLCode = 0, student **is not** identified as an EL student,
  + If ELLCode = 1, the student **is** identified as an EL student,
  + If ELLCode not in (0,1), it means the student is not currently identified as an EL but was identified before as one,
  + If ELLCode = "F", the student **is not** identified as an EL student but is in the First Year of Monitoring status,
  + IF ELLCode = "S", the student **is not** identified as an EL student but is in the Second Year of Monitoring status,
  + If ELLCode = 3, the student **is not** identified as an EL student but is in the Third Year of Monitoring status,
  + If ELLCode = 4, the student **is not** identified as an EL student but is in the Fourth Year of Monitoring status,
  + If ELLCode = 5, the student **is not** identified as an EL student but was at one point more than 4 years ago.
  
* **EL Reclassification** An EL who has passed the Texas English Language Proficiency Assessment System (TELPAS) will be reclassified as a Non-EL and monitored for 4 years. After reclassification the students ELLCode will not change to 0, it will move through the codes for the 4 years of monitoring status and then change to an ELLCode of 5  

* **_[SPED]:_** SPED students are composed of RISE and Life Skills students, not including Critical Student Intervention (CSI) students.
  + If SPED = 0, it means the student is not identified as a SPED student, otherwise then SPED = 1.
  
* **_[InstructionalSettingCode]:_** Students who are classified as SPED may receive additional services. Their services can be found in the **InstructionalSettingDescription** column, which is linked to a specific [PEIMS code](http://ritter.tea.state.tx.us/peims/standards/1314/c035.html):

  InstructionalSettingCode | Description
  -------------------------|-------------
 0 | No additional SPED services
 1 | Homebound
 40 | Mainstream
 41 | Resource Room/Services < 21%
 42 | Resource Room/Services 21- <50%
 43 | Resource Room/Services 50- 60%
 44 | Resource Room/Services > 60%
 
  + If InstructionalSettingCode = 43 or 44, the student is typically in RISE.

* **_[DisabilityCode]_**

  DisabilityCode | Description
  ---------------|-------------
 0 | No disability
 1 | Orthopedic impairment
 2 | Other health impairment
 3 | Auditory impairment
 4 | Visual impairment
 5 | Deaf-Blind
 6 | Intellectual Disability
 7 | Emotional disturbance
 8 | Learning disability
 9 | Speech impairment
 10 | Autism
 13 | Traumatic brain injury
 14 | Noncategorical early childhood

```{sql, eval=FALSE}
SELECT DISTINCT 
 [StudentNumber], 
 [PrimaryDisabilityCode] AS [DisabilityCode], 
 [PrimaryDisabilityDescription] AS [DisabilityDescription]

 FROM (

SELECT DISTINCT [StudentNumber],[PrimaryDisabilityCode], [PrimaryDisabilityDescription]

 FROM [PROD1].[Schools].[Students]
 WHERE AcademicYear='2018-2019'

 UNION ALL

SELECT DISTINCT  [StudentNumber], [SecondaryDisabilityCode], [SecondaryDisabilityDescription]

 FROM [PROD1].[Schools].[Students]
 WHERE AcademicYear='2018-2019'

 UNION ALL

SELECT DISTINCT  [StudentNumber], [TertiaryDisabilityCode], [TertiaryDisabilityDescription]

 FROM [PROD1].[Schools].[Students]
 WHERE AcademicYear='2018-2019') A WHERE [PrimaryDisabilityDescription] <> '' ORDER BY StudentNumber


```

* **_[EconomicDisadvantageCode]:_** A.K.A ECD 
  + If EconomicDisadvantageCode = 0, then the student is _not identified as economically disadvantaged_, otherwise the student is identified as **economically disadvantaged**,
  + If EconomicDisadvantageCode = 1, then the student is identified as an economically disadvantaged and eligible for free meals,
  + If EconomicDisadvantageCode = 2, the student is also identified as an economically disadvantaged but only eligible for reduced-price meals,
  + If EconomicDisadvantageCode = 99, it means the student is identified as an economically disadvantaged one but has another economic disadvantage.
  
* **ECDFlag** Consolidates the codes above to 0/1 indicator
  + If ECDFlag = TRUE (1), the student **is** identified as economically disadvantaged,
  + If ECDFlag = FALSE (0), the student **is not** identified as economically disadvantaged.


* **_Race / Ethnicity:_** Same like in TX
  + If the student identifies as Hispanic or Latino then the student is classified as Hispanic/Latino,
  + If the student **does not** identify as Hispanic or Latino and only selected one race the student is classified as that race,
  + If the student **does not** identify as Hispanic or Latino and selected more than one race, then the student is classified as Two or more races.

* **_[RowIsCurrent]:_** Indicates the most current row for a student, **Only to be used on the current school year**

### Common queries

We frequently need to disaggregate data by year, region, school, and so on, while retaining basic information about a student. The Students table does not directly contain the school or region names, so we need to join the Schools table and Regions table. 

Two common queries involving students are 

1. producing a *roster* of all students that fit certain characteristics and
2. *counting* the number of students that fit certain characteristics.

Suppose we are tracking the number of seniors on track to go to college. The following are two queries to pull a *roster* of currently enrolled seniors:

* SQL example:
```{sql, eval=FALSE}
SELECT [AcademicYear]
	  ,C.[RegionDescription]  
	  ,B.[SchoolName]
	  ,[GradeLevelID]
	  ,[StudentNumber]
    ,[StudentFullName]
    ,[FirstName]
    ,[MiddleInitial]
    ,[LastName]

--join Schools and Regions tables
  FROM [RGVPDSD-DWPRD1].[PROD1].[Schools].[Students] AS A
    LEFT JOIN [RGVPDSD-DWPRD1].[PROD1].[Schools].[Schools] AS B
      ON A.[SchoolNumber] = B.[SchoolNumber]
    INNER JOIN [RGVPDSD-DWPRD1].[PROD1].[Schools].[Regions] AS C
      ON B.[RegionID] = C.[RegionID]

--filter for currently enrolled seniors in 2021-2022
  WHERE [AcademicYear] = '2021-2022'
    AND [GradeLevelID] = '12'
    AND [ExitDate] > '2022-05-01'
    AND [RowIsCurrent] = '1'

--optional, filter RISE students
--  AND [InstructionalSettingCode] NOT IN ('43', '44')
    
```

* R example:
```{r r_current_seniors, echo=TRUE, eval=FALSE}
get_students() %>%

# filter for currently enrolled seniors in 2021-2022
  filter(AcademicYear == "2021-2022",
         GradeLevelID == "12",
         ExitDate > "2022-05-01",
         RowIsCurrent == 1) %>%
  
# optional, filter RISE students
# filter(!(InstructionalSettingCode %in% c("43", "44"))) %>%
  
# join Schools and Regions tables
  inner_join(get_schools(),
             by = c("SchoolNumber" = "SchoolNumber")) %>%
  inner_join(get_regions(),
             by = c("RegionID" = "RegionID")) %>%
  
  select(AcademicYear,
         RegionDescription,
         SchoolName,
         GradeLevelID,
         StudentNumber,
         StudentFullName,
         FirstName,
         MiddleInitial,
         LastName)
```

However, to *count* the number of seniors by school and region, group the students by those characteristics, and then use an aggregate function to count the number per group.

* SQL example:
```{sql, eval=FALSE}
SELECT [AcademicYear]
    ,CASE
	     WHEN C.[RegionDescription] IN ('Austin', 'San Antonio') THEN 'Central Texas'
		   WHEN C.[RegionDescription] IN ('Lower Valley', 'Mid Valley', 'Upper Valley') THEN 'Rio Grande Valley'
		   ELSE C.[RegionDescription]
	   END AS [Area]
	  ,C.[RegionDescription]
	  ,B.[SchoolName]
	  ,[GradeLevelID]
	  
--count number of student IDs to get total per group
	  ,COUNT([StudentNumber]) AS [EnrolledSeniors]
	  
  FROM [RGVPDSD-DWPRD1].[PROD1].[Schools].[Students] AS A
	LEFT JOIN [RGVPDSD-DWPRD1].[PROD1].[Schools].[Schools] AS B  
		ON A.[SchoolNumber] = B.[SchoolNumber]  
	INNER JOIN [RGVPDSD-DWPRD1].[PROD1].[Schools].[Regions] AS C 
		ON B.[RegionAltID] = C.[RegionID]  
		
  WHERE [AcademicYear] = '2022-2023' 
	  AND [ExitDate] > '2022-08-17'
	  AND [RowIsCurrent] = '1'
	  AND [GradeLevelID] = '12'
--  AND [InstructionalSettingCode] NOT IN ('43', '44')

--grouping variables to determine aggregates	
  GROUP BY [AcademicYear]
      ,[Area]
		  ,C.[RegionDescription]
		  ,B.[SchoolName]
		  ,[GradeLevelID]
  ORDER BY [Area]
		  ,[RegionDescription]
      ,[SchoolName]
```


* R example:
```{r count_seniors, eval=FALSE}
get_students() %>%
  
  filter(AcademicYear == "2022-2023",
         GradeLevelID == "12",
         ExitDate > lubridate::today(),
         RowIsCurrent == 1) %>%
  # filter(!(InstructionalSettingCode %in% c("43", "44"))) %>%
  
  inner_join(get_schools(),
             by = c("SchoolNumber" = "SchoolNumber")) %>%
  inner_join(get_regions(),
             by = c("RegionAltID" = "RegionID")) %>%

  # grouping variables to determine aggregates  
  group_by(RegionDescription,
           SchoolName,
           GradeLevelID) %>%
  
  # count number of students to get total per group
  summarize(EnrolledSeniors = n()) %>%
  
  mutate(Area = case_when(
    RegionDescription %in% c("Lower Valley", "Mid Valley", "Upper Valley") ~ "Rio Grande Valley",
    RegionDescription %in% c("Austin", "San Antonio") ~ "Central Texas",
    TRUE ~ RegionDescription
  )) %>%
  arrange(Area,
          RegionDescription,
          SchoolName)
```


::: {.gotcha}
As IDEA continues to expand, it is important to reexamine these tables to incorporate differences in demographics coding, nomenclature, etc., to account for states and regional authorizers using different conventions.
:::


## Schools and regions

### Schools table

The Schools table provides essential information about each school, including their contact information, full and short names, and regions, all of which can be connected to other tables via the SchoolNumber.

**Database:** [RGVPDSD-DWPRD1].[PROD1].[Schools].[Schools]

**Main metrics:** School names, PowerSchool numbers, state IDs, regions

**Important columns:**

* **[SchoolNumber]:** official PowerSchool school ID; most frequently used to link to other tables
* **[SchoolName]:** full name of the school, e.g. IDEA Frontier College Preparatory
* **[SchoolShortName]:** campus name, e.g. Frontier
* **[StateSchoolNumber]:** number used by the campus' respective state for reporting (note: this column may not have correct values for all schools)
* **[SchoolAbbreviation]:** abbreviation of school name (not commonly used)
* **[SchoolLowestGrade]:** lowest grade level, where PK = -1, K = 0
* **[SchoolHighestGrade]:** highest grade level
* **[SchoolStreet], [SchoolCity], [SchoolState], [SchoolZipCode], [SchoolPhone], [SchoolFax], [PrincipalName], [PrincipalPhone], [PrincipalEmail], [CountyNumber], [CountyName]**: contact information
* **[RegionID]:** official IDEA region when describing location of school; links to the Regions table
* **[VPofSchools]:** regional Vice President (VP) who oversees the school; links to Employees table using EmployeeKey
* **[ExecutiveDirector]:** Executive Director (ED) OR Regional Superintendent who oversees the region; links to Employees table using EmployeeKey
* **[RegionDirectorOfOperations]:** Regional Director of Operations (RDO) who oversees the region; links to Employees table using EmployeeKey
* **[CollegeSuccessDirector]:** Director of College Counseling (DCC) at the school; links to Employees table using EmployeeKey
* **[Area]:** describes if the school is under the division of RGV, Texas, or IPS & SA, i.e. which superintendent oversees the school
* **[RegionAltID]:** describes the Subregion, if applicable (default is Region); mostly used for RGV schools to divide into Lower, Mid and Upper Valley; links to the Regions table
* **[IsDeprecated]:** 0/1 indicator of open vs. closed schools or old vs. new school numbers. Oscar Dunn is coded as 1, since it closed at the end of 2021-2022. The two Tampa schools (Hope and Victory) have 4 rows, 2 with the school number used in 2021-2022 and 2 with the new school numbers being used in the 2022-2023 school year. Note: If you are needing data on Hope or Victory for 2021-2022 you will need to use the old school numbers where IsDeprecated == 1. If you want data for the current year, you will need to filter to IsDeprecated == 0. 

**Context:**

* The [SchoolName] column is the PowerSchool name for the school, not necessarily its official name. Many high school names in PowerSchool are shortened to "College Prep" instead of "College Preparatory". Some schools may officially be named "IDEA Academy/College Preparatory [ShortName]". 
* Note that both Middle School and High School are administered under a single College Preparatory; however, they receive separate PowerSchool instances.
* For historical reasons, IDEA Academy Donna is labeled as "IDEA Academy Primary", IDEA Middle School Donna is labeled as "IDEA Academy", and IDEA College Preparatory Donna is labeled as "IDEA College Preparatory". (It is suggested you rename these to the modern names).
* IDEA San Juan Academy is [physically in a different location](https://ideapublicschools.org/our-schools/idea-san-juan/) than is IDEA San Juan College Prep, rather than being on the same campus. Systems and individuals that are traditionally shared between campuses, such as the APO, are distinct for each school. Their [SchoolShortName] is also different - "San Juan AC" and "San Juan CP", respectively. (It is suggested you rename these both to be "San Juan" when pulling in data.)
* IDEA Allan was renamed as IDEA Montopolis, so older records may still reflect this name.
* IDEA Travis Academy is an in-district partnership with Midland ISD. Records from this school may not necessarily be accurate or interpretable, since values may reflect coding choices at Midland ISD.


### Regions table

The Regions table provides keys for each RegionDescription, which can be connected to other tables via the RegionID.

**Database:** [RGVPDSD-DWPRD1].[PROD1].[Schools].[Regions]

**Main metrics:** region numbers, region names, state, parent region (if applicable)

**Important columns:**

* **[RegionID]:** region number, used to link back to the [Schools] table
* **[RegionDescription]:** full name of the region or subregion
* **[State]:** full name of the state
* **[ParentRegion]:** link to the region number that contains that subregion; 0 if not usually combined with other regions

**Context:**

* IDEA originally started as a single school in Donna, TX in 2000, and made its first expansions to Quest and Frontier in 2006. The notion of a regional structure outside of the Rio Grande Valley (RGV) did not start until IDEA expanded in 2012, when Allan (now Montopolis) opened in Austin, and Carver, in San Antonio.
* IDEA did not open outside of Texas until the Southern Louisiana (SOLA) region opened its first schools, Bridge and Innovation, in 2018.
* Informally, the RGV "super region" can be divided into Lower, Mid, and Upper Valley, and these subregions all have the same parent region (RGV). At some point, IDEA formally divided RGV schools into Lower and Upper Valley regions, each with a separate Executive Director, but the Lower/Upper distinction is no longer officially used. The RGV may also be considered an Area, governed by the Area Superintendent rather than an Executive Director. 
* Occasionally, the Central Texas "super region" is used to include the San Antonio and Austin regions, but this is not included in the Regions table.
* An Area is the portfolio of a superintendent, which includes multiple regions. The three Areas are "RGV", "Texas", and "IPS and SA", which are included in the [Schools] table (see above entry).


::: {.gotcha}
Different teams across IDEA may or may not split their portfolios using the Lower/Upper or the Lower/Mid/Upper subregions. Before assigning regions to RGV schools, check with the partnering team to see how they categorize RGV schools.

If the team does NOT use the subregions, then link the Regions and Schools table using [Regions].[RegionID] = [Schools].[RegionID]. If the team uses the Lower/Mid/Upper subregions, then link using [Regions].[RegionID] = [Schools].[RegionAltID]. If the team uses the Lower/Upper subregions, you will need to ask the team how they divide their portfolio of schools.
:::


## Student Attendance

Daily attendance is taken through PowerSchool to see which students are presently at school or not. By default, a student is assumed present until they are marked *absent (A)* by the teacher, or as *tardy present (TP)* at the school's front office. Schools will typically take a preliminary attendance snapshot within the first available period (i.e. either homeroom or 1st period) to determine which families need to be called. Then, each school designates an **Official Attendance-Taking Time or Period (OATT/OATP)**, a roughly 10-minute window for all teachers with a class at that time to submit the day's official attendance roster. The OATP snapshot is what is sent to the state for Average Daily Attendance (ADA) counts and is what is recorded in our warehouse.

**Database:** [RGVPDSD-DWPRD1].[PROD1].[Attendance].[Students]
* **Current Year + 3 Past Years** This table currently has data for the current year (2022-2023), and the last three years data (2021-2022, 2020-2021, & 2019-2020). Once the current year is complete and the next year begins, 2019-2020 data will drop off of this table and the new academic year will be added.

**For Attendance data Prior to 2019-2020:**
**Database:** [RGVPDSD-DWPRD1].[PROD1].[Attendance].[StudentsHistorical]
* **Academic Years: 2021-2022 - 2015-2016** This is the official historical table. All of the same columns as the [Attendance].[Students] table. Was told that when the academic year ends the data will roll over to this table.

**Main metrics:** Full-day vs. half-day attendance; number of absences; new to IDEA students

**Important columns:**

* **[AcademicYear]**: yyyy-yyyy
* **[SchoolName]**: IDEA [Campus] [SchoolType]
* **[schoolnumber]**: Note that the column name is stored in all lowercase.
* **[SchoolShortName]**: Campus name only
* **[SchoolType]**: Academy or College Prep
* **[RegionDescription]**
* **[GradeLevelID]**: Uses the values -1 and 0 for PK and K, respectively.
* **[GradeLevel]**: Uses the labels PK and K.
* **[AttStudentKey]**: Identifying key for each student for each day.
* **[WeekNumber]**: Gives the week of the school year.
* **[DateNumber]**: Gives the day number of the school year (1, 2, 3, ..., 180). Note that some schools have different start dates and DateNumber begins counting on the school's FDOS for the year. 
* **[AttDate]**: Gives the actual date of the record. 
* **[Membership]**: Indicates 1.00 for a full-day student (nearly everyone) and 0.50 for a half-day student (Pre-K only).
* **[Absences]**: Indicates 1.00 for a full-day absence, or 0.50 for a half-day absence.
* **[VAbsences]**: Unsure if this is different from [Absences].
* **[SchoolYear]**: Indicates 1 for a new-to-IDEA student, or 2+ for a returning student.
* **[SchoolTypeOperation]:** DO NOT USE. This information has been found to be inaccurate. We will work on creating a table that has the corrected information by school and academic year, but for now, "Launching", "Scaling", and "Fully Scaled" operation data is stored in an Excel file in the Enrollment GitHub repo.


### Common queries

To pull student daily attendance for the current school year at a specific school on a certain date, you can use a few functions to pull the data:

```{r attendance_ex, echo=TRUE, eval=FALSE}
current_school_year <- "2021-2022"
our_school <- "IDEA Walzem Academy"
date <- "2022-04-01"

# gets individual attendance records by student

get_student_daily_attendance() %>%  # this is a built-in function for {ideadata}
  filter(AcademicYear == current_school_year,
         SchoolName == our_school,
         AttDate == date) 

```


Average daily attendance (ADA) is computed as the ratio of present students to enrolled students, which can be found using:

```{r ada_ex, echo=TRUE, eval=FALSE}
get_student_daily_attendance() %>%
  rename(SchoolNumber = schoolnumber,
         Region = RegionDescription) %>%
  
# pick a specific school and academic year 
  filter(AcademicYear == current_school_year,
         SchoolName == our_school) %>%

# groups by school for a particular day  
  group_by(AcademicYear,
           Region,
           SchoolNumber,
           SchoolName,
           SchoolShortName,
           SchoolType,
           AttDate,
           DateNumber) %>%
  distinct() %>%
  summarise(n_enrolled = sum(Membership),
            n_absent = sum(Absences)) %>%

# computes ADA as 1 - absence ratio
  mutate(ada_percent = 1 - (n_absent/n_enrolled)) %>% 
```

If you are needing attendance data for enrollment purposes for years prior to 2015-2016, you will need to use a different table.

**Database** [RGVPDSD-DWPRD1].[PROD1].[ADA].[StudentDailyMembership]
This table contain daily attendance for 2002-2003 through 2020-2021 Academic Years. However, the data for 2016-2017 through 2018-2019 appears to be incorrect. Only use this table for daily attendance prior to 2015-2016 School Year. There are far fewer columns on this table, but the ones needed for calculating ADA and/or enrollment for years prior to 2015-2016 are there.

**Important columns:**

* **[AcademicYear]:** yyyy-yyyy
* **[SchoolNumber]:** Local school number
* **[SchoolName]:** Long school name
* **[StudentNumber]:** Local student ID number
* **[GradeLevelID]:** Numeric grade level, where Pre-K == -1 and K == 0
* **[Membership]:** Indicates 1.00 for a full-day student (nearly everyone) and 0.50 for a half-day student (Pre-K only)
* **[AttDate]:** Gives the actual date of the record


### Other uses

Attendance can also be used to obtain **daily total enrollment** at multiple levels, including by grade level at each school.

```{r enrollment_ex, echo=TRUE, eval=FALSE}
get_student_daily_attendance() %>%
  rename(SchoolNumber = schoolnumber,
         Region = RegionDescription) %>%
  
# pick a specific school and academic year 
  filter(AcademicYear == current_school_year,
         SchoolName == our_school) %>%
  
# groups by grade level at a school per day  
  group_by(SchoolNumber,
           SchoolName,
           SchoolShortName,
           SchoolType,
           GradeLevelID,
           AttDate,
           DateNumber) %>%                     
  distinct() %>%
  
# counts total number of students enrolled  
  summarise(n_enrolled = sum(Membership))

```


The other side of enrollment is **student persistence**. To see how enrollment is used with student persistence, jump to [the student persistence section](https://idea-analytics.github.io/r_and_a_manual/key-tables-measures-and-metrics.html#student-persistence-using-attendance-tables-a.k.a.-cohort-persistence).


## Student Persistence

Student Persistence is one of the most important measures that Chiefs, VPs, and School Leaders are always monitoring to understand our efforts in providing high quality services to our students and families. In order for a student to actualize the benefits of IDEA the student must persist through high school. We say a student persisted if the student was enrolled and attended school for the entire academic school year, returned the following school year, and was enrolled and attending on the Monday after the first week of school (the First Day of Persistence FDOP). In other words, the student was enrolled and attended school on the "First Day of Persistence (FDOP)" for two consecutive years. The only exception to this are New Students that enroll after the FDOP, attend the entire year, return the following year, and are enrolled and attending for FDOP. These new students were not enrolled and attending for 2 consecutive FDOPs because they enrolled after the first FDOP but would be considered as persisting.

Example: John enrolled in IDEA Public Schools on August 19, 2019, and attended school on FDOP, which is the Monday after the first week of school. John attended the entire 2019-2020 academic year and was enrolled and attending on the FDOP of the following academic year (FDOP for 2020-2021 school year). If John was enrolled but did not attend on FDOP because he didn't return until September, John would not count as a student who persisted.

**Database:** [1065574-SQLPRD1].[Persistence]

**Main metrics:** District, Regional, School, Sub-population, Grade Level

**General Formula:** 1 â€“ (Leavers/All Students)

**Tables:**

* **_[dbo].[PersistenceCode]:_** This table is used to calculate the persistence rate for the current academic year. The data is updated every three hours, weekdays. LeaverWeek column shows the week the student left the district (beginning the week containing the FDOP, which is the second week of the academic year)? After the end of the current school year but before the persistence year has ended (the summer months), the week the student leaves the district is captured in the ULeaverWeek column. The PersistenceWeek column shows the current week of persistence for the school and region that the student attends (this week will be different across schoools and regions due to differing start dates and FDOPs).

* **_[dbo].[PersistenceHistorical]:_** This table contains the final persistence data for previous persistence years starting with 2018-2019 and up through 2020-2021. This table is updated with the previous years' data after that year is closed out and the next persistence year has started.

**Context:**

* **Who Counts?** 

To calculate persistence, we need to filter out all students who should be excluded from the denominator, filter(EXCLUDE == 0, FDOPCOUNTP == 1). This limits the denominator to only those students that should be counted. These two filters should be used when looking at current **or** historical data.

* **Who is a Leaver?**

All leavers are coded as FDOPLEAVER = 1, if FDOPLEAVER = 0 then they are a current student.

* **Other Useful Fields:**
  + **AcademicYear:** the academic year
  + **RegionDescription:** the region the student is in
  + **SchoolShortName:** the school the student attends
  + **SchoolType:** Academy of College Prep
  + **GradeLevelID:** Student's grade level (integer ranging from -1 = Pre-K to 12)
  + **NewStudent:** indicates if the student is new (1) or returning (0)
  + **PersistenceWeek:** the current week of persistence based on the student's region and school (ranges from 1-52)
  + **LeaverWeek:** the persistence week the student left the district
  + **ULeaverWeek:** the persistence week the student left the district if it was after the last day of school
  + **PersistenceCode:** the reason the student left the district
  + **PersistenceCategory:** the category that the reason falls into for why the student left the district
  + **PersistenceComment:** any additional comments the SIS clerk has for why the student left the district
  + **EnrollmentStatus:** contains the enrollment status, but is not accurate **Do Not Use**
  + **EntryDate:** the date the student entered school for a particular academic year or the date the student started school at a new campus
  + **ExitDate:** the last day of the academic year (last day of school) or the day the student left the district (summer leavers are sometimes snapped back to the last day of school, beware)
  + **Recapture:** 0/1 indicator of students that have left and then returned to the district. 1 = recaptured, 0 = not recaptured. **Not to be Trusted**
  + **SummerLeaver:** 0/1 indicator for students that leave during the summer
  + **StudentGender:** "M"/"F" gender indicator
  + **Race:** race indicator combined with ethnicity; ex. "WHITE-HISPANIC"
  + **IsHispanic:** indicates whether the student identifies as Hispanic; values = "HISPANIC" or "NON-HISPANIC"
  + **Migrant:** 0/1 indicator of student's migrant status
  + **LEP:** 0/1 indicator of student's English Learner status. **Inaccurate Don't Use**
  + **Sped:** 0/1 indicator of student's SPED status
  + **EconDisad:** 0/1 indicator of student's economic disadvantaged status. **Inaccurate Don't Use**
  + **IsCSI:** 0/1 indicator for students identified as CSI
  + **ContinuousEnrollment:** 0/1 indicates whether the student has been continuously enrolled
  
### Student Persistence using Attendance Tables (A.K.A. Cohort Persistence)

Occasionally student persistence is requested for school years that are not captured in the Persistence Tables, i.e., persistence data prior to 2018-2019, or for cohorts of students overtime. When this occurs the Attendance tables can be used to "snapshot" whether a student is attending at a particular date at the beginning of the school year in question and is still attending at a particular date in the beginning of the following school year. This way of calculating persistence has its drawbacks, mainly that it is not as precise as persistence calculated with the persistence table, results will differ slightly depending on the "snapshot" date selected. 

**Database:** [1065574-SQLPRD1].[PROD1]

**Tables:**

* **_[Attendance].[Students]:_** This table contains daily attendance data from 2017-2018 school year to present. To see what students are attending on a specific date use the AcademicYear column to select the specific school year and AttDate to select the date (in YYYY-MM-DD format). Be sure to filter before collecting the data due to the large size of the table, contains a row for every school day every student attended for all years contained in the table.

* **_[ADA].[StudentDailyMembership]_** This table contains daily attendance data for 2016-2017 and prior. Contains some of the same columns as the Attendance.Students table, such as AttDate, AcademicYear, and SchoolNumber. To see what students are attending on a specific day you would use the same profess described above.

* **Other Useful Fields:**
  + **AcademicYear:** the academic year (in both tables)
  + **SchoolName:** the long school name (in both tables)
  + **schoolnumber:** the school number (in the StudentDailyMembership (2016-2017 & prior) table it is SchoolNumber)
  + **SchoolShortName:** the school's short name (ex. Donna; Only in the Attendance.Students table - 2017-2018 to present)
  + **SchoolType:** indicates Academy or College Prep (Only in the Attendance.Students table - 2017-2018 to present)
  + **RegionDescription:** the region (Only on the Attendance.Students table - 2017-2018 to present)
  + **GradeLevelID:** student's grade level (in both tables)
  + **WeekNumber:** the number of the week in the academic year (in both tables) **Use Cautiously** the weeks are **NOT** numbered properly in 2019-2020 contains in the Attendance.Students table
  + **AttDate:** the date for the attendance record (format "YYYY-MM-DD") (in both tables)
  + **Membership:** indicates full-time = 1.0 or half-time = .5 (half-day pre-k students)
  + **SchoolTypeOperation:** indicates if the school is "FULL SCALE", "SCALING", "or "LAUNCHING" (Only in the Attendance.Students table - 2017-2018 to present) and applies to Academies and College Preps individually


## College Application & Matriculation Metrics

To close the opportunity gap, IDEA Public Schools is committed to a vision of College for All Children. The College Success Team (CST) lead the initiative of monitoring and identifying the best College/University for each senior student, this is possible with data dashboards that use [Naviance](https://www.hobsons.com/solution/naviance/) information.

**Database:** [1065574-SQLPRD1].[PROD1].[Colleges]

**Main metrics:** Percent at least 1 application, Percent at least 1 submission, Percent at least 1 acceptance

**Most important columns:** [Stage] | [ResultCode]

**Tables:**

* **_[Colleges]:_** Here we will find all colleges/universities. What makes this table important is the [CEEB] field which is an ID that can help us connect distinct college related data sources like Naviance with National Student Clearinghouse data!

* **_[CollegeTuition]:_** Besides having in-state tuition and out-of-state tuition data, we can also find two other unique codes ([OPEID] and [ACTCode]), that will aid us to college data across different data sources. 

* **_[EDocs]:_** Details about  application submitted e-documents.

* **_[StudentCollegeApplication]:_** This is the main table. We use it to calculate most of the metrics the College Application & Matriculation dashboard has. We can find data from 2018 up to the current academic year.

* **_[StudentCollegeApplicationsSummary]:_** 

* **_[StudentScholarships]:_** 


**Context:**

* **Who Counts?** 



* **Who is a 4year 2year?**




## Lottery Data

IDEA uses [Stream](https://stream.ideapublicschools.org/admin/login) to capture information about enrollment applications, lottery, contracts, and enrollment goals. There are three main tables: **Daily, Historical, and Snapshot**. 

* The **Daily** table provides daily application data from the Stream table for the current academic year.
* The **Historical** table provides the same information as **Daily** but for previous academic years.
* The **Snapshot** table provides only the submitted, accepted, capacity, and ratios for each school and grade level by day.

**Tables:** 

* [RGVPDRA-DASQL].[StreamLotteryEnrollment].[dbo].[StreamLotteryDaily]
* [RGVPDRA-DASQL].[StreamLotteryEnrollment].[dbo].[StreamLotteryHistorical]
* [RGVPDRA-DASQL].[StreamLotteryEnrollment].[dbo].[StreamLotterySnapshot]

**Main metrics:** Number of contracts available / offered, number of contracts submitted / accepted, ratios of submitted / accepted contracts to capacity

**Important columns in the Daily and Historical tables:**

* **[AcademicYear]**: yyyy-yyyy
* **[Campus]**: equivalent to school short name
* **[School]**: labeled as Academy or College Prep
* **[Grade]**: labeled as PK, K, 1, 2, ...
* **[ApplicantTarget]**: number of student applications targeted, usually a 2:1 ratio of applications to capacity
* **[CapacityContract]**: number of seats available in that grade level
* **[OfferAccepted]**: number of offers accepted (may be higher than capacity)
* **[NonEnrollment]**: 
* **[DuplicateTransfer]**:
* **[TransferStudent]**:
* **[ReturningStudent]**:
* **[NewStudent]**:
* **[UnacceptedApplicant]**:
* **[Submitted]**:
* **[RatioAcceptance]**: calculated as [OfferAccepted] $\div$ [CapacityContract]
* **[RatioSubmission]**: calculated as [Submitted] $\div$ [CapacityContract]
* **[Active]**: 1 = grade level will have non-zero enrollment for that academic year (i.e. there are students in that grade level, but the school may or may not be accepting applications); 0 = grade level will not be enrolled for that academic year (i.e. there are no students in that grade level, and the school is not accepting applications for that grade level)
* **[DateStamp] (only in Daily)**: when the data was pulled from Stream

**Important columns in the Snapshot table:**

* **[Date]**: yyyy-mm-dd
* **[Campus]**: equivalent to school short name
* **[School]**: labeled as Academy or College Prep
* **[Grade]**: labeled as Pre-K, Kindergarten, 1st, 2nd, ...
* **[Submitted]**: 
* **[AcceptedOffers]**:
* **[CapacityContract]**: number of seats available in that grade level
* **[RatioAcceptance]**: calculated as [OfferAccepted] $\div$ [CapacityContract]
* **[RatioSubmission]**: calculated as [Submitted] $\div$ [CapacityContract]



## Critical Student Intervention (CSI) Identification {#csi}

To address the achievement gap among students, Dolores Gonzalez, our Chief Program Office introduced the Critical Student Intervention (CSI) program in 2014. Currently, along with her team, Tricia Lopez our VP of Special Programs lead IDEA Public Schools in the effort of reducing the performance gaps. To help Tricia's team, the Software Development team put together a table where we can find the students who are enrolled in a CSI math and/or reading intervention program. 

**Database:** [1065574-SQLPRD1].[PROD1].[Schools]

**Main metrics:** Percent of CSI students by school

**Most important columns:** [ProgramID] | [InterventionType]

**Tables:**

* **_[StudentCSI]:_** This table will aid us to identify the students who are/were enrolled in math or reading CSI intervention programs. Since sometimes there is need of data adjustments from one year to another, the [ProgramID] number might change. However, this should not be a problem because we can always use the [InterventionType] field to correctly associate an intervention program to either math or reading.

* **SQL Code Example:**

```{sql, eval=FALSE}
SELECT DISTINCT AcademicYear, [Subject], ProgramID
FROM (SELECT [AcademicYear]
      ,[ProgramID]
      ,CASE 
	  WHEN [InterventionType] LIKE '%math%' THEN 'Math'
	  WHEN [InterventionType] LIKE '%reading%' THEN 'Reading' ELSE NULL END AS [Subject]
  FROM [1065574-SQLPRD1].[PROD1].[Schools].[StudentCSI]
  WHERE [AcademicYear] = '2018-2019' AND (InterventionType LIKE '%math%' OR InterventionType LIKE '%reading%')) AS A
GROUP BY AcademicYear, [Subject], ProgramID
```

* **_[StudentCSIDetails]:_** Here we find detailed information about the students who are or were part of a CSI program. We can either use this table to get a student count per school, or we could match the **_[StudentCSI]_** table data to the Students table and get the percent of CSI students per school.

* **SQL Code Example:**

```{sql, eval=FALSE}
SELECT A.StudentNumber, A.SchoolNumber, B.SchoolName, D.[Subject]
FROM [1065574-SQLPRD1].[PROD1].[Schools].[Students] AS A
INNER JOIN [1065574-SQLPRD1].[PROD1].[Schools].[Schools] AS B
ON A.SchoolNumber = B.SchoolNumber
LEFT JOIN [1065574-SQLPRD1].[PROD1].[Schools].[StudentCSI] AS C
ON A.StudentNumber = C.StudentNumber AND A.AcademicYear = C.AcademicYear
LEFT JOIN 
(SELECT DISTINCT AcademicYear, [Subject], ProgramID
FROM (SELECT [AcademicYear]
      ,[ProgramID]
      ,CASE 
	  WHEN [InterventionType] LIKE '%math%' THEN 'Math'
	  WHEN [InterventionType] LIKE '%reading%' THEN 'Reading' ELSE NULL END AS [Subject]
  FROM [1065574-SQLPRD1].[PROD1].[Schools].[StudentCSI]
  WHERE [AcademicYear] = '2020-2021' AND (InterventionType LIKE '%math%' OR InterventionType LIKE '%reading%')) AS A
GROUP BY AcademicYear, [Subject], ProgramID) AS D
ON C.ProgramID = D.ProgramID
WHERE A.AcademicYear = '2020-2021'
```

## Assessment Data

### State of Texas Assessments of Academic Readiness (STAAR)

The STAAR assessments are Texas' standardized tests administered every year, except for 2019-2020 due to COVID-19. Grades 3-8 take the Reading and Math assessments every year with Writing, Science and Social Studies assessed at specific grades. Testing schedule is as follows:

  + **Grade 3:** Reading and Math
  + **Grade 4:** Reading, Math, and Writing
  + **Grade 5:** Reading, Math, and Science
  + **Grade 6:** Reading and Math
  + **Grade 7:** Reading, Math, and Writing
  + **Grade 8:** Reading, Math, Science, and Social Studies

Assessments are provided in both English and Spanish up to the 5th grade, after which the test is only in English. Students in High School (and some middle school students) complete End-of-Course (EOC) assessments to fulfill graduation requirements. In order to graduate in Texas a student must have taken and passed 5 EOC assessments: Algebra I (taken in 8th grade at IDEA), Biology, English I, English II, and United States History. Students with significant cognitive disabilities would take the STAAR Alternate 2 (STAAR Alt 2) rather than the STAAR. This alternate test is administered by the teacher and is usually excluded from our analyses on academic achievement.

All STAAR assessments, excluding STAAR Alt 2, have 3 performance standards: Approaches, Meets, and Masters. When the assessments were rolled out the passing standard was going to be moved up to the Meets standard after a few years when more than half of students were meeting the Approaches standard. This has not yet happened so the passing standard remains at the Approaches performance level. These three performance levels are **NOT** mutually exclusive, meaning that students that achieve the Masters standard are included in the Meets count and the Approaches count, and students achieving the Meets standard are included in the Approaches count. The STAAR Alt 2 has two performance levels: Developing, Satisfactory, and Accomplished. The passing standard for the STAAR Alt 2 is the Satisfactory performance standard. Again these performance levels (Satisfactory and Accomplished) are not mutually exclusive categories, students who achieve the Accomplished standrad are included in the counts for Satisfactory.

**Database:** [RGVPDRA-DASQL].[Dashboard]

**Tables:**

* **_[dbo].[STAAR]:_** This table contains all of the STAAR and STAAR Alt 2, 3-8 and EOC assessment data. Data in this table has multiple rows for each student, for each test and each administration that could be up to 3 administrations in one year. For example, a fifth grade student will take reading, math and science in one year, so that's 3 rows for each test. Further, if this student doesn't pass reading or math in March (the first administration) then they would take it again in May (second administration or retake). If they didn't pass it in May they could take it a third time in June. Only 5th and 8th grade students get three opportunities to take Reading and Math in one academic year, this is due to the Student Success Initiative (SSI) requirements that every 5th and 8th grade student pass STAAR Reading and Math to be promoted to the next grade. Although there are exceptions to this rule (i.e., the requirement was waived in 2020-2021), one should expect more than one administration for some students. Below the description of columns is an example in r to get the students best score and de-duplicate to get one row per student.

* **Useful Fields:**

  + **AdminDate:** indicates the two digit month and two digit year of administration for grades 3-8, these are character values. Ex. Spring 2021 administration dates for 3-8 would be "0421" and "0521"; for EOC exams the AdminDate for Spring 2021 is "1521".
  + **GradeLevel:** character values, two digit grade. Ex. "06"
  + **CountyDistrictCampusNumber:** indicates the School Number
  + **LastName:** Student's Last Name
  + **FirstName:** Student's First Name
  + **StudentID:** Student's testing ID number **Not the IDEA Student Number**
  + **RaceReportingCategory:** Student's reported race, Ex. "H" = Hispanic
  + **EcoDisadvantage:** Student's economic disadvantaged status, 0, 1, 2, and 9.
  + **LEP:** Student's Limited English Proficiency status Codes: C = Current LEP, F = 1st year of monitoring, S = 2nd year of monitoring, T = 3rd year of monitoring, R = 4th year of monitoring,  E = former LEP more than 4 years since reclassification, and 0 = Non-LEP
  + **SpEd:** 0/1 indicator for receiving special education services
  + **AtRisk:** 0/1 indicator of at-risk status
  + **LocalStudentID:** Student's 108 number
  + **SubjectCode:** tested subject, i.e., Reading, Math, Science, Algebra I, etc.
  + **TestDate:** equivalent to the AdminDate but **only** for 3-8
  + **ScoreCode:** indicates whether the test was scored or student was absent. **ALWAYS** filter to ScoreCode == "S" to get the scored tests
  + **TestVersion:** "S" = STAAR and "T" = STAAR Alt 2
  + **ScaleScore:** provides the scaled score for that assessment and can be used to find the best score/most recent administration
  + **LevelII:** 0/1 indicator for the Approaches performance standard **Note** When filtered to STAAR ALt 2 this indicates whether or not the student achieved the Satisfactory performance level
  + **LevelIIFinal:** 0/1 indicator for the Meets performance standard **Note** When filtered to STAAR ALt 2 this indicates whether or not the student achieved the Satisfactory performance level
  + **LevelIII:** 0/1 indicator for the Masters performance standard **Note** When filtered to STAAR ALt 2 this indicates whether or not the student achieved the Accomplished performance level
  + **SchoolYear:** character value indicating the academic year, Ex. "2020-2021"
  + **BestRecord:** logical column supposed to indicate if that record is the best for that student for that test **DOES NOT WORK**

* R example:
```{r STAAR, echo=TRUE, eval=FALSE}

STAAR <- get_table(.table_name = "STAAR", .database_name = "Dashboard", 
                   .schema = "dbo", .server_name = "RGVPDRA-DASQL") %>%
  filter(TestVersion == "S", ScoreCode == "S", SubjectCode == "Math",
         AdminDate %in% c("0421", "0521")) %>%
  select(StudentID,
         LocalStudentID,
         GradeLevel,
         SubjectCode,
         AdminDate,
         ScoreCode,
         ScaleScore,
         Approaches = LevelII,
         Meets = LevelIIFinal,
         Masters = LevelIII) %>%
  mutate(StudentID = as.numeric(StudentID),
         LocalStudentID = as.numeric(LocalStudentID),
         StudentNumber = if_else(LocalStudentID %in% c(0, NA), StudentID, LocalStudentID),
         StudentNumber = as.numeric(StudentNumber)) %>%
  filter(StudentNumber != 0) %>%
  select(-StudentID, -LocalStudentID, -ScoreCode) %>%
  group_by(StudentNumber) %>%
  distinct() %>%
  collect()

# To get a count of distinct student numbers to see how much duplication exists
Count_stus <- STAAR %>%
  distinct(StudentNumber)

# Getting max score to de-duplicate
staar_math <- STAAR %>%
  group_by(StudentNumber) %>%
  mutate(Best_score = max(ScaleScore),
         BestScoreFlag = if_else(Best_score == ScaleScore, 1, 0)) %>%
  filter(BestScoreFlag == 1) %>%
  select(-BestScoreFlag, -Best_score, -AdminDate, -ScaleScore, -GradeLevel) %>%
  distinct()

# Calculating the % Approaches and the % Masters
pct_apprhs_mstrs <- staar_math %>%
  select(Approaches, Masters) %>%
  summarize(n_students = n(),
         n_apprchs = sum(Approaches),
         n_mstrs = sum(Masters)) %>%
  mutate(pct_app = n_apprchs/n_students,
         pct_mst = n_mstrs/n_students)

```


### District assessments

IDEA produces in-house assessments for each major content area except for national and state EOY assessments. Typically, courses require all teachers and students to administer the same summative assessments and benchmarks, but formative assessments are left to each teacher to create.

Summative assessments evaluate what a student has learned over time and is usually more high-stakes than other types of assignments. Examples of the most common summative assessments we administer include:

* Unit exams
* End-of-module assessments
* Mid-unit exams (also mid-unit quizzes)
* Mid-module assessments
* Semester exams
* Final exams
* Projects, papers, performances, and portfolios
* Quarterly interim assessments (former)
* Bi-weekly assessments (former)

Formative assessments are used to reveal student progress in the learning cycle and are usually more low-stakes. A small sample of formative assessments we use are:

* Daily exit tickets
* Weekly quizzes
* Progress checks
* Peer feedback
* and more...

In this section, we will only consider district assessments that are *centralized*, meaning that most schools must administer and scan in the results into Illuminate ([Texas](https://ideapublicschools.illuminateed.com/) | [Louisiana](https://ideala.illuminateed.com/live/?Main_NotDashboardPage=&prev_page=Main_NotDashboardPage&page=SisLogin)), our main assessment platform.

#### Illuminate data

The IABWA tables (for **I**nterim **A**ssessments and **B**i**w**eekly **A**ssessments) contain student-level and item-level performance records for any assessment administered on Illuminate, Edulastic, SchoolCity, or SchoolNet. Although we have phased out IAs and BWAs in favor of UEs (unit exams) and EOMs (end of module assessments), the table nevertheless contains all types of assessments scanned in since 2014-2015.

The most common way of entering data into Illuminate is through a scantron-like answer form, which Illuminate automatically generates when an assessment is created. The teacher has the option of generating answer forms either with the individual student ID prepopulated (most common) or with a generic ID key that must be bubbled in (less common).  Students (or teachers, if the response is open-ended) bubble in their responses, upload each answer form via webcam or scanner, until all data is collected.

Illuminate then stores all data at two different levels:

1. student-level, and

2. item-level by student,

each of which is stored in a different IABWA table. 

##### Naming conventions

Most assessments have a standard naming convention:

**SUBJECTCODE_COURSE_SEMESTER_ASSESSMENT**

For example, a 6th grade ELA semester exam in Fall 2021 may have the name **ELA_6thReading_F21_SE**, or the first AP Spanish Language unit exam may have the name **SPA_APSpanishLanguage_F21_UE1**.


::: {.gotcha}
Note that as IDEA continues to expand, some, but not all, exams are now coded as

STATEABBR_SUBJECTCODE_COURSE_SEMESTER_ASSESSMENT

For example, the first 6th grade math unit exam administered in Louisiana may be coded as LA_MAT_6thMath_F22_U1, while the first 6th grade math unit exam in Texas (covering different material) may be coded as TX_MAT_6thMath_F22_U1.

Current state abbreviations include TX, LA, FL, and OH, so it may be prudent to filter rows by state FIRST before finding an appropriate exam to analyze.
:::


A list of the most common codes used is included below:

  Subject Code | Semester | Assessment
  -------------|----------|------------
 ELA - English Language Arts | Fyy - Fall (year) | UE - unit exam
 MAT - Mathematics | SPyy - Spring (year) | EOM - end of module
 SCI - Science | | MUE - mid-unit exam
 HUM or SS - Social Studies | | MM - mid-module
 TECH - Technology | | SE or SEM - semester exam
 RTTC - Road to and through College | | FE or FINAL - final exam
 SPA - Spanish | | MOCK - mock exam
 CSCI - Computer Science | | IA - former interim assessment (final exam coded as IA4)
 TELPAS - TELPAS mock| | BWA - biweekly assessment

Note that each assessment, except for semester and final exams, is typically numbered (see examples). You can use regular expressions to parse these codes and select which exams are needed. Note that different states may use different names for equivalent exams. Similarly, multiple-part exams may deviate from this convention to indicate each part of the assessment. 

##### IABWA

This table aggregates item-level data for each student and reports a student's performance on a single assessment.

**Table:** [1064613-SQLPRD2].[PROD2].[Assessments].[IABWA]

**Main metrics:** Student-level data describing the assessment taken, the overall score, the performance band, and the mastery level

**Assessment columns:** 

* **[IABWAID]:** This number uniquely links a student to an assessment.
* **[ExternalTestID]:** This number uniquely identifies an assessment.
* **[SubjectID]:** This number describes if the subject is math, ELA, etc. 
* **[Subject]:** Same as SubjectID, but in words. 
* **[AssessmentDate]:** Date of the assessment.
* **[AssessmentName]:** Name of the assessment (see naming conventions, above).
* **[AssessmentReporingName]:** One of two columns to choose a type of assessment.
* **[AssessmentType]:** One of two columns to choose a type of assessment.
* **[RawScore]:** Raw number of points earned on the assessment.
* **[PercentCorrect]:** Percent earned on the assessment.
* **[PointsPossible]:** Total raw points available on the assessment.
* **[PerfomanceBand]:** Description of the student's performance (typically "Critical", "Did Not Meet", "Approaches", "Meets", "Masters")
* **[PerfomanceBandNumber]:** Numeric code for PerformanceBand (typically 1-5).
* **[PBStartRange]:** Lower bound for a performance band, as a percent.
* **[PBEndRange]:** Upper bound for a performance band, as a percent.
* **[Description]:** Description of performance band interval, in words.
* **[Mastery]:** 1 = demonstrated mastery (e.g. passed, approaches+, etc.); 0 = did not
* **[src]:** Source of asssessment data (most are Illuminate, but some are from legacy systems)
* **[Scope]:** Similar to AssessmentType, but more detailed.
* **[State]:** Two-letter state code (TX, LA, FL, OH) where the assessment is administered.

**Other useful fields:** 

* **[AcademicYear]:** Use to filter the table to the appropriate school year.
* **[StudentNumber]:** Use to link to the students table.
* **[TcpCourseCode]:** This code uniquely links the assessment to the TCP course code.

##### IABWAItems

This table displays item-level data for each student for each assessment. This information can be useful for looking up items with specific TEKS, taught/not-taught objectives, multiple-choice vs. open-ended responses, etc.

**Table:** [1064613-SQLPRD2].[PROD2].[Assessments].[IABWAItems]

**Main metrics:** Item-level data describing the assessment taken, the question and part, the student's response, the correct answer, correctness, the standard, question type, weight, and points

**Assessment columns:** 

* **[IABWAItemID]:** This number uniquely links an assessment item part to a student.
* **[IABWAID]:** This number uniquely links a student to an assessment.
* **[ExternalTestID]:** This number uniquely identifies an assessment.
* **[AssessmentDate]:** Date of the assessment.
* **[SubjectID]:** This number describes if the subject is math, ELA, etc. 
* **[Subject]:** Same as SubjectID, but in words. 
* **[AssessmentReporingName]:** One of two columns to choose a type of assessment.
* **[AssessmentType]:** One of two columns to choose a type of assessment.
* **[QuestionNumber]:** Describes the question and part (e.g. 1a, 1b, 2, etc.)
* **[ItemStudentResponse]:** The response(s) scanned in by the student or teacher
* **[ItemCorrectResponse]:** The response(s) needed for full credit on the item.
* **[standard]:** Describes the objective number, TEKS, etc.
* **[StandardsDescription]:** Description of the objective in words.
* **[Supporting]:** 1 = is a supporting objective; 0 = not
* **[Readiness]:** 1 = is a readiness objective; 0 = not
* **[isCorrect]:** 1 = student earned credit for question number; 0 = not
* **[Points]:** Total points available for item.
* **[ReportingGroups]:** Describes the reporting group of objectives.
* **[IsRubric]:** ?
* **[State]:** Two-letter state code (TX, LA, FL) where the assessment is administered.
* **[QuestionTypeName]:** Describes the type of question (Multiple Choice, etc.)
* **[QuestionType]:** Same as QuestionTypeName but abbr.
* **[ExtraCredit]:** 1 = extra credit question; 0 = not
* **[IsAdvanced]:** 1 = advanced question; 0 = not
* **[Weight]:** 
* **[Maximum]:** Maximum points available for item.
* **[ItemWeight]:** Weight of item on question.
* **[Taught]:** empty
* **[MultiRubric]:** ?
* **[Taughtabbr]:** NT = objective not taught; T = objective taught
* **[PossibleScore]:** Maximum (weighted?) points available for item.
* **[RawScore]:** Raw points the student earned for this item.
* **[PointsCorrect]:** Weighted points the student earned for this item.
* **[PossibleScoreTaught]:** Conditional on taught objective: 0 = not taught; PossibleScore = taught
* **[RawScoreTaught]:** Conditional on taught objective: 0 = not taught; RawScore = taught
* **[PointsCorrectTaught]:** Conditional on taught objective: 0 = not taught; PointsCorrect = taught

**Other useful fields:** 

* **[AcademicYear]:** Use to filter the table to the appropriate school year.
* **[StudentNumber]:** Use to link to the students table.

#### Other sources

As IDEA grows nationally, certain regions have assessment-specific platforms that do not directly flow into the data warehouse. For example, Louisiana schools use [ANet](https://www.achievementnetwork.org/) to house some of their LEAP-aligned assessments.


### DIBELS

Dynamic Indicators of Basic Early Literacy Skills, or [DIBELS](https://dibels.uoregon.edu/) is a series of short literacy tests for early grade levels (K-2) originally developed by the University of Oregon Center on Teaching and Learning (CTL). We use these tests to inform us on reading outcomes for our youngest scholars at BOY, MOY, and EOY.

Data can be viewed through [Amplify](https://mclass.amplify.com/portal/) but is usually pulled and stored in the warehouse.

**Table:** [791150-HQVRA].[Dashboard].[dbo].[DIBELS_MCLASS]

**Main metrics:** Counts of students meeting the standard, below or above the standard overall and for individual tests; Scores for individual tests

**Most important columns:** [Assessment Measure-Composite Score-Levels] | [Assessment Measure-Composite Score-Score] | [Assessment Measure-FSF-Levels] | [Assessment Measure-FSF-Score] | [Assessment Measure-LNF-Levels] | [Assessment Measure-LNF-Score] | [Assessment Measure-PSF-Levels] | [Assessment Measure-PSF-Score] | [Assessment Measure-NWF (CLS)-Levels] | [Assessment Measure-NWF (CLS)-Score] | [Assessment Measure-NWF (WWR)-Levels] | [Assessment Measure-NWF (WWR)-Score] | [Assessment Measure-DORF (Fluency)-Levels] | [Assessment Measure-DORF (Fluency)-Score] | [Assessment Measure-DORF (Accuracy)-Levels] | [Assessment Measure-DORF (Accuracy)-Score] | [Assessment Measure-DORF (Retell)-Levels] | [Assessment Measure-DORF (Retell)-Score] | [Assessment Measure-DORF (Retell Quality)-Levels] | [Assessment Measure-DORF (Retell Quality)-Score] | [Assessment Measure-DORF (Errors)-Score] | [Assessment Measure-Daze-Levels] | [Assessment Measure-Daze-Score] | [Assessment Measure-Daze (Correct)-Score] | [Assessment Measure-Daze (Incorrect)-Score]

**Other useful fields:** 

* **[School Year], [School Name], [Assessment Grade]:** use to filter the table to the appropriate students
* **[Benchmark Period]:** labeled as BOY, MOY, or EOY
* **[Student ID (District ID)]:** use to match students back to the Students table.
* Note that data for IDEA Travis in 2021-22 BOY did not have Student IDs. You must match these students (and potentially others) by **[Student Last Name]** and **[Student First Name]**. If a match is not returned, check for misspellings of the names or hyphenated/dual last names. DIBELS data often omits a hyphen or the second last name entirely.
  
**Context:**

* **Who is assessed?** 

All students in K, 1 and 2 are assessed using DIBELS. 

* **Which tests/columns are actually used?**
  + **[Assessment Measure-Composite Score-Levels]:** A *composite level* is assigned to all students, regardless if they complete all sections. Levels include "Well Below Benchmark", "Below Benchmark", "At Benchmark", and "Above Benchmark".
  + **[Assessment Measure-Composite Score-Score]:** A *composite score* is assigned to all students, regardless if they complete all sections. The interval of composite scores is [200, 467+).
  + **[Assessment Measure-LNF-Levels]:** The LNF, or *Letter Naming Fluency* test, is administered to K-1 students only. Levels include "Well Below Benchmark", "Below Benchmark", and "At Benchmark".
  + **[Assessment Measure-LNF-Score]:** This is a numerical measure of LNF in the interval [0, 59+).
  + **[Assessment Measure-PSF-Levels]:** The PSF, or *Phonemic Segmentation Fluency* test, is administered to K-1 students only. Levels include "Well Below Benchmark", "Below Benchmark", "At Benchmark", and "Above Benchmark".
  + **[Assessment Measure-PSF-Score]:** This is a numerical measure of PSF in the interval [0, 61+).
  + **[Assessment Measure-NWF (CLS)-Levels]:** The NWF, or *Nonsense Word Fluency* test, is administered to K-2 students. Levels include "Well Below Benchmark", "Below Benchmark", "At Benchmark", and "Above Benchmark".
  + **[Assessment Measure-NWF (CLS)-Score]:** This is a numerical measure of NWF in the interval [0, 46+).
  + **[Assessment Measure-DORF (Fluency)-Levels]:** The DORF, or *DIBELS Oral Reading Fluency* test, is administered to 1-2 students. Levels include "Well Below Benchmark", "Below Benchmark", "At Benchmark", and "Above Benchmark".
  + **[Assessment Measure-DORF (Fluency)-Score]:**  This is a numerical measure of DORF - Fluency in the interval [0, 164+).
  + **[Assessment Measure-DORF (Accuracy)-Levels]:** The DORF, or *DIBELS Oral Reading Fluency* test, is administered to 1-2 students. Levels include "Well Below Benchmark", "Below Benchmark", and "At Benchmark".
  + **[Assessment Measure-DORF (Accuracy)-Score]:** This is a numerical measure of DORF - Accuracy in the interval [0, 96+).
  + The remaining assessment measures were used historically but are no longer actively tested.

* **Why are there NAs for certain tests if the student took the assessment?**

DIBELS has multiple required measures but can be implemented using *gating rules*, which stop the assessment if a rule is met. If a student (1) scores below a minimum threshold or (2) tests out by scoring above the highest benchmark on a specified gating measure, then the remaining assessments are not administered, and the measure is left blank. The gating measure changes, depending on the grade level and benchmark period.

* **Who can I ask for more context?**

  + Nkosi Geary-Smith is the Director of Early Literacy. She has context on the DIBELS assessment and how it is used.
  + Chris Gonzalez is the VP of Accountability. He pulls the DIBELS data and uploads it to the warehouse.
  + The University of Oregon maintains the [DIBELS site](https://dibels.uoregon.edu/) and publishes guides on administering and interpreting scores.
  

### Renaissance STAR (also RenStar, Little Star)
[Reinaissance](https://www.renaissance.com/about-us/) [Star Assessments](https://www.renaissance.com/products/star-assessments/) are a suite of of norm-referenced assessments used by IDEA for both screening (particularly for identifying students in need of [CSI support](#csi)) as well as gauging achievement and growth of students in a way that is agnostics to both grade-level and location (i.e., Texas, Louisiana, Florida, or Ohio).  

It is particularly useful for evaluative purposes as it allows us to use normed data (e.g., national percentile rank, grade-level equivalent) to understand how students are progressing in their learning; it effectively gives an indirect measure of student acumen (though this can be confounded by a student's effort on any given test).

#### Where to get RenStar data?
Accessessing these data in the warehouse at this juncture is not always obvious.  There are number of tables (e.g. `[PROD2].[Assessments].[LittleStar]`) that are not freuqently updated. The best best is to use:

`[RGVPDSD-DWSRC4].[SRC_AR].[StarMathV2]` and `[RGVPDSD-DWSRC4].[SRC_AR].[StarReadingV2]`

### AP

IDEA's Advanced Placement (AP) for all model requires most 9th through 12th grade students take AP coursework and provides opportunities for students to take AP Exams at no cost. All campuses offer AP courses and exams, however, IB schools offer far fewer options than the non-IB campuses. 

All AP student data is downloaded directly from the College Board's website. Once downloaded it is imported, cleaned and added to the R&A's server. In order to get the data into a usable format, with local student numbers and deduplicated, there are several scripts in the AP project on github that need to be run in a specific order.

**Step One:** pull down the 21-22 AP Report project on github.
**Step Two:** Open the main r project within that folder "21-22 AP Report". Within that project there are 2 main sub-project folders. Use the AP Requests.
**Step Three:** Run the 2 scripts in the data folder. First run "1_Load_AP_Data", this pulls all of the AP tables. Next run "2_Load_Students_Data", this pulls all of the students and excludes Thrive students at Toros CP which are students with severe cognitive disabilities that are over the age of 18.
**Step Four:** Run the 3 scripts in the munge folder. First run "1_Clean_AP_Data", this script cleans up the AP data, like making sure that if a student takes the same AP Exam two different years and pass both with the same score that they are not counted twice towards the 3 exams passed for AP Scholars classification. Then run "2_Scaffold_AP_Data", this creates several scaffolds such as the exams taken in the years that they take them. This script results in a base data file that can be used call "ap_df". This file will include duplicate tests if taken in a different year by the same student. There are not many of these students that took the same exam that they had already passed in an attempt to get a better score. Finally run "3_Scaffold_AP_Scholars_Data", this is the same script as #2, but ensures that the file that is produced by the script doesn't include duplicate exams for the calculation of AP Scholars. The resultant file from this last script is called "ap_df_scholars".

### IB

The [International Baccalaureate](https://ibo.org/), or IB, is an international organization that oversees a rigorous education programme, similar to that of the College Board in the US. They host four different programmes:

1. PYP - Primary Years Programme
2. MYP - Middle Years Programme
3. DP - Diploma Programme
4. CP - Career-related Programme

#### Coursework and scoring

* **Coursework**

Currently, five IDEA college preps - Donna, Frontier, Brownsville, McAllen, and South Flores - offer the Diploma Programme. The DP is a series of courses, assessments, and portfolios in six different groups and the Core:

* Group 1: Studies in langauge and literature (primary language)
* Group 2: Language acquistion (secondary language)
* Group 3: Individuals and society (social sciences)
* Group 4: Sciences
* Group 5: Mathematics
* Group 6: The arts
* Core: Theory of knowledge (TOK); Extended essay (EE); Creativity, activity, service (CAS)

At IDEA, students will generally take one course in each group, and Diploma students will also work on the Core. Within IDEA, the Core is also referred to as Group 7.

Futhermore, each IB course is usually offered at both the Higher Level (HL) and Standard Level (SL). Due to graduation requirements, IDEA only offers Group 1 (English) and Group 3 (History of the Americas) courses at HL. However, the remaining coursework can be selected at either HL or SL, depending on the student's preferences, goals, and school offerings.

From the Core, only Theory of Knowledge is a regularly taught course and is not designated as HL or SL. The Extended Essay is an independent research project conducted by each student in a specified subject and can be supervised by any teacher. CAS is a portfolio of a student's creative endeavors, physical activities, and community service throughout the programme.

* **Course grades, predicted grades, and assessment**

A combination of internally and externally assessed components throughout a single course determine the final *course grade* for a student, where grades range from 1 to 7, OR E to A, with incomplete scores marked as N or P. If a course or assessment is given a numeric grade, then 4 and above is considered passing. If a course is given a letter grade, then D, C, B, and A are passing. A grade of N or P in any assessment typically disqualifies a student from earning that course grade and earning an IB Diploma.

Internal assessments are graded by the teacher and then moderated by an IB external reader. External assessments are graded only by an IB external reader. All May papers (examinations) are externally assessed, but not all courses have May papers. Most courses have multiple internally and externally assessed components.

Prior to submitting all coursework for the student, each teacher must also give a *predicted grade*, a single value predicting that student's course grade.

Courses from Groups 1 to 6 are awarded a numeric grade, TOK and EE are awarded a letter grade, and CAS is awarded a satisfactory / non-satisfactory completion.

* **IB Diploma vs. Certificate (Course)**

Not all students are required to take courses in all groups. Students who participate in coursework in all required groups and the Core pursue the [*IB Diploma*](https://www.ibo.org/programmes/diploma-programme/), which is a recognition of exemplary work, somewhat equivalent to the AP Capstone Diploma Program. The business rules are quite complicated for earning a diploma, but students generally need to have a certain combination of HL and SL points with a cumulative course score across all groups of at least 24 and all passing requirements met in the Core.

A student may also opt for the *Bilingual Diploma*, which is usually satisfied by passing scores in at least two courses taken in a secondary language. Most students who have earned the Bilingual Diploma at IDEA will have taken both a Group 1 English course and a Group 1 Spanish course.

Students who do not participate in the Diploma are labeled as *Certificate* or *Course* students.

#### Data

There are two main IB tables: **[IBScores]** and **[IBDiploma]**.

::: {.gotcha}
The IB tables only offer a snapshot of final course grades. They do not include a breakdown of internal and external assessment scores taken throughout the course.

When reporting IB data, confirm what level of data is needed. Suggested groupings are: all course grades for Groups 1-6 only; all course grades and TOK; and all course grades, TOK and EE. If individual assessments are needed, then consult with the campus' IB Coordinator.
:::

**Table:** [RGVPDSD-DWPRD2].[PROD2].[Assessments].[IBScores]

* **Main metrics:** final course grades and predicted grades for each IB course, TOK, and EE

* **Useful Fields:** 

  + **StudentNumber:** student ID number
  + **SubjectID:** *do not use!* This does not correspond to IB Groups
  + **CourseName:** Long course name
  + **CourseNumber:** course number
  + **TCPCourseCode:** link to TCP tables
  + **Level:** HL or SL for courses; TK for Theory of Knowledge; EE for Extended Essay
  + **Language:** primary language of instruction (either English or Spanish)
  + **Category:** Diploma, Course, or Certificate
  + **PredictedGrade:** teacher's predicted course grade from 1 to 7 or E to A
  + **Grade:** actual course grade from 1 to 7, E to A, N or P
  + **ScaledTotalMarkForSubject:** not used
  + **AcademicYear:** yyyy-yyyy

**Table:** [RGVPDSD-DWPRD2].[PROD2].[Assessments].[IBDiploma]

* **Main metrics:** description of IB Diploma results for each Diploma candidate

* **Useful Fields:**

  + **StudentNumber:** student ID number
  + **EETOKPoints:** number of bonus points awarded from combination of TOK and EE letter grades; may be missing, 0, 1, 2, or 3
  + **TotalPoints:** cumulative points earned from course grades and bonus points
  + **Result:** description if the IB Diploma or Bilingual Diploma was awarded or not
  + **DiplomaRequirementsCode:** describes why the diploma requirements were not met; blank if all diploma was awarded
  + **AcademicYear:** yyyy-yyyy
  + **SchoolTermID:** Indicates the quarter or year, Ex. 3000 = 2020-2021 SY, 3100 = 2021-2022 SY

**Data collection:**

* Note that IB data must be obtained campus-by-campus, as the IB authorizes the individual campus, not the district, as a World School.
* There are three files that contain relevant IB data:

1. Candidate Report - this is a summary of the course grades and IB diploma result by student (corresponds to [IBDiploma] table)
2. Subject Report - this is a summary of the course grades by subject (corresponds to [IBScores] table)
3. Candidate-Subject-Component Report - this is a detailed list of all assessments and results for each student (not currently captured in the warehouse)


### ACT

The [ACT](https://www.act.org/) is the main college-entrance exam taken by IDEA students. Students typically start taking the exam in 10th grade, then have multiple attempts in 11th grade while taking their ACT Prep course, and have a final opportunity in 12th grade prior to submitting college applications. 

Note the ACT is handled by the College Success team, not the Assessments team.

**Tables:** [RGVPDSD-DWSRC2].[SRC_ACT].[ACT].[StudentScores]

**Main metrics:** composite and section scores; test date; CEEB code

**Important columns:**

* **[ID_Local]**: student 1080/1081 number
* **[F_Name]**: first name
* **[M_Initial]**: middle initial
* **[L_Name]**: last name
* **[HS_GradeLev]**: grade level when the exam was taken, labeled as "11th Grade", etc. (may also be "6th or 7th Grad", "8th Grade" for Duke TIP students)
* **[HS_Code]**: high school CEEB code (ACT school code)
* **[HS_GradYr]**: graduation year
* **[Test_Dte]**: month and year of test, labeled as mmyyyy
* **[Eng]**: English test score
* **[Mth]**: mathematics test score
* **[Rdg]**: reading test score
* **[Sci]**: science test score
* **[Composite]**: composite score (the average of English, Math, Reading, Science, rounded to the nearest integer)
* **[Writing]**: writing test score, if taken
* **[STEM]**: overall performance on math and science tests
* **[ELA]**: overall performance on English, reading, and writing tests

**Important context:**

* ACT section and composite scores range from 1 to 36, where 36 is the highest score possible.
* The writing section is scored from 2 to 12. This section is not included in the composite score but is often required by many schools.
* The **[HS_GradeLev]** is not the current [GradeLevelID] - it is the grade level in which the exam was taken.
* The high school CEEB (*College Entrance Examination Board*) code is not the [SchoolNumber] - it is issued by the ETS (*Educational Testing Service*) for use in sending test scores to colleges, who also have their own CEEB codes.
* For more information about scoring, refer to this page from the ACT: [Understanding Your Scores](https://www.act.org/content/act/en/products-and-services/the-act/scores/understanding-your-scores.html)
* To filter scores by test date, it is advised to parse the **[Test_Dte]** column into a **[Month]** and **[Year]** column to read the dates correctly.

**Data collection:**

* Full access to ACT data belongs to people with a Trusted Agent account on the ACT website. This data is updated in the warehouse on a monthly basis.
* Janna Wiley is the Director of ACT Programming. She has context on ACT administration and data.

**SQL example:**

```{sql, eval=FALSE}
SELECT [ID_Local] AS [StudentNumber]
      ,[F_Name] AS [FirstName]
      ,[M_Initial] AS [MiddleInitial]
      ,[L_Name] AS [LastName]
	    ,REPLACE([HS_GradeLev], 'th Grade', '') AS [GradeLevelTaken]
      ,[HS_Code] AS [CEEBCode]
      ,[HS_GradYr] AS [ClassYear]
      ,[Test_Dte] AS [TestDate]
	    ,SUBSTRING([Test_Dte], 1, 2) AS [Month]
	    ,SUBSTRING([Test_Dte], 3, 6) AS [Year]
      ,[Eng] AS [English]
      ,[Mth] AS [Mathematics]
      ,[Rdg] AS [Reading]
      ,[Sci] AS [Science]
      ,[Composite]
      ,[Writing]
      ,[STEM]
      ,[ELA]
  FROM [RGVPDSD-DWSRC2].[SRC_ACT].[ACT].[StudentScores]
  WHERE [HS_GradYr] IN ('2023', '2024', '2025', '2026')
	AND [HS_GradeLev] NOT IN ('8th Grade', '6th or 7th Grad')   
  ORDER BY [HS_GradYr], [HS_Code], [L_Name], [Year], [Month]
```


**R example:**

```{r act_example, eval=FALSE}
get_table(.table_name = "StudentScores",
          .server_name = "RGVPDSD-DWSRC2",
          .database_name = "SRC_ACT",
          .schema = "ACT") %>%
  
  # filter for current high school students and tests taken in high school only
  filter(HS_GradYr %in% c("2023", "2024", "2025", "2026"),
         !(HS_GradeLev %in% c("6th or 7th Grad", "8th Grade"))) %>%
  mutate(Month = str_sub(Test_Dte, 1, 2),
         Year = str_sub(Test_Dte, 3, 6)) %>%
  
  # rename columns for readability, future joins
  select(StudentNumber = ID_Local,
         FirstName = F_Name,
         MiddleInitial = M_Initial,
         LastName = L_Name,
         GradeLevelTaken = HS_GradeLev,
         CEEBCode = HS_Code,
         ClassYear = HS_GradYr,
         TestDate = Test_Dte,
         Month,
         Year,
         English = Eng,
         Mathematics = Mth,
         Reading = Rdg,
         Science = Sci,
         Composite,
         Writing,
         STEM,
         ELA) %>%
  collect() %>%
  mutate(GradeLevelTaken = str_replace(GradeLevelTaken, "th Grade", "")) %>%
  arrange(ClassYear, CEEBCode, LastName, Year, Month)
```

### LEAP

Louisiana Educational Assessment Program (LEAP)
The LEAP assessments are Louisiana standardized tests administered every year. Students in grades 3 through 8 take the assessments in English Language Arts, mathematics, science, and social studies every year. High school students take the English I, English II, Algebra I, Geometry, U.S. History, and Biology tests once while in high school.  Student results are reported using five performance levels (Advanced, Mastery, Basic, Approaches Basic, Unsatisfactory). These assessments are aligned to the Louisiana Students Standards and provide a consistent measure of student performance and growth from grades three through eleven.

Students with significant cognitive disabilities take the LEAP Connect (Alternate Assessment).
English learners from K-12 take the English Language Proficiency test (ELPT).

LEAP test results summarized by school and grade level can be downloaded from the [Louisiana Department of Education website](https://www.louisianabelieves.com/resources/library/elementary-and-middle-school-performance) (file name: 2022 leap 2025 state lea school achievement level summary). Students in grades 3-8 need to score Basic or higher in at least 3 tests and need to score Approaching Basic or higher in the fourth test.

The downloaded workbook has tabs for each grade level and a tab for high school. Filter the spreadsheet for the IDEA schools in LA. Use the expected number of students to participate and the participation rate columns to calculate the actual number of students who participated. To calculate the percentage of students who passed each subject, add the percentage of students who scored at the Advanced, Mastery, and Basic levels.

#### Student level LEAP

If you need to have student level LEAP data, you can access (Spring 2021 and 2022) 3-8 assessment results [here](https://ideapublicschoolsorg.sharepoint.com/RA/SitePages/Official-LEAP-Campus-Reports.aspx). Follow the link for LEAP2025 for the main stream results, LEAP Connect for the alternate assessment results, and ELPT for the English Language Proficiency assessment results. Then select the campus you are interested in and open the excel spreadsheet to see the data.

Useful Fields:
â€¢	SchoolNbr
â€¢	SchoolName
â€¢	LASID
â€¢	Grade
â€¢	ELAVoidFlag
â€¢	ELAAchievement (Advanced, Mastery, and Basic scores represent passing that subject test)
â€¢	MathVoidFlag
â€¢	MathAchievement (Advanced, Mastery, and Basic scores represent passing that subject test)
â€¢	SocialVoidFlag
â€¢	SocialAchievement (Advanced, Mastery, and Basic scores represent passing that subject test)
â€¢	ScienceVoidFlag
â€¢	ScienceAchievement (Advanced, Mastery, and Basic scores represent passing that subject test)

## Student Course Grades

There are 2 tables in the warehouse that collect student course grades, `[RGVPDSD-DWPRD1].[PROD1].[Schools].[StudentCurrentGrades]` and `[RGVPDSD-DWPRD1].[PROD1].[Schools].[StudentHistoricalGrades]`. The [StudentCurrentGrades] table is where course grades land from PowerSchool for students in 8th through 12th grade. This table is used to calculate *On track to Graduate* and to check if students are passing their courses. These grades change! After the quarter is complete the grades are moved to the [StudentHistoricalGrades] table as a permanent grade. The [StudentHistoricalGrades] table contains grades for all students in Pre-K though 12th. This is the table used for transcripts and report cards.

**Table:** [RGVPDSD-DWPRD1].[PROD1].[Schools].[StudentCurrentGrades]

**Useful Fields:** 

* **SchoolNumber:** Local school number
* **SchoolName:** Long school name
* **CourseName:** Long course name
* **CourseNumber:** Course number
* **CreditType:** Abbreviated subject, such as ELA or MA for Math
* **StudentNumber:** Local student ID number
* **StudentName:** Last, First Middle name
* **GradeLevelID:** Grade level (only 8th through 12th)
* **StoreCode:** Indicates the whether the grade is for the year-long course, semester, or quarter
* **PercentGrade:** The numerical grade 0-100
* **LetterGrade:** Wouldn't use this column it is a mix of letter grades and numerical grades
* **SchoolTermID:** Indicates the quarter or year, Ex. 3000 = 2020-2021 SY, 3100 = 2021-2022 SY

**Table:** [RGVPDSD-DWPRD1].[PROD1].[Schools].[StudentHistoricalGrades]

**Useful Fields:**

* **StudentNumber:** Local student ID number
* **AcademicYear:** The academic year
* **SchoolTermID:** Indicates the quarter or year, Ex. 3000 = 2020-2021 SY, 3100 = 2021-2022 SY
* **SchoolNumber:** Local school number
* **CourseName:** Long course name
* **CourseNumber:** Course number
* **CreditType:** Abbreviated subject, such as ELA or MA for Math
* **StaffID:** The Staff ID number for the teacher of the course (not sure if this is accurate)
* **TeacherName:** The teacher's name (Last, First) again not sure if this is accurate
* **GradeLevelID:** Grade level (only 8th through 12th)
* **StoreCode:** Indicates the whether the grade is for the year-long course, semester, or quarter (Y1 = end-of-course grade for the year)
* **PercentageGrade:** The numerical grade 0-100
* **LetterGrade:** Appears to be all letter grades (Character column)
* **AbsentCount:** Count of absences
* **TardyCount:** Count of tardies
* **State:** 2 letter abbreviation for the student's State 




## Teacher, Staff, and Hiring Data

### Compass data

[Compass](https://compass.ideapublicschools.org) is an internally developed application that collects and displays most employee data. The applications production database provides tidy tables for most staff data **and should be considered the source of truth** for these data 

:::tip
You'll need to reach out to Chris Haid to access the database. A set of compass functions for `[ideadata](idea-analytics.github.io/ideadata)` is in development.
:::

#### Key Tables

- 2x2s in TwoByTwos
  - Employee_ID
  - Question_ID
  - Year
  - SortOrder (specific to year and Reviewer amd ois order displayed in Cornerstone)
  - Reviewer
  - IsComplete (completed by Staff and Manager)

- TCPs
    - EmployeeID
    - ManagerID
    - CompositeScore
    - CoreValues
    - GETRAtings
    - ParentSurvey
    - StudentAchievment
    - StudentSurvey
    - TCPPlacementLevel
- TCPPlacement
- TalentReviews
- SkywardPositions 
    - Historical staffing and positional data not in TylerMunis
    - Get's merged with Position History data
- RoadMap
    - Roadmap Users (crosswalk to roadmap UniqueID, Email address, EmployeeID)
    - RoadmapPrograms
    - RoadmapProgramScores
- QuestionsGPTW (going to be changed) and super wide
- Profiles
    - Big primary table
    - All of tylermunis's active employee data. 
        - pull most recent active directory data (AD_ fields)
            - AD_SamAccountName
            - Personal Email
    - Positions: Historical Positional Data
        - ProfileID 
        - EmployeeID
        - DataSource (where it comes from )
- JobVite
    - similar to Roadmap
    - JobviteUsers 
    - app id, 
    - candidateid 
    - can link to other JobVite tables
- GPTW
    - Pivoted by employee id for performance manager
    - **This will change** (moving to tidy data)
- Goals
- External Position (not ussedd, but for user added data not captured in Jobvite)
- AnnualPerformanceReviews
 



### Jobvite Data

Jobvite is the application and hiring platform used for all staff.  There job listings are posted, the interview process managed, and even initial onboarding tasks are progress monitored. 

R&A has used data from Jobvite to analyze pre-hiring measures of hired teachers. 

Here are some more facts about Jobvite data:

* Obtained from HA team. It is the data from the candidates who apply for IDEA jobs through Jobvite. Not all candidates applied through Jobvite in the past, but HA is trying to streamline the application process and require all applicants for all jobs (at all campuses) to apply through Jobvite. A particular Jobvite file is labeled with a year, such as "Jobvite 19-20", and it will have  `Candidate Submit Date` column , which ranges from October 2019 to July 2019 (October to July, proceeding the year the applicant will likely begin work). An applicant from that range will most likely have a Start Date of August 2019 (or sometime in the calendar year 2019, starting AFTER the 19-20 Academic Year). However, there are cases when applicants will, for example, apply during the 10/2018 - 07/2019 cycle (Jobvite 19-20), but will not have a Start Date until 2020 or 2021 (so they do not start the following FAll after the application cycle). 

This file has columns including `Candidate Full Name`, `Candidate Email` (personal email used for the application), and some of the pre-hire selection measures, such as `GPA`. However, all 5 of the pre-hire selection measures are not included yet (`STEM Major`, `Teach For America status`, `Experience prior to IDEA`, `GPA`, and `Teacher Certification`). The data also has no `Employee IDs yet`, as this is a list of candidates. There is a `Hire Yes/No` column, which has "Yes" for those candidates who were offered a position and "No" for those who were not offered a position. Also, there is a Requisition ID which is basically an application ID number, and it is unique per application. If a single candidate applies for more than one position, the candidate will have a unique ID for each application. Below are some bullet points describing Jobvite and Teacher Export (TCP) data.
          
* Jobvite Data: applications for Teacher Roles through Jobvite (when filtering for `Category == "Teaching"`)

  * Contains those who are Hired = Yes and Hired = No.  â€œHired = Yesâ€ means they were offered the position, but does not indicate if they actually accepted it or not. 
  * Each person may apply for more than one job opening, and if they do, they will have a unique Requisition ID for each application (even though it is the same candidate applying for each separate position).
  * Has Personal Email and Full Name
  * Does NOT have IDEA Work Email, nor EmployeeID
  * Contains everyone who applied from October to the following July, for the subsequent August Start Date (or start dates after that year).
  * Most people who applied in Jobvite 19-20 (10/18 to 7/19), started in Fall 2019 (19-20 Academic Year) 
  * But a few of those started in Fall 2020 and Fall 2021.


**Jobvite File facts** 
  1.   19-20 â€“ 2nd year of Jobvite data (18-19 is first and starting year of Jobvite data)
    * (renamed Jobvite 18-19 SY by Brittany; email 10-12-21)
    * Contains Tentative Start Date column â€“ shows which month/year a new hire started working
    (doesnâ€™t have to be immediately following that yearâ€™s hiring cycle)

  2.  20-21 â€“ 3rd year of Jobvite data 
    * (renamed Jobvite 18-19 SY by Brittany; email 10-12-21)
    * Contains Tentative Start Date column

  3. 21-22 â€“ 4th year of Jobvite data
    *(renamed Jobvite 18-19 SY by Brittany; email 10-12-21)
    * Contains Tentative Start Date column

### TCP/Teacher Export Data

TCP/Teacher Export Data: Teacher Performance data.
* Includes TCP Level Placements, TCP Composite Score, and TCP component scores (all 5 components). 
  + GET (35%): Manager and Self Guidepost Ratings contain 6 components
  + Lesson Planning & Delivery
  + Culture, Etc.
  + Components 1-5 are counted, and Component 6 (Core Values) is calculated outside of this score (it is 1 of 5 Components of TCP, worth 5% on its own). Each Guidepost Component has 4-5 items listed under each as 1A, 1B, etc.
  + Parent Survey (5%): Show up as BLANK if <10 surveys were completed per teacher
  + Student Survey (5%) Shows up as BLANK if:
    + <10 surveys were completed per teacher
    + Some campuses encourage students and parents to fill out the surveys, and some do not
  + Teacher taught SPED, pre-K, K, 1st, or 2nd Grades (those students are not able to take the survey/or too young).
  + Student Achievement (50%) Testing such as STAAR, etc.
  + Core Values (5%)
  + Contains IDEA Work Email and EmployeeID
  + Contains First Name and Last Name (but not Full Name)
  
**There is a 2 year HOLD for the pandemic** (we donâ€™t know who was â€œheldâ€), meaning someone at a particular level (E.g., Level 5) could stay at that level for 2 years without having a composite score that is as high as that level. Usually, the hold is for 1 year.


* If someone got a 4.5+ composite TCP score for 2 consecutive years in a row, they can move to a Level 5 TCP without having the required years of Experience.
* For those who do not have Student Achievement scores, the highest TCP Level they can reach is a Level 3 (since Student Achievement is 50% of the score).
* If a person is missing one of the other 4 TCP components, such as Parent Survey, the points that were from that score are reallocated to the other components, weights the in the same ratios as the other components (so itâ€™s not an equal allocation, meaning you donâ€™t divide the percentage 4 equal ways).

1. 18-19  
 * From Alex, see email

2. 19-20  
  * (â€œMaura Reportâ€, from Alex, new sheet on the Teacher Export 18-19 file from Alex, see email).
  * Need to request this from Blanca (data table we are already using for TSLIP grant, that we can re-use for Teacher Hiring Round 3).
  * 20-21 â€“ 3rd year of Jobvite data 
  * from Blanca Carillo, contains ONLY those with â€œTeacherâ€ roles OR New Hires and Promotions; all co-teachers, etc., have been removed for that academic year 
  * Contains some who started work in 21-22 but are New Hires or Promotions, so rated Level 1 TCP (but didnâ€™t work in the 20-21 school year

3. 21-22 â€“ NO
  * **Not available yet!**
  * No one was rated for this school year because itâ€™s not over yet, and the TCP ratings/placements have not yet been conducted. Will likely be finalized in Fall 2022 by early-October.


#### Data Matching
* Jobvite 19-20 to Teacher Export 19-20
* Jobvite 20-21 to Teacher Export 20-21
Jobvite 21-22 to Teacher Export 21-22 â€“ Canâ€™t do this! TE 21-22 NOT available yet!
* Thus, we will be looking at 2 years of data:
   +	Jobvite 19-20 to Teacher Export 19-20
   +	Jobvite 20-21 to Teacher Export 20-21
      + Removing Jobvite 19-20 Hires who did not have a Start Date of Fall 2019
      + Removing Jobvite 20-21 Hires who did not have a Start Date of Fall 2020
      + Removing all Teacher Export 19-20 and Teacher Export 20-21, who did not have a Job Title of Teacher
      + Removing all Teacher Export 19-20 and Teacher Export 20-21, who had Internal Role as:
*	SPED CP
*	SPED AC
*	PE CP
*	PE CP
*	(This is due to SPED and PE having state mandated license requirements that are not required for the other subjects, so these should be analyzed separately).
      + Removing Teacher Export 19-20 Teachers who did not work in 19-20 Academic Year 
*	(Removing New Hires, Promotions starting in Fall 2020, TCP Level 1)
      + Removing Teacher Export 20-21 Teachers who did not work in 20-21 Academic Year
*	(Removing New Hires, Promotions starting in Fall 2020, TCP Level 1)

#### Join Tables
How to join? 
*	Jobvite has no Employee ID & no Work Email and Teacher Export has Employee ID and Work Email.
*	Possibly join through PROD1.Staffing.Employees table, which contains First Name, Last Name, Birthday, Employee ID, and Work Email

#### APR Report for June 2022 

##### Promotions

Examining the rate of promotions for both teachers and teacher-leaders, from 20-21 SY to 21-22 SY. We are using the HRIS report from May 21, 2021 and comparing it to the HRIS report from June 10, 2022.

###### Teacher Promotions
We are looking at Area = "TEACHER" for 20-21 SY. Then inner_join to all employees on the HRIS for 21-22 SY. There were a total of 2,388 employees who worked for both schools years (20-21 and 21-22 SY), including as a Teacher for 20-21 SY and as either a Teacher or different Area for 21-22 SY. We found 39 Job Titles for Areas other than "TEACHER" on the 21-22 SY report (for teachers who had changed Areas from 20-21 to 21-22 SY). One Job Title was "INTERVENTIONIST" which moved from Area = "TEACHER" to Area = "INSTRUCTIONAL SUPPORT". We concluded this was incorrect and should remain as Area = "TEACHER", and so was counted as "no change". The remaining 38 Job Titles that fell outside of Area = "TEACHER" were included as either Promotions or Demotions.

**Promotions**

LICENSED SPECIALIST IN SCHOOL PSYCHOLOGY INTERN
ASST. PRINCIPAL OF INSTRUCTION
REGIONAL ATHLETICS MANAGER
COLLEGE COUNSELOR
SPECIAL PROGRAMS COACH
DI PROGRAM MANAGER
REGIONAL SPECIAL PROGRAMS COACH
DIRECTOR OF COLLEGE COUNSELING


ASSISTANT PRINCIPAL OF INSTRUCTION
SCHOOL COUNSELOR
STUDENT SUCCESS ADVISOR
EDU DIAGNOSTICIAN
REGIONAL INTERVENTION COACH
ASST. PRINCIPAL OF OPERATIONS
TALENT RECRUITER
PRINCIPAL OF INSTRUCTION
PUBLIC RELATIONS MANAGER

EDUCATIONAL DIAGNOSTICIAN
TALENT PARTNER
MATH CURRICULUM WRITER
SPANISH LANGUAGE ARTS CURRICULUM MANAGER
ASSISTANT PRINCIPAL OF OPERATIONS IN RESIDENCE
ASSISTANT PRINCIPAL OF OPERATIONS
ELA SECONDARY CURRICULUM MANAGER
STATE REPORTING COORDINATOR

ASST. PRINCIPAL OF INSTRUCTION IN RESIDENCE
SOCIAL STUDIES CURRICULUM MANAGER
K-2 REGIONAL MATH COACH
MATH CURRICULUM MANAGER
DIGITAL COMMUNICATIONS MANAGER
SR. COLLEGE COUNSELOR
TECHNICAL TRAINER
PROJECT MANAGER
JOB COACH

**Demotions**
FLEX TEACHER
TESTING COORDINATOR
SUBSTITUTE TEACHER
SPECIAL EDUCATION RISE CO-TEACHER

###### Teacher-Leader Promotions

#### TCP Retention and TCP Levels

One part of TSLIP looks at Teacher Retention by TCP Placement Level. The reason for this is because if we know more about how many teachers are leaving from particular levels, we may be able to determine some reasons why, and then provide needed support to help retain teachers. Mid-year and even end-of-year teachers leaving is a huge disruption to students and it is expensive and time-intensive for the hiring team. It can have impacts on student achievement and classroom culture

##### Missing TCP Placements
TCP Retention for 20-21 SY had to be calculated using predictions for TCP Placements for 50% of the leavers. About 200/400 leavers were not placed into TCP Levels because the manager knew that the teacher would not be returning.  For the 21-22 SY, all teachers will be placed into a TCP Level regardless of known leaver status, and this should be available on the Teacher Export sheet on the TCP website application.

##### Leaver Rates

Leaver rates for 21-22 SY were approximately: 17% Level 1, 16% Level 2, 11% Level 3, 7% Level 4, 6% Level 5. Level 1 makes up the largest total number of teachers at IDEA; approximately 1/3 teachers at IDEA are "New To IDEA" Level 1. That number is followed closely by Level 4, which makes up the next largest group of teachers at IDEA. Level 2 has the smallest number of teachers. This level is a sort of "holding" level for teachers who didn't score high enough on the composite score to go straight from Level 1 to Level 3. The traditional path is the promote straight from Level 1 after the first year, to Level 3.

Total n for Each TCP Level, out of 3432 Teachers at IDEA, 21-22 SY:
Level 1: 1185
Level 2: 90
Level 3: 813
Level 4: 1010
Level 5: 334


Going forward for 21-22 SY TCP, all teachers will supposedly be placed into TCP Placement Levels, regardless of known leaver-status. This should help with the issue of analyzing retention of teachers by TCP Level, as all teachers would have a TCP Level when the October 2022 TCP comes out. Teachers who left should still be available in the Teacher Export file on the TCP website/application, according to the TCP Team.

##### Exit Surveys for Teacher Leavers
Not all of the leavers from the TCP Retention project matched up with those who took Exit Surveys. In many cases, there was no Exit Survey for a Teacher Leaver from the TCP Retention project, and in other cases, the leavers with an Exit Survey were not on the TCP Retention project list of teachers. We are still reviewing the Exit Survey data for clues as to why teachers from various levels might leave (either mid-year or EOY), and we hope this can help inform ways to support teachers so they do not leave and are instead retained.

#### TCP Levels and Composite Score Correlations

TCP Placement Levels do not have a high correlation with TCP Composite Scores. The Composite Score is composed of 5 components (GET Ratings, Student Achievement, Parent Surveys, Student Surveys, and Core Values). This score is loosely used to decide a teacher's Placement Level, but can be overridden by a manager, a "hold", or an appeal (generally initiated by the manager). Teachers who do not get receive a high enough Composite Score to reach the TCP Placement Level they reached the prior year, a "hold" is placed to keep the teacher at the prior year's level (given that it is higher than this year's Composite Score). The teacher then has 1 academic year to get their Composite Score raised high enough to keep their TCP Placement Level. During the Pandemic, teachers were given 2 years (and then now, indefinite years of pandemic holds) of "hold" time instead of 1. General cut-offs are: 0 to 1.4 rounds to Level 1, 1.5 to 2.4 rounds to Level 2, 2.5 to 3.4 rounds to Level 3, 3.5 to 4.4 rounds to Level 4, and 4.5+ rounds to Level 5. There is also a hold for changing content. If a teacher was teaching chemistry and is now teaching physics, that teacher would be placed on a hold for 1 year. Some teachers were on hold for 2-3 years prior to the pandemic 19-20 SY, according to the TCP Team, and thus some teachers have been on a TCP hold for 5-6 years at this point (end of 21-22 SY). Business rules may change or be updated and added to again, for the 22-23 SY.




