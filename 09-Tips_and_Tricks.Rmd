# Tips and Tricks

This section of the R&A Manual is for sharing various tricks that will make your life easier.  Rules of thumb, explanations of how things work, and useful design patterns are all fair game. 

## Data processing 

### De-duplicating rows in a table 

There are two general purpose design patterns for de-duplicating rows in a table in either R or SQL.

1. In R use the `dplyr::distinct()` function and in SQL use a DISTINCT clause.  This is pretty simple and robust, with the typical use case of wanting to reduce many **identical** rows to a single row. 
2. If you are trying to de-duplicate where you may have **non-identical** rows (e.g., a student has taken the same assessment multiple times in a testing window and you'd like to get the latest date) then you can use a `group_by-arrange-row_number-filter` pattern:

    1. Group your data by the columns that uniquely identify your object of inquiry (e.g., student number)
    2. Arrange or sort the data by a column that puts an ordering you care about within the grouped data (e.g., assessment date)
    3. Create a new column that contains the row number *within the group* (e.g, a student with 5 assessments in a window would have `row_number = 1` for the first date and `row_number = 5` for the last date).
    4. Filter for the maximum row number(with `dplyr::filter()` in R or a `WHERE` clause in SQL)
    5. (optional) Ungroup your data. 
  

### Hierarchical aggregation, or the `list-group_by` trick

Schools data, especially student-level data, is embedded in hierarchical groupings. 

Let's look at the [Zillow Home Value Index](https://www.zillow.com/research/data/) for all homes (i.e, single family homes, condos, and coops),  which is a smoothed, seasonally adjusted measure of the typical home value and market changes across a given region. It reflects the typical value for homes in the 35th to 65th percentile range at the neighborhood level. These home values are available for every month from January 2000 to May 2022. 

It's a very large dataset so we'll use the [Apache Arrow Project](https://arrow.apache.org/docs/r/)'s feather file format  to read the data into R and subset the housing costs to IDEA metro areas since 2020. 

```{r read-zillow}
library(arrow)
zillow <- read_feather("./zillow.feather")


idea_metros <- tibble(Metro = c("Austin-Round Rock",
                                "Dallas-Fort Worth-Arlington",
                                "San Antonio-New Braunfels",
                                "Brownsville-Harlingen",
                                "El Paso",
                                "Houston-The Woodlands-Sugar Land",
                                "Baton Rouge",
                                "Tampa-St. Petersburg-Clearwater",
                                "Jacksonville",
                                "Cincinnati"
                                )
                      )



zillow_idea <- zillow %>% 
  inner_join(idea_metros, by = "Metro") %>% 
  select(RegionID:CountyName, matches("202*-")) %>% # matches() selects columns with regular expressions
  janitor::clean_names()

knitr::kable(
  head(zillow_idea),
   booktabs = TRUE,
  caption = 'first 10 rows of Zillow Home Value Index data')
  
```

These data are nested: we have 22 years of home prices within neighborhoods, cities, counties, metro areas, and states. Often we'd like to do aggregations over these various levels.  That is, we'd like to know the mean home price for each neighborhood, city, and county.  A typical work flow to achieve this is to write a `{dplyr}` pipeline for one `group_by` combination and then copy-and-paste that pipeline and change the columns in the `group_by` call. While this works in a pinch, its repetitive (violating [the DRY doctrine](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)), prone to copying errors and leaves with a bunch of dataframe that have increasingly awkward names. 

A more elegant solution is to leverage `{purrr}`'s `map_df` function and to pass it a list of already grouped data frames. Here's a simple example:

```{r group-by-list-simple}
library(purrr)

zillow_idea_county_state <- zillow_idea %>% 
  # create a list of grouped data.frames
  {list(
    group_by(., county_name, state), # the . is a  placeholder for the data.frame being piped in 
    group_by(., state)
  )} %>% 
  map_df(., ~{
    .x %>% # .x is a placeholder for each element of list above passed to map_df
      # across allows us to apply a function to every column satisfying a condition
      summarize(across(starts_with("x20"), ~mean(.x, na.rm = TRUE)))
  })
  

knitr::kable(zillow_idea_county_state)
```

We now have a data frame with county-level *and* state-level means in one data frame.  Notice however that the `county_name` field is `NA` for state level data:

```{r filter-na}
zillow_idea_county_state %>% 
  filter(is.na(county_name)) %>% 
  knitr::kable()
```

This makes sense, since `map_df` uses `bind_rows` to concatenate the aggregated tables together and will fill columns that are missing in a data frame with NA values.  With a simple example like this we could add an extra step at the end of our pipeline to provide more information:

```{r replace_na}
zillow_idea_county_state <- zillow_idea %>% 
  # create a list of grouped data.frames
  {list(
    group_by(., county_name, state), # the . is a  placeholder for the data.frame being piped in 
    group_by(., state)
  )} %>% 
  map_df(., ~{
    .x %>% # .x is a placeholder for each element of list above passed to map_df
      # across allows us to apply a function to every column satisfying a condition
      summarize(across(starts_with("x20"), ~mean(.x, na.rm = TRUE)))
  }) %>% 
  replace_na(list(county_name = "State Avg"))

zillow_idea_county_state %>% 
  filter(is.na(county_name))

zillow_idea_county_state %>% 
  filter(county_name == "State Avg") %>% 
  knitr::kable()
```


Why have all of this in one data frame? Because it can now be easily graphed:

```{r zillow_graph}
library(ggrepel)
library(lubridate)


zillow_idea_county_state %>% 
  # pivot to a long table by year
  pivot_longer(cols = starts_with("x"), names_to = "month", values_to = "home_value") %>% 
  # let's clean up the month column by removing the leading x and casting to a date
  mutate(month = str_remove(month, "x"), 
         month = lubridate::ymd(month)) %>% 
  
  ggplot(aes(x = month, home_value, color = county_name == "State Avg")) + 
  geom_line(aes(group=county_name)) +
  # geom_dl(aes(color = county_name == "State Avg", label = county_name), 
  #         method = "maxvar.qp") +
  geom_text_repel(data = . %>% 
                    filter(month == max(month)),
                  aes(label = county_name), 
                  xlim = c(ymd("2022-05-31"), Inf),
                  size  = 1.5,
                  force             = 0.05,
                  nudge_x           = 5,
                  direction         = "y",
                  hjust             = 0, 
                  vjust = 0.5, 
                  max.overlaps = Inf, ) +
  scale_y_continuous(labels = scales::dollar_format()) +
  ideacolors::scale_color_idea(labels = c("County Avg","State Avg")) +
  
  coord_cartesian(clip = FALSE, expand = TRUE, xlim = c(ymd("2020-01-03"), 
                                                        ymd("2023-12-31"))) +
  facet_wrap(~state) +
  ideacolors::theme_idea_min() +
  labs(y = "Typical home values", 
       x = "Month",
       color = "") 
  


```

#### Saving grouping info

Another trick is to save the `group_by` info that you are passing with the grouped data.frame into a column.  Here's an example:

```{r grouping-info}

zillow_idea_neighborhood <- zillow_idea %>% 
  # create a list of grouped data.frames
  {list(
    group_by(., region_name, city, metro, county_name, state), 
    group_by(., city, metro, county_name, state), 
    group_by(., county_name, state),
    group_by(., state)
  )} %>% 
  map_df(., ~{
    .x %>% # .x is a placeholder for each element of list above passed to map_df
      # across allows us to apply a function to every column satisfying a condition
      summarize(across(starts_with("x20"), ~mean(.x, na.rm = TRUE))) %>% 
      mutate(groups = paste(group_vars(.x), collapse = "|"))
  })



zillow_idea_neighborhood %>% 
  filter(groups == "city|metro|county_name|state") %>% 
  top_n(20) %>% 
  knitr::kable()
```


### Duplicating rows

Suppose you have aggregated data like the table below, but instead, you need the duplicated rows to create a visualization for each instance of the region. For example, we might need 3 duplicate rows for East Baton Rouge, 9 duplicate rows for Austin, and so on.

```{r wide-table-counts}
grouped_data <- tribble(
  ~ state, ~ region, ~ n_campuses,
  "FL", "Jacksonville", 2,
  "FL", "Tampa", 2,
  "LA", "East Baton Rouge", 3,
  "OH", "Cincinnati", 2,
  "TX", "Austin", 9,
  "TX", "El Paso", 5,
  "TX", "Greater Houston Area", 3,
  "TX", "Permian Basin", 2,
  "TX", "Rio Grande Valley", 26,
  "TX", "San Antonio", 15,
  "TX", "Tarrant County", 4
)

knitr::kable(grouped_data)
```

To do so, we could simply use `tidyr::uncount()` to duplicate the rows. The first 10 rows are printed as an example:

```{r long-table-duplicates}
grouped_data %>%
  uncount(n_campuses) %>%
  slice_head(n = 10) %>%
  knitr::kable()
```

The data frame could then be built out with information pertaining to each school, or could be plotted more easily in a `ggplot` call. Check the [documentation](https://tidyr.tidyverse.org/reference/uncount.html) for other useful arguments.

### Creating "Scaffolds" when joining data  

Coming Soon. . . 


## Statistical pitfalls

### Simpson's Paradox

When identifying trends in a distribution, be careful of interpreting those trends, particularly when there is hierarchy involved in the population. The ranking, magnitude, and direction of the trends can reverse when conditioning on a certain level of a hierarchy. For example, STAAR achievement levels as a function of minutes on an individual learning program like Dreambox may not be significant at the district level, but could be significant when at the school or classroom level.

### Bias-variance tradeoff

## Troubleshooting messages and errors

### Connection to the dashboard

When working with ideadata in RStudio, issues with connecting to the servers and dashboards that host ideadata may occur. For example, warnings or errors may include Resetting connection to conn_Dashboard and/or environment server connections may show ptr as "<pointer: (nil)>.

To resolve this issue, consider following:

Check your VPN status, code accuracy, environment, connections, and config settings if using ProjectTemplate [What goes where][What goes where].

VPN (aka Global Protect)
VPN connection is required for server access.
Check to make sure you are logged in to VPN. 
If you are not logged in, please enter your IDEA login credentials. 
When logged in and connected to VPN, please continue to next steps. 

Code Accuracy:
Check your code for accuracy. 
If you are trying to connect to a specific table, be sure the information for table, database, schema, and server names are listed correctly based on the most updated information.

Environment and Connections: 
Review your environment and connections.
For example, check your environment for the server connections. If "null pointer" (<pointer: (nil)>), then you will need to reset the connection and try reloading the project.

Depending on the issue, select from the following commands to enter in the console:
restart R session (CTRL + Shift + F10): to clear objects and libraries
`rm()`:to remove one or more objects from a specified environment, example`rm(conn_Dashboard)`
`disconnect()`: to close a connection, example `disconnect(conn_PROD1)`
`clear.cache()`: to clear the cache, example: `clear.cache(conn_PROD1)`

Then try reloading the project and running the script. 

## .Renviron

`.Renviron` is a file that can be used to create environment variables in R. One large benefit is that credentials such as API keys no longer need to be included in R scripts. 

To open and make changes to your `.Renviron` file, run the following code:

```{r}
library(usethis)
usethis::edit_r_environ()
```

Once this file is open, changes can be made. To access IDEA data, the following should be included:

``` {r}
IDEA_RNA_DB_UID = ‘firstname.lastname’
IDEA_RNA_DB_PWD = ‘password’ (your IDEA password)
IDEA_RNA_ODC_DRIVER = ‘{ODBC Driver __ for SQL Server}’ (substitute blank with current number)
```

To access Posit Connect, the following should be included:

```{r}
CONNECT_API_KEY = 'your key here'
CONNECT_SERVER = 'https://analytics.ideapublicschools.org/'
```

As an additional example, there may be a need for a Mapbox API token. If so, include the following in the `.Renviron` file:

```{r}
MAPBOX_API_TOKEN = 'your key here'
```

Once that is included in the `.Renviron` file, the key can be called within an R script with ‘Sys.getenv(“MAPBOX_API_TOKEN”)’.

If changes are made to the `.Renviron` file, don’t forget to restart R to have changes take effect.

## IDEA Acronym Guide

IDEA uses acronyms frequently. Several of these acronyms have been compiled in [this resource](https://ideapublicschoolsorg.sharepoint.com/:x:/s/ResearchandAnalysis/ERRH0_4pZSJMiKoGMClX-0kBtV3QTF0hnswjxo7BFY0LGw?e=yHhKGA).
