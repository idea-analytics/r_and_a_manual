# Tips and Tricks

This section of the R&A Manual is for sharing various tricks that are will make you life easier.  Rules of thumb, explanations of how things work, and useful design patterns are all fair game. 

## Data processing 

### De-duplicating rows in a table. 

There are two general purpose design patterns for de-duplicating rows in a table in either R or SQL.

1. In R use the `dplyr::distinct()` function and in SQL use a DISTINCT clause.  This is pretty simple and robust, with the typical use case of wanting to reduce many **identical** rows to a single row. 
2. If you are trying to de-duplicate where you may have **non-identical** rows (e.g., a student has taken the same assessment multiple times in a testing window and you'd like to get the latest date) then you can use a `group_by-arrange-row_number-filter` pattern:

    1. Group you data by the columns that uniquely identify your object of inquiry (e.g., student number)
    2. Arrange or sort the data by a column that puts an ordering you care about within the grouped data (e.g., assessment date)
    3. Create a new column that contains the row number *within the group* (e.g, a student with 5 assessments in a window would have `row_number = 1` for the first date and `row_number = 5` for the last date).
    4. Filter for the maximum row number(with `dplyr::filter()` in R or a `WHERE` clause in SQL)
    5. (optional) Ungroup your data. 
  

### Hierarchical aggregation, or the `list-group_by` trick

Schools data, especially student-level data, is embedded in hierarchical groupings. 

Let's look at the [Zillow Home Value Index](https://www.zillow.com/research/data/) for all homes (i.e, single family homes, condos, and coops),  which is smoothed, seasonally adjusted measure of the typical home value and market changes across a given region It reflects the typical value for homes in the 35th to 65th percentile range at the neighborhood level. These home values are available for every month from January 2000 to May 2022. 

It's a very large dataset so we'll use the [Apache Arrow Project](https://arrow.apache.org/docs/r/)'s feather file format  to read the data into R and subset the housing costs to IDEA metro areas since 2020. 

```{r read-zillow}
library(arrow)
zillow <- read_feather("./zillow.feather")


idea_metros <- tibble(Metro = c("Austin-Round Rock",
                                "Dallas-Fort Worth-Arlington",
                                "San Antonio-New Braunfels",
                                "Brownsville-Harlingen",
                                "El Paso",
                                "Houston-The Woodlands-Sugar Land",
                                "Baton Rouge",
                                "Tampa-St. Petersburg-Clearwater",
                                "Jacksonville",
                                "Cincinnati"
                                )
                      )



zillow_idea <- zillow %>% 
  inner_join(idea_metros, by = "Metro") %>% 
  select(RegionID:CountyName, matches("202*-")) %>% # matches() selects columns with regular expressions
  janitor::clean_names()

knitr::kable(
  head(zillow_idea),
   booktabs = TRUE,
  caption = 'first 10 rows of Zillow Home Value Index data')
  
```

These data are nested: we have 22 years of home prices within neighborhoods, cities, counties, metro areas, and states. Often we'd like to do aggregations by over these various level.  That is, we'd like to know the mean home price for each neighborhood, city and county.  A typical work flow to achieve this is to write a `{dplyr}` pipeline for one `group_by` combination and then copy-and-paste that pipeline and change the columns in the `group_by` call. While this works in a pinch, its repetitive (violating [the DRY doctrine](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)), prone to copying errors and leaves with a bunch of dataframe that are have increasingly awkward names. 

A more elegant solution is to leverage `{purrr}`'s `map_df` function and to pass it a listy of already grouped data frames. Here's a simple example:

```{r group-by-list-simple}
library(purrr)

zillow_idea_county_state <- zillow_idea %>% 
  # create a list of grouped data.frames
  {list(
    group_by(., county_name, state), # the . is a  placeholder for the data.frame being piped in 
    group_by(., state)
  )} %>% 
  map_df(., ~{
    .x %>% # .x is a placeholder for each element of list above passed to map_df
      # across allows us to apply a function to every column satisfying a condition
      summarize(across(starts_with("x20"), ~mean(.x, na.rm = TRUE)))
  })
  

knitr::kable(zillow_idea_county_state)
```

We now have a data frame with county-level *and* state-level means in one data frame.  Notice however that the `county_name` field is `NA` for state level data:

```{r filter-na}
zillow_idea_county_state %>% 
  filter(is.na(county_name)) %>% 
  knitr::kable()
```

This makes sense, since `map_df` uses `bind_rows` to concatonate the aggregated tables together and will fill columns that are missing in data frame with NA values.  With simple example like this we could add an extra step at the end of our pipline to provide more information:

```{r replace_na}
zillow_idea_county_state <- zillow_idea %>% 
  # create a list of grouped data.frames
  {list(
    group_by(., county_name, state), # the . is a  placeholder for the data.frame being piped in 
    group_by(., state)
  )} %>% 
  map_df(., ~{
    .x %>% # .x is a placeholder for each element of list above passed to map_df
      # across allows us to apply a function to every column satisfying a condition
      summarize(across(starts_with("x20"), ~mean(.x, na.rm = TRUE)))
  }) %>% 
  replace_na(list(county_name = "State Avg"))

zillow_idea_county_state %>% 
  filter(is.na(county_name))

zillow_idea_county_state %>% 
  filter(county_name == "State Avg") %>% 
  knitr::kable()
```


Why have all of this one data frame? Because it can now be easily graphed:

```{r zillow_graph}
library(ggrepel)
library(lubridate)


zillow_idea_county_state %>% 
  # pivot to a long table by year
  pivot_longer(cols = starts_with("x"), names_to = "month", values_to = "home_value") %>% 
  # let's clean up the month column by removing the leading x and casting to a date
  mutate(month = str_remove(month, "x"), 
         month = lubridate::ymd(month)) %>% 
  
  ggplot(aes(x = month, home_value, color = county_name == "State Avg")) + 
  geom_line(aes(group=county_name)) +
  # geom_dl(aes(color = county_name == "State Avg", label = county_name), 
  #         method = "maxvar.qp") +
  geom_text_repel(data = . %>% 
                    filter(month == max(month)),
                  aes(label = county_name), 
                  xlim = c(ymd("2022-05-31"), Inf),
                  size  = 1.5,
                  force             = 0.05,
                  nudge_x           = 5,
                  direction         = "y",
                  hjust             = 0, 
                  vjust = 0.5, 
                  max.overlaps = Inf, ) +
  scale_y_continuous(labels = scales::dollar_format()) +
  ideacolors::scale_color_idea(labels = c("County Avg","State Avg")) +
  
  coord_cartesian(clip = FALSE, expand = TRUE, xlim = c(ymd("2020-01-03"), 
                                                        ymd("2023-12-31"))) +
  facet_wrap(~state) +
  ideacolors::theme_idea_min() +
  labs(y = "Typical home values", 
       x = "Month",
       color = "") 
  


```

#### Saving grouping info

Another trick is to save the `group_by` info that you are passing with the grouped data.frame into a column.  Here's an example:

```{r grouping-info}

zillow_idea_neighborhood <- zillow_idea %>% 
  # create a list of grouped data.frames
  {list(
    group_by(., region_name, city, metro, county_name, state), 
    group_by(., city, metro, county_name, state), 
    group_by(., county_name, state),
    group_by(., state)
  )} %>% 
  map_df(., ~{
    .x %>% # .x is a placeholder for each element of list above passed to map_df
      # across allows us to apply a function to every column satisfying a condition
      summarize(across(starts_with("x20"), ~mean(.x, na.rm = TRUE))) %>% 
      mutate(groups = paste(group_vars(.x), collapse = "|"))
  })



zillow_idea_neighborhood %>% 
  filter(groups == "city|metro|county_name|state") %>% 
  top_n(20) %>% 
  knitr::kable()
```




### Creating "Scaffolds" when joining data.  

Coming Soon. . . 


## Statistical pitfalls

### Simpson's Paradox

When identifying trends in a distribution, be careful of interpreting those trends, particularly when there is hierarchy involved in the population. The ranking, magnitude, and direction of the trends can reverse when conditioning on a certain level of a hierarchy. For example, STAAR achievement levels as a function of minutes on an individual learning program like Dreambox may not be significant at the district level, but could be significant when at the school or classroom level.

### Bias-variance tradeoff

## Troubleshooting messags and errors

### Connection to the dashboard

Insert popout box here with the image or type the code in the code font to display the error messages:
"Resetting connection to conn_Dashboard" and 
"Error: nanodbc/nanodbc.cpp:1021: 08001: [Microsoft][ODBC Driver 17 for SQL Server]Named Pipes Provider: Could not open a connection to SQL Server [53]. [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired. [Microsoft][ODBC Driver 17 for SQL Server]A network-related or instance-specific error has occured while establishing a connection to sQL Server. Server is not found or not accessible. Check if instance name is correct and if SQL Server is configured to allow remote connections. For more information see SQL Server Books Online."
conn_Dashboard may show the ptr as "<pointer: (nil)>"

When working on R scripts in RStudio, an issue that may occur involves connecting to the servers and dashboards that host ideadata.
To resolve this issue, consider following the following steps. 

Step 1. The first step upon receiving an error involving connecting to the SQL Servers is always to check VPN connection. VPN may also be referred to as "Global Protect." 
Check to make sure you are logged in to VPN. 
If you are not logged in, please enter your IDEA login credentials. 
If logged in and connected to VPN, please continue to the next step. 

Step 2. The next step is to check your code. Be sure that you have all information listed correctly. If you are trying to connect to a specific table in ideadata, be sure the table name, database name, schema, and server name are correct. 
For example: 
```{r check code example}

# For example, check the code below for accuracy. Be sure the .table_name, #.database_name, .schema, and .server_name list the most updated and accurate information. 

df <- get_table(.table_name = "STAAR",
                      .database_name = "Dashboard",
                      .schema = "dbo",
                      .server_name = "RGVPDRA-DASQL")
```

Step 3. If the code is correct, the next step is to remove the connection to the Dashboard. 
To remove the connection, enter the following code into the script or console: `rm(conn_Dashboard)`

Step 4. After removing the connection to the dashboard, try re-running the script.
(Note. If you entered the remove command into the R script, be sure to comment out before re-running the script.)
